{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/ptls-experiments/scenario_rosbank'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# from torch.fx import symbolic_trace\n",
    "# import torch.ao.quantization.quantize_fx as quantize_fx\n",
    "# from typing import List, Dict\n",
    "\n",
    "# class FXWrapping(nn.Module):\n",
    "#     def __init__(self, base, qconfig, *args, **kwargs):\n",
    "#         super().__init__()\n",
    "#         self.base = base\n",
    "#         self.example_input = None\n",
    "#         self.qconfig = qconfig\n",
    "#         self.args = args\n",
    "#         self.kwargs = kwargs\n",
    "#         self._is_prepared = self._is_converted = False\n",
    "\n",
    "#     def forward(self, *args, **kwargs):\n",
    "#         # if not isinstance(self.base, nn.RNNBase):\n",
    "#         #     args = (args[0],)\n",
    "#         self.example_input = (args, kwargs)\n",
    "#         return self.base(*args, **kwargs)\n",
    "    \n",
    "#     def prepare(self):\n",
    "#         assert self.example_input is not None, 'Run model with example input!'\n",
    "#         self.base = quantize_fx.prepare_fx(self.base, self.qconfig, \n",
    "#                                            self.example_input[0], **self.example_input[1])\n",
    "#         self._is_prepared = True\n",
    "                               \n",
    "#     def convert(self):\n",
    "#         assert self._is_prepared, 'Prepare model before convertation!'\n",
    "#         self.base = quantize_fx.convert_fx(self.base)\n",
    "#         self._is_converted = True\n",
    "        \n",
    "\n",
    "# class FXWrapper:\n",
    "#     wrappings: Dict[str, FXWrapping] = {'basic': FXWrapping}\n",
    "\n",
    "#     def __init__(self, qconfig, wrapping='basic', is_qat=False):\n",
    "#         self.is_qat = is_qat\n",
    "#         self.qconfig = qconfig\n",
    "#         self._wrapped_modules: List[FXWrapping] = []\n",
    "#         self.wrapping = self.wrappings[wrapping]\n",
    "\n",
    "#     def wrap(self, model: nn.Module, batch_sample):\n",
    "#         model.eval()\n",
    "#         self._wrap_init(model)\n",
    "#         model(batch_sample)\n",
    "    \n",
    "#     def _wrap_init(self, model: nn.Module):\n",
    "#         named_children = list(model.named_children())\n",
    "#         if not named_children:\n",
    "#             named_children = [('', model)]\n",
    "#         for name, child in named_children:\n",
    "#             trace = self.is_traceable(child)\n",
    "#             if trace:\n",
    "#                 wrapped = self.wrapping(child, self.qconfig)\n",
    "#                 self._wrapped_modules.append(wrapped)\n",
    "#                 setattr(model, name, wrapped)\n",
    "#                 print(name, 'name')\n",
    "#             elif not name:\n",
    "#                 self._wrap_init(child)\n",
    "\n",
    "#     def prepare(self):\n",
    "#         for module in self._wrapped_modules:\n",
    "#             module.prepare()\n",
    "    \n",
    "#     def convert(self):\n",
    "#         for module in self._wrapped_modules:\n",
    "#             module.convert()\n",
    "            \n",
    "#     @classmethod\n",
    "#     def is_traceable(cls, module: nn.Module):\n",
    "#         try:\n",
    "#             trace = symbolic_trace(module)\n",
    "#             return trace\n",
    "#         except:\n",
    "#             return None\n",
    "        \n",
    "# from torch.ao.quantization.qconfig_mapping import QConfigMapping, get_default_qconfig_mapping\n",
    "# qconfig_mapping = get_default_qconfig_mapping('x86')\n",
    "\n",
    "# class TestModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.lstm = nn.LSTM(10, 11)\n",
    "#         self.head = nn.Sequential(nn.Linear(11, 1), nn.ReLU())\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x, _ = self.lstm(x)\n",
    "#         x = self.head(x)\n",
    "#         return x\n",
    "\n",
    "# batch_sample = torch.randn((19, 7, 10))\n",
    "# # batch_sample = torch.randn((19, 11))\n",
    "\n",
    "# model = TestModel()\n",
    "# wrapper = FXWrapper(qconfig_mapping)\n",
    "# wrapper.wrap(model, batch_sample)\n",
    "# wrapper.prepare()\n",
    "# wrapper.convert()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper.convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conventional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1+cu117'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_size(model: torch.nn.Module):\n",
    "    const = 1\n",
    "    size = 0\n",
    "    for module in model.parameters():\n",
    "        size += module.numel() * module.element_size()\n",
    "    for buffer in model.buffers():\n",
    "        size += buffer.numel() * buffer.element_size()\n",
    "    return size / const\n",
    "\n",
    "def gain(base, comp):\n",
    "    base, comp = estimate_size(base), estimate_size(comp)\n",
    "    return (base - comp) / base * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to 224x224\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert grayscale to 3 channels\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize for 3 channels\n",
    "])\n",
    "\n",
    "# Load MNIST dataset\n",
    "PATH = 'MNIST'\n",
    "trainset = torchvision.datasets.MNIST(root=PATH, train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.MNIST(root=PATH, train=False, download=True, transform=transform)\n",
    "\n",
    "import numpy as np\n",
    "idx = np.arange(32)\n",
    "trainset = Subset(trainset, idx)\n",
    "testset = Subset(testset, idx)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "# Load ResNet-50 model\n",
    "model = models.resnet50(pretrained=True)  # You can use pretrained weights\n",
    "# Modify the first layer to accept 3 channel input\n",
    "model.conv1 = torch.nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 10)  # Adjust output for 10 classes (0-9)\n",
    "\n",
    "# To use the model for training\n",
    "model.eval()\n",
    "\n",
    "example_inputs = (next(iter(trainloader))[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_quantize_model(model, qconfig=None, backend='x86'):\n",
    "    if qconfig is None:\n",
    "        torch.backends.quantized.engine = backend\n",
    "        qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "    model.eval()\n",
    "    \n",
    "    model.qconfig = qconfig\n",
    "    model_quantized = torch.quantization.prepare(torch.ao.quantization.QuantWrapper(model), inplace=False)\n",
    "    model_quantized = torch.quantization.convert(model_quantized, inplace=True)\n",
    "    return model_quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['model', 'run_fn', 'run_args', 'mapping', 'inplace'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from inspect import signature\n",
    "\n",
    "(signature(torch.ao.quantization.quantize).parameters).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/observer.py:1204: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch.ao.quantization\n",
    "\n",
    "placeholder = lambda *args, **kwargs: None\n",
    "\n",
    "qm = torch.ao.quantization.quantiz(sup_module, placeholder, (1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/observer.py:1204: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "qm = post_quantize_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_quant(quantized_model, path):\n",
    "    torch.jit.save(torch.jit.script(quantized_model), path)\n",
    "\n",
    "def load_quant(path):\n",
    "    script = torch.jit.load(path)\n",
    "    return script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.92875846184091"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gain(model, qm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceToTarget(\n",
       "  (seq_encoder): RnnSeqEncoder(\n",
       "    (trx_encoder): TrxEncoder(\n",
       "      (embeddings): ModuleDict(\n",
       "        (small_group): NoisyEmbedding(\n",
       "          150, 32, padding_idx=0\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (custom_embeddings): ModuleDict(\n",
       "        (amount_rur): LogScaler()\n",
       "      )\n",
       "    )\n",
       "    (seq_encoder): RnnEncoder(\n",
       "      (reducer): LastStepEncoder()\n",
       "      (rnn): GRU(33, 48, batch_first=True)\n",
       "    )\n",
       "  )\n",
       "  (head): Head(\n",
       "    (model): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): QuantizedLinear(in_features=48, out_features=4, scale=1.0, zero_point=0, qscheme=torch.per_channel_affine)\n",
       "        (1): LogSoftmax(dim=1)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (loss): NLLLoss()\n",
       "  (train_metrics): ModuleDict(\n",
       "    (MulticlassAccuracy): MulticlassAccuracy()\n",
       "  )\n",
       "  (valid_metrics): ModuleDict(\n",
       "    (MulticlassAccuracy): MulticlassAccuracy()\n",
       "  )\n",
       "  (test_metrics): ModuleDict(\n",
       "    (MulticlassAccuracy): MulticlassAccuracy()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Bottleneck(nn.Module):\n",
      "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
      "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
      "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
      "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
      "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
      "\n",
      "    expansion: int = 4\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        inplanes: int,\n",
      "        planes: int,\n",
      "        stride: int = 1,\n",
      "        downsample: Optional[nn.Module] = None,\n",
      "        groups: int = 1,\n",
      "        base_width: int = 64,\n",
      "        dilation: int = 1,\n",
      "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
      "    ) -> None:\n",
      "        super().__init__()\n",
      "        if norm_layer is None:\n",
      "            norm_layer = nn.BatchNorm2d\n",
      "        width = int(planes * (base_width / 64.0)) * groups\n",
      "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
      "        self.conv1 = conv1x1(inplanes, width)\n",
      "        self.bn1 = norm_layer(width)\n",
      "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
      "        self.bn2 = norm_layer(width)\n",
      "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
      "        self.bn3 = norm_layer(planes * self.expansion)\n",
      "        self.relu = nn.ReLU(inplace=True)\n",
      "        self.downsample = downsample\n",
      "        self.stride = stride\n",
      "\n",
      "    def forward(self, x: Tensor) -> Tensor:\n",
      "        identity = x\n",
      "\n",
      "        out = self.conv1(x)\n",
      "        out = self.bn1(out)\n",
      "        out = self.relu(out)\n",
      "\n",
      "        out = self.conv2(out)\n",
      "        out = self.bn2(out)\n",
      "        out = self.relu(out)\n",
      "\n",
      "        out = self.conv3(out)\n",
      "        out = self.bn3(out)\n",
      "\n",
      "        if self.downsample is not None:\n",
      "            identity = self.downsample(x)\n",
      "\n",
      "        out += identity\n",
      "        out = self.relu(out)\n",
      "\n",
      "        return out\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import ast\n",
    "import re\n",
    "# print(inspect.getsource(type(model)))\n",
    "# for name, c in model.named_children():\n",
    "#     print(name, c)\n",
    "# (?<substrate>\\w+?) ?[=\\/\\*\\+]= ?(?<other>\\w+)\n",
    "print(inspect.getsource(type(model.layer1[0])))\n",
    "ast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FedCore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fc_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mptls\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfedcore_compression\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfc_setups\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TEST\n\u001b[1;32m      2\u001b[0m QUANT \u001b[38;5;241m=\u001b[39m TEST\n\u001b[1;32m      3\u001b[0m qkey \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_aware_quant\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/ptls-experiments/pytorch-lifestream/ptls/fedcore_compression/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfc_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfc_setups\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fc_utils'"
     ]
    }
   ],
   "source": [
    "from ptls.fedcore_compression.fc_setups import TEST\n",
    "QUANT = TEST\n",
    "qkey = 'training_aware_quant'\n",
    "# qkey = 'post_training_quant'\n",
    "QUANT['initial_assumption'] = [qkey]\n",
    "QUANT['model_params'][qkey] = {'epochs': 1}\n",
    "QUANT['distributed_compression'] = False\n",
    "\n",
    "QUANT['common']['save_each'] = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'QUANT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mQUANT\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'QUANT' is not defined"
     ]
    }
   ],
   "source": [
    "QUANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fedcore.api.main.FedCore at 0x7f4d13e7bb20>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fedcore.api.main import FedCore\n",
    "from fedcore.api.utils.data import get_compression_input\n",
    "\n",
    "compinp = get_compression_input(model, train_dataloader=trainloader, calib_dataloader=trainloader, num_classes=10)\n",
    "# fcomp = FedCore(compression_task='composite_compression',\n",
    "#                 need_evo_opt=False,\n",
    "                \n",
    "#                 initial_assumption=['training_aware_quant'])\n",
    "fcomp = FedCore(**QUANT, need_evo_opt=False)\n",
    "\n",
    "fcomp  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-25 16:39:44,746 - Initialising FedCore Repository\n",
      "2024-11-25 16:39:44,948 - Initialising solver\n",
      "2024-11-25 16:39:44,950 - Initialising experiment setup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-25 16:39:45 [INFO] Attention Blocks: 0\n",
      "2024-11-25 16:39:45 [INFO] FFN Blocks: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### PTFXADAPT _get_quantizable_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "_get_bf16_ops_recursively\n",
      "### _cfgs_to_fx_cfgs OrderedDict()\n",
      "Quantized model training supports CPU only\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch #: 100%|██████████| 1/1 [00:03<00:00,  3.39s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "fcomp.fit((compinp, model), manually_done=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(testloader))[0]#.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### MODULE QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.012489816173911095, zero_point=0, padding=(3, 3))\n",
      "### non valid <class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>\n",
      "### MODULE QuantizedConvReLU2d(64, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.00820652861148119, zero_point=0)\n",
      "### non valid <class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>\n",
      "### MODULE QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.006752930115908384, zero_point=0, padding=(1, 1))\n",
      "### non valid <class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>\n",
      "### MODULE QuantizedConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.014969165436923504, zero_point=148)\n",
      "### non valid <class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "### MODULE QuantizedConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.019215935841202736, zero_point=128)\n",
      "### non valid <class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "### MODULE QuantizedConvReLU2d(256, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.005573154892772436, zero_point=0)\n",
      "### non valid <class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>\n",
      "### MODULE QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.006982532329857349, zero_point=0, padding=(1, 1))\n",
      "### non valid <class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>\n",
      "### MODULE QuantizedConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.019167553633451462, zero_point=125)\n",
      "### non valid <class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "### MODULE QuantizedConvReLU2d(256, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.005272907204926014, zero_point=0)\n",
      "### non valid <class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>\n",
      "### MODULE QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.0073172966949641705, zero_point=0, padding=(1, 1))\n",
      "### non valid <class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>\n",
      "### MODULE QuantizedConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.020491192117333412, zero_point=136)\n",
      "### non valid <class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "### MODULE QuantizedConvReLU2d(256, 128, kernel_size=(1, 1), stride=(1, 1), scale=0.00792172271758318, zero_point=0)\n",
      "### non valid <class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>\n",
      "### MODULE QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.006761183030903339, zero_point=0, padding=(1, 1))\n",
      "### non valid <class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>\n",
      "### MODULE QuantizedConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.016344282776117325, zero_point=140)\n",
      "### non valid <class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "### MODULE QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=0.012717998586595058, zero_point=118)\n",
      "### non valid <class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "### MODULE QuantizedConvReLU2d(512, 128, kernel_size=(1, 1), stride=(1, 1), scale=0.003387660952284932, zero_point=0)\n",
      "### non valid <class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>\n",
      "### MODULE QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.004854259081184864, zero_point=0, padding=(1, 1))\n",
      "### non valid <class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>\n",
      "### MODULE QuantizedConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.016187284141778946, zero_point=129)\n",
      "### non valid <class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "### MODULE QuantizedConvReLU2d(512, 128, kernel_size=(1, 1), stride=(1, 1), scale=0.0047142719849944115, zero_point=0)\n",
      "### non valid <class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>\n",
      "### MODULE QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.00512347836047411, zero_point=0, padding=(1, 1))\n",
      "### non valid <class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>\n",
      "### MODULE QuantizedConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.015170459635555744, zero_point=132)\n",
      "### non valid <class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "### MODULE QuantizedConvReLU2d(512, 128, kernel_size=(1, 1), stride=(1, 1), scale=0.004762618336826563, zero_point=0)\n",
      "### non valid <class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>\n",
      "### MODULE QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.007112165447324514, zero_point=0, padding=(1, 1))\n",
      "### non valid <class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>\n",
      "### MODULE QuantizedConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.014897962100803852, zero_point=117)\n",
      "### non valid <class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "### MODULE QuantizedConvReLU2d(512, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.009053245186805725, zero_point=0)\n",
      "### non valid <class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>\n",
      "### MODULE QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.005139123648405075, zero_point=0, padding=(1, 1))\n",
      "### non valid <class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>\n",
      "### MODULE QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=0.013089382089674473, zero_point=124)\n",
      "### non valid <class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "### MODULE QuantizedConv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), scale=0.014203527942299843, zero_point=143)\n",
      "### non valid <class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "### MODULE QuantizedConvReLU2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.005011160392314196, zero_point=0)\n",
      "### non valid <class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>\n",
      "### MODULE QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.004881533794105053, zero_point=0, padding=(1, 1))\n",
      "### non valid <class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>\n",
      "### MODULE QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=0.013255818746984005, zero_point=136)\n",
      "### non valid <class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "### MODULE QuantizedConvReLU2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.0050560785457491875, zero_point=0)\n",
      "### non valid <class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>\n",
      "### MODULE QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.004310206975787878, zero_point=0, padding=(1, 1))\n",
      "### non valid <class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>\n",
      "### MODULE QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=0.009477833285927773, zero_point=136)\n",
      "### non valid <class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "### MODULE QuantizedConvReLU2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.005439266096800566, zero_point=0)\n",
      "### non valid <class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>\n",
      "### MODULE QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.004307362250983715, zero_point=0, padding=(1, 1))\n",
      "### non valid <class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>\n",
      "### MODULE QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=0.01142102014273405, zero_point=124)\n",
      "### non valid <class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "### MODULE QuantizedConvReLU2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.004547261167317629, zero_point=0)\n",
      "### non valid <class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>\n",
      "### MODULE QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.0040996987372636795, zero_point=0, padding=(1, 1))\n",
      "### non valid <class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>\n",
      "### MODULE QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=0.009448938071727753, zero_point=133)\n",
      "### non valid <class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "### MODULE QuantizedConvReLU2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.006420395337045193, zero_point=0)\n",
      "### non valid <class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>\n",
      "### MODULE QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.004354475997388363, zero_point=0, padding=(1, 1))\n",
      "### non valid <class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>\n",
      "### MODULE QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=0.012275868095457554, zero_point=156)\n",
      "### non valid <class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "### MODULE QuantizedConvReLU2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.007777408231049776, zero_point=0)\n",
      "### non valid <class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>\n",
      "### MODULE QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.006745269056409597, zero_point=0, padding=(1, 1))\n",
      "### non valid <class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>\n",
      "### MODULE QuantizedConv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), scale=0.02888716198503971, zero_point=124)\n",
      "### non valid <class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "### MODULE QuantizedConv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), scale=0.03285527974367142, zero_point=119)\n",
      "### non valid <class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "### MODULE QuantizedConvReLU2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.006289450917392969, zero_point=0)\n",
      "### non valid <class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>\n",
      "### MODULE QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.004347581882029772, zero_point=0, padding=(1, 1))\n",
      "### non valid <class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>\n",
      "### MODULE QuantizedConv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), scale=0.026158370077610016, zero_point=131)\n",
      "### non valid <class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "### MODULE QuantizedConvReLU2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.006746557075530291, zero_point=0)\n",
      "### non valid <class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>\n",
      "### MODULE QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.004992177709937096, zero_point=0, padding=(1, 1))\n",
      "### non valid <class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>\n",
      "### MODULE QuantizedConv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), scale=0.0631210207939148, zero_point=94)\n",
      "### non valid <class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "### module name k conv1\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.012489816173911095, zero_point=0, padding=(3, 3))\n",
      "name skipped \n",
      "### module name k layer1.0.conv1\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConvReLU2d(64, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.00820652861148119, zero_point=0)\n",
      "name skipped \n",
      "### module name k layer1.0.conv2\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.006752930115908384, zero_point=0, padding=(1, 1))\n",
      "name skipped \n",
      "### module name k layer1.0.conv3\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.014969165436923504, zero_point=148)\n",
      "name skipped \n",
      "### module name k layer1.0.downsample.0\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.019215935841202736, zero_point=128)\n",
      "name skipped \n",
      "### module name k layer1.1.conv1\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConvReLU2d(256, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.005573154892772436, zero_point=0)\n",
      "name skipped \n",
      "### module name k layer1.1.conv2\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.006982532329857349, zero_point=0, padding=(1, 1))\n",
      "name skipped \n",
      "### module name k layer1.1.conv3\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.019167553633451462, zero_point=125)\n",
      "name skipped \n",
      "### module name k layer1.2.conv1\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConvReLU2d(256, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.005272907204926014, zero_point=0)\n",
      "name skipped \n",
      "### module name k layer1.2.conv2\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.0073172966949641705, zero_point=0, padding=(1, 1))\n",
      "name skipped \n",
      "### module name k layer1.2.conv3\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.020491192117333412, zero_point=136)\n",
      "name skipped \n",
      "### module name k layer2.0.conv1\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConvReLU2d(256, 128, kernel_size=(1, 1), stride=(1, 1), scale=0.00792172271758318, zero_point=0)\n",
      "name skipped \n",
      "### module name k layer2.0.conv2\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.006761183030903339, zero_point=0, padding=(1, 1))\n",
      "name skipped \n",
      "### module name k layer2.0.conv3\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.016344282776117325, zero_point=140)\n",
      "name skipped \n",
      "### module name k layer2.0.downsample.0\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=0.012717998586595058, zero_point=118)\n",
      "name skipped \n",
      "### module name k layer2.1.conv1\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConvReLU2d(512, 128, kernel_size=(1, 1), stride=(1, 1), scale=0.003387660952284932, zero_point=0)\n",
      "name skipped \n",
      "### module name k layer2.1.conv2\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.004854259081184864, zero_point=0, padding=(1, 1))\n",
      "name skipped \n",
      "### module name k layer2.1.conv3\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.016187284141778946, zero_point=129)\n",
      "name skipped \n",
      "### module name k layer2.2.conv1\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConvReLU2d(512, 128, kernel_size=(1, 1), stride=(1, 1), scale=0.0047142719849944115, zero_point=0)\n",
      "name skipped \n",
      "### module name k layer2.2.conv2\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.00512347836047411, zero_point=0, padding=(1, 1))\n",
      "name skipped \n",
      "### module name k layer2.2.conv3\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.015170459635555744, zero_point=132)\n",
      "name skipped \n",
      "### module name k layer2.3.conv1\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConvReLU2d(512, 128, kernel_size=(1, 1), stride=(1, 1), scale=0.004762618336826563, zero_point=0)\n",
      "name skipped \n",
      "### module name k layer2.3.conv2\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.007112165447324514, zero_point=0, padding=(1, 1))\n",
      "name skipped \n",
      "### module name k layer2.3.conv3\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.014897962100803852, zero_point=117)\n",
      "name skipped \n",
      "### module name k layer3.0.conv1\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConvReLU2d(512, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.009053245186805725, zero_point=0)\n",
      "name skipped \n",
      "### module name k layer3.0.conv2\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.005139123648405075, zero_point=0, padding=(1, 1))\n",
      "name skipped \n",
      "### module name k layer3.0.conv3\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=0.013089382089674473, zero_point=124)\n",
      "name skipped \n",
      "### module name k layer3.0.downsample.0\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), scale=0.014203527942299843, zero_point=143)\n",
      "name skipped \n",
      "### module name k layer3.1.conv1\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConvReLU2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.005011160392314196, zero_point=0)\n",
      "name skipped \n",
      "### module name k layer3.1.conv2\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.004881533794105053, zero_point=0, padding=(1, 1))\n",
      "name skipped \n",
      "### module name k layer3.1.conv3\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=0.013255818746984005, zero_point=136)\n",
      "name skipped \n",
      "### module name k layer3.2.conv1\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConvReLU2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.0050560785457491875, zero_point=0)\n",
      "name skipped \n",
      "### module name k layer3.2.conv2\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.004310206975787878, zero_point=0, padding=(1, 1))\n",
      "name skipped \n",
      "### module name k layer3.2.conv3\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=0.009477833285927773, zero_point=136)\n",
      "name skipped \n",
      "### module name k layer3.3.conv1\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConvReLU2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.005439266096800566, zero_point=0)\n",
      "name skipped \n",
      "### module name k layer3.3.conv2\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.004307362250983715, zero_point=0, padding=(1, 1))\n",
      "name skipped \n",
      "### module name k layer3.3.conv3\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=0.01142102014273405, zero_point=124)\n",
      "name skipped \n",
      "### module name k layer3.4.conv1\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConvReLU2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.004547261167317629, zero_point=0)\n",
      "name skipped \n",
      "### module name k layer3.4.conv2\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.0040996987372636795, zero_point=0, padding=(1, 1))\n",
      "name skipped \n",
      "### module name k layer3.4.conv3\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=0.009448938071727753, zero_point=133)\n",
      "name skipped \n",
      "### module name k layer3.5.conv1\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConvReLU2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.006420395337045193, zero_point=0)\n",
      "name skipped \n",
      "### module name k layer3.5.conv2\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.004354475997388363, zero_point=0, padding=(1, 1))\n",
      "name skipped \n",
      "### module name k layer3.5.conv3\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=0.012275868095457554, zero_point=156)\n",
      "name skipped \n",
      "### module name k layer4.0.conv1\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConvReLU2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.007777408231049776, zero_point=0)\n",
      "name skipped \n",
      "### module name k layer4.0.conv2\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.006745269056409597, zero_point=0, padding=(1, 1))\n",
      "name skipped \n",
      "### module name k layer4.0.conv3\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), scale=0.02888716198503971, zero_point=124)\n",
      "name skipped \n",
      "### module name k layer4.0.downsample.0\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), scale=0.03285527974367142, zero_point=119)\n",
      "name skipped \n",
      "### module name k layer4.1.conv1\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConvReLU2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.006289450917392969, zero_point=0)\n",
      "name skipped \n",
      "### module name k layer4.1.conv2\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.004347581882029772, zero_point=0, padding=(1, 1))\n",
      "name skipped \n",
      "### module name k layer4.1.conv3\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), scale=0.026158370077610016, zero_point=131)\n",
      "name skipped \n",
      "### module name k layer4.2.conv1\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConvReLU2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.006746557075530291, zero_point=0)\n",
      "name skipped \n",
      "### module name k layer4.2.conv2\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.004992177709937096, zero_point=0, padding=(1, 1))\n",
      "name skipped \n",
      "### module name k layer4.2.conv3\n",
      "### RTN QUANTIZE\n",
      "### model QuantizedConv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), scale=0.0631210207939148, zero_point=94)\n",
      "name skipped \n"
     ]
    }
   ],
   "source": [
    "compressed =fcomp.optimised_model.export_compressed_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): TestLinear(in_features=2048, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyTorchFXModel(\n",
       "  (_model): GraphModule(\n",
       "    (conv1): QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.013203829526901245, zero_point=0, padding=(3, 3))\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Module(\n",
       "      (0): Module(\n",
       "        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.0058478922583162785, zero_point=0)\n",
       "        (conv2): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.007402090821415186, zero_point=0, padding=(1, 1))\n",
       "        (conv3): QuantizedConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.011489166878163815, zero_point=121)\n",
       "        (downsample): Module(\n",
       "          (0): QuantizedConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.02344454638659954, zero_point=113)\n",
       "        )\n",
       "      )\n",
       "      (1): Module(\n",
       "        (conv1): QuantizedConvReLU2d(256, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.008299344219267368, zero_point=0)\n",
       "        (conv2): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.008430622518062592, zero_point=0, padding=(1, 1))\n",
       "        (conv3): QuantizedConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.0167658980935812, zero_point=113)\n",
       "      )\n",
       "      (2): Module(\n",
       "        (conv1): QuantizedConvReLU2d(256, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.007984941825270653, zero_point=0)\n",
       "        (conv2): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.00995570421218872, zero_point=0, padding=(1, 1))\n",
       "        (conv3): QuantizedConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.018559785559773445, zero_point=138)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Module(\n",
       "      (0): Module(\n",
       "        (conv1): QuantizedConvReLU2d(256, 128, kernel_size=(1, 1), stride=(1, 1), scale=0.008484484627842903, zero_point=0)\n",
       "        (conv2): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.007756521925330162, zero_point=0, padding=(1, 1))\n",
       "        (conv3): QuantizedConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.018062176182866096, zero_point=116)\n",
       "        (downsample): Module(\n",
       "          (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=0.023035796359181404, zero_point=92)\n",
       "        )\n",
       "      )\n",
       "      (1): Module(\n",
       "        (conv1): QuantizedConvReLU2d(512, 128, kernel_size=(1, 1), stride=(1, 1), scale=0.004422048106789589, zero_point=0)\n",
       "        (conv2): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.004841320216655731, zero_point=0, padding=(1, 1))\n",
       "        (conv3): QuantizedConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.01626114547252655, zero_point=127)\n",
       "      )\n",
       "      (2): Module(\n",
       "        (conv1): QuantizedConvReLU2d(512, 128, kernel_size=(1, 1), stride=(1, 1), scale=0.005120512563735247, zero_point=0)\n",
       "        (conv2): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.005079571157693863, zero_point=0, padding=(1, 1))\n",
       "        (conv3): QuantizedConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.014294909313321114, zero_point=131)\n",
       "      )\n",
       "      (3): Module(\n",
       "        (conv1): QuantizedConvReLU2d(512, 128, kernel_size=(1, 1), stride=(1, 1), scale=0.007231660187244415, zero_point=0)\n",
       "        (conv2): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.006897990126162767, zero_point=0, padding=(1, 1))\n",
       "        (conv3): QuantizedConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.020605916157364845, zero_point=78)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Module(\n",
       "      (0): Module(\n",
       "        (conv1): QuantizedConvReLU2d(512, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.007587697356939316, zero_point=0)\n",
       "        (conv2): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.005764462519437075, zero_point=0, padding=(1, 1))\n",
       "        (conv3): QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=0.014933954924345016, zero_point=130)\n",
       "        (downsample): Module(\n",
       "          (0): QuantizedConv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), scale=0.012340874411165714, zero_point=123)\n",
       "        )\n",
       "      )\n",
       "      (1): Module(\n",
       "        (conv1): QuantizedConvReLU2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.005848073400557041, zero_point=0)\n",
       "        (conv2): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.007174815982580185, zero_point=0, padding=(1, 1))\n",
       "        (conv3): QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=0.015162862837314606, zero_point=115)\n",
       "      )\n",
       "      (2): Module(\n",
       "        (conv1): QuantizedConvReLU2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.004517953377217054, zero_point=0)\n",
       "        (conv2): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.004119871184229851, zero_point=0, padding=(1, 1))\n",
       "        (conv3): QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=0.009556043893098831, zero_point=123)\n",
       "      )\n",
       "      (3): Module(\n",
       "        (conv1): QuantizedConvReLU2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.005027614533901215, zero_point=0)\n",
       "        (conv2): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.00424227537587285, zero_point=0, padding=(1, 1))\n",
       "        (conv3): QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=0.012993773445487022, zero_point=102)\n",
       "      )\n",
       "      (4): Module(\n",
       "        (conv1): QuantizedConvReLU2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.005434568040072918, zero_point=0)\n",
       "        (conv2): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.004168105777353048, zero_point=0, padding=(1, 1))\n",
       "        (conv3): QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=0.009503541514277458, zero_point=140)\n",
       "      )\n",
       "      (5): Module(\n",
       "        (conv1): QuantizedConvReLU2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.005711632315069437, zero_point=0)\n",
       "        (conv2): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.00666151475161314, zero_point=0, padding=(1, 1))\n",
       "        (conv3): QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=0.011109561659395695, zero_point=144)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Module(\n",
       "      (0): Module(\n",
       "        (conv1): QuantizedConvReLU2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.006446360610425472, zero_point=0)\n",
       "        (conv2): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.006687342654913664, zero_point=0, padding=(1, 1))\n",
       "        (conv3): QuantizedConv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), scale=0.028209371492266655, zero_point=121)\n",
       "        (downsample): Module(\n",
       "          (0): QuantizedConv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), scale=0.03233707696199417, zero_point=113)\n",
       "        )\n",
       "      )\n",
       "      (1): Module(\n",
       "        (conv1): QuantizedConvReLU2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.004995489027351141, zero_point=0)\n",
       "        (conv2): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.004438791889697313, zero_point=0, padding=(1, 1))\n",
       "        (conv3): QuantizedConv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), scale=0.025361621752381325, zero_point=133)\n",
       "      )\n",
       "      (2): Module(\n",
       "        (conv1): QuantizedConvReLU2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.005385942291468382, zero_point=0)\n",
       "        (conv2): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.004873066209256649, zero_point=0, padding=(1, 1))\n",
       "        (conv3): QuantizedConv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), scale=0.05763248726725578, zero_point=94)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\n",
    "    '/ptls-experiments/scenario_rosbank/models/mles_model.p'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PTLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 30000 records to train: 20000, valid: 3000, test: 7000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functools import partial\n",
    "from ptls.frames import PtlsDataModule\n",
    "from ptls.nn import TrxEncoder, RnnSeqEncoder, Head\n",
    "from ptls.data_load.datasets import MemoryMapDataset\n",
    "from ptls.preprocessing.pandas.pandas_preprocessor import PandasDataPreprocessor\n",
    "from ptls.frames.supervised import SeqToTargetDataset, SequenceToTarget\n",
    "from ptls.data_load.utils import collate_feature_dict\n",
    "from ptls.frames.inference_module import InferenceModule\n",
    "\n",
    "df_target = pd.read_csv(\n",
    "    \"https://huggingface.co/datasets/dllllb/age-group-prediction/resolve/main/train_target.csv?download=true\",\n",
    ")\n",
    "df_target.head()\n",
    "\n",
    "df_target_train, df_target_test = train_test_split(\n",
    "    df_target, test_size=7000, stratify=df_target[\"bins\"], random_state=142)\n",
    "df_target_train, df_target_valid = train_test_split(\n",
    "    df_target_train, test_size=3000, stratify=df_target_train[\"bins\"], random_state=142)\n",
    "print(\"Split {} records to train: {}, valid: {}, test: {}\".format(\n",
    "    *[\n",
    "      len(df)\n",
    "      for df in [df_target, df_target_train, df_target_valid, df_target_test]\n",
    "    ]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 26450577 transactions to train: 17622321, valid: 2634248, test: 6194008\n"
     ]
    }
   ],
   "source": [
    "df_trx = pd.read_csv(\n",
    "    \"https://huggingface.co/datasets/dllllb/age-group-prediction/resolve/main/transactions_train.csv.gz?download=true\",\n",
    "    compression=\"gzip\",\n",
    ")\n",
    "\n",
    "df_trx_train = pd.merge(pd.DataFrame(df_trx), df_target_train[\"client_id\"], on=\"client_id\", how=\"inner\")\n",
    "df_trx_valid = pd.merge(pd.DataFrame(df_trx), df_target_valid[\"client_id\"], on=\"client_id\", how=\"inner\")\n",
    "df_trx_test = pd.merge(pd.DataFrame(df_trx), df_target_test[\"client_id\"], on=\"client_id\", how=\"inner\")\n",
    "print(\"Split {} transactions to train: {}, valid: {}, test: {}\".format(\n",
    "    *[len(df) for df in [df_trx, df_trx_train, df_trx_valid, df_trx_test]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Dask Server\n",
      "Link Dask Server - http://10.32.15.89:8787/status\n",
      "Record in dataset, train 20000, valid 3000, test 7000\n",
      "Each record is a client with list of transactions\n"
     ]
    }
   ],
   "source": [
    "preprocessor = PandasDataPreprocessor(\n",
    "    col_id=\"client_id\",\n",
    "    col_event_time=\"trans_date\",\n",
    "    event_time_transformation=\"none\",\n",
    "    cols_category=[\"small_group\"],\n",
    "    cols_numerical=[\"amount_rur\"],\n",
    "    return_records=False,\n",
    ")\n",
    "df_data_train = preprocessor.fit_transform(df_trx_train)\n",
    "df_data_valid = preprocessor.transform(df_trx_valid)\n",
    "df_data_test = preprocessor.transform(df_trx_test)\n",
    "\n",
    "print(\n",
    "    \"Record in dataset, train {}, valid {}, test {}\".format(\n",
    "        *[len(df) for df in [df_data_train, df_data_valid, df_data_test]]\n",
    "    )\n",
    ")\n",
    "print(\"Each record is a client with list of transactions\")\n",
    "\n",
    "df_target = df_target.rename(columns={\"bins\": \"target_bin\"})\n",
    "\n",
    "df_data_train = pd.merge(pd.DataFrame(df_data_train), df_target, on=\"client_id\")\n",
    "df_data_valid = pd.merge(pd.DataFrame(df_data_valid), df_target, on=\"client_id\")\n",
    "df_data_test = pd.merge(pd.DataFrame(df_data_test), df_target, on=\"client_id\")\n",
    "\n",
    "df_data_train = df_data_train.to_dict(orient=\"records\")\n",
    "df_data_valid = df_data_valid.to_dict(orient=\"records\")\n",
    "df_data_test = df_data_test.to_dict(orient=\"records\")\n",
    "\n",
    "# show first 10 transactions from one record\n",
    "rec = df_data_train[0]\n",
    "{k: v[:10] if type(v) is torch.Tensor else v for k, v in rec.items()}\n",
    "\n",
    "dataset_train = MemoryMapDataset(df_data_train)\n",
    "dataset_valid = MemoryMapDataset(df_data_valid)\n",
    "dataset_test = MemoryMapDataset(df_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_encoder = RnnSeqEncoder(\n",
    "    trx_encoder=TrxEncoder(\n",
    "        embeddings={\n",
    "            \"small_group\": {\"in\": 150, \"out\": 32},\n",
    "        },\n",
    "        numeric_values={\n",
    "            \"amount_rur\": \"log\",\n",
    "        },\n",
    "        embeddings_noise=0.001,\n",
    "    ),\n",
    "    hidden_size=48,\n",
    ")\n",
    "sup_module = SequenceToTarget(\n",
    "    seq_encoder=seq_encoder,\n",
    "    head=Head(input_size=seq_encoder.embedding_size, objective=\"classification\", num_classes=4),\n",
    "    loss=torch.nn.NLLLoss(),\n",
    "    metric_list=torchmetrics.Accuracy(task=\"multiclass\", num_classes=4),\n",
    "    optimizer_partial=partial(torch.optim.Adam),\n",
    "    lr_scheduler_partial=partial(torch.optim.lr_scheduler.StepLR, step_size=4, gamma=0.5),\n",
    ")\n",
    "sup_data = PtlsDataModule(\n",
    "    train_data=SeqToTargetDataset(dataset_train, target_col_name=\"target_bin\", target_dtype=torch.long),\n",
    "    valid_data=SeqToTargetDataset(dataset_valid, target_col_name=\"target_bin\", target_dtype=torch.long),\n",
    "    test_data=SeqToTargetDataset(dataset_test, target_col_name=\"target_bin\", target_dtype=torch.long),\n",
    "    train_batch_size=128,\n",
    "    valid_batch_size=1024,\n",
    "    train_num_workers=8,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch = next(iter(sup_data.train_dataloader()))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_module.seq_encoder.trx_encoder.qconfig = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/observer.py:1204: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "qm = post_quantize_model(sup_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.ao.quantization\n",
    "\n",
    "\n",
    "class MultipleQuantize(torch.ao.nn.quantized.modules.Quantize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m      \u001b[0mqm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mType:\u001b[0m           Quantize\n",
      "\u001b[0;31mString form:\u001b[0m    Quantize(scale=tensor([1.]), zero_point=tensor([0]), dtype=torch.quint8)\n",
      "\u001b[0;31mFile:\u001b[0m           ~/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/nn/quantized/modules/__init__.py\n",
      "\u001b[0;31mSource:\u001b[0m        \n",
      "\u001b[0;32mclass\u001b[0m \u001b[0mQuantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34mr\"\"\"Quantizes an incoming tensor\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m     `scale`: scale of the output Quantized Tensor\u001b[0m\n",
      "\u001b[0;34m     `zero_point`: zero_point of output Quantized Tensor\u001b[0m\n",
      "\u001b[0;34m     `dtype`: data type of output Quantized Tensor\u001b[0m\n",
      "\u001b[0;34m     `factory_kwargs`: Dictionary of kwargs used for configuring initialization\u001b[0m\n",
      "\u001b[0;34m         of internal buffers. Currently, `device` and `dtype` are supported.\u001b[0m\n",
      "\u001b[0;34m         Example: `factory_kwargs={'device': 'cuda', 'dtype': torch.float64}`\u001b[0m\n",
      "\u001b[0;34m         will initialize internal buffers as type `torch.float64` on the current CUDA device.\u001b[0m\n",
      "\u001b[0;34m         Note that `dtype` only applies to floating-point buffers.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Examples::\u001b[0m\n",
      "\u001b[0;34m        >>> t = torch.tensor([[1., -1.], [1., -1.]])\u001b[0m\n",
      "\u001b[0;34m        >>> scale, zero_point, dtype = 1.0, 2, torch.qint8\u001b[0m\n",
      "\u001b[0;34m        >>> qm = Quantize(scale, zero_point, dtype)\u001b[0m\n",
      "\u001b[0;34m        >>> # xdoctest: +SKIP\u001b[0m\n",
      "\u001b[0;34m        >>> qt = qm(t)\u001b[0m\n",
      "\u001b[0;34m        >>> print(qt)\u001b[0m\n",
      "\u001b[0;34m        tensor([[ 1., -1.],\u001b[0m\n",
      "\u001b[0;34m                [ 1., -1.]], size=(2, 2), dtype=torch.qint8, scale=1.0, zero_point=2)\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mscale\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mzero_point\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_point\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactory_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mfactory_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQuantize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'scale'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'zero_point'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mzero_point\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                                          \u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfactory_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'dtype'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize_per_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                                         \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mfrom_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32massert\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'activation_post_process'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_post_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_qparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mQuantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_point\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_post_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m'scale={}, zero_point={}, dtype={}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_point\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule."
     ]
    }
   ],
   "source": [
    "??qm.quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "quantize_per_tensor() received an invalid combination of arguments - got (PaddedBatch, float, int, torch.dtype), but expected one of:\n * (Tensor input, Tensor scale, Tensor zero_point, torch.dtype dtype)\n      didn't match because some of the arguments have invalid types: (!PaddedBatch!, !float!, !int!, torch.dtype)\n * (Tensor input, float scale, int zero_point, torch.dtype dtype)\n      didn't match because some of the arguments have invalid types: (!PaddedBatch!, float, int, torch.dtype)\n * (tuple of Tensors tensors, Tensor scales, Tensor zero_points, torch.dtype dtype)\n      didn't match because some of the arguments have invalid types: (!PaddedBatch!, !float!, !int!, torch.dtype)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mqm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_batch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/stubs.py:62\u001b[0m, in \u001b[0;36mQuantWrapper.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m---> 62\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(X)\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdequant(X)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/nn/quantized/modules/__init__.py:63\u001b[0m, in \u001b[0;36mQuantize.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize_per_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m                                     \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_point\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: quantize_per_tensor() received an invalid combination of arguments - got (PaddedBatch, float, int, torch.dtype), but expected one of:\n * (Tensor input, Tensor scale, Tensor zero_point, torch.dtype dtype)\n      didn't match because some of the arguments have invalid types: (!PaddedBatch!, !float!, !int!, torch.dtype)\n * (Tensor input, float scale, int zero_point, torch.dtype dtype)\n      didn't match because some of the arguments have invalid types: (!PaddedBatch!, float, int, torch.dtype)\n * (tuple of Tensors tensors, Tensor scales, Tensor zero_points, torch.dtype dtype)\n      didn't match because some of the arguments have invalid types: (!PaddedBatch!, !float!, !int!, torch.dtype)\n"
     ]
    }
   ],
   "source": [
    "qm(example_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_quantize_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compression trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1+cu118'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'arange'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 38\u001b[0m\n\u001b[1;32m     34\u001b[0m model_to_quantize\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     36\u001b[0m example_inputs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(data_module\u001b[38;5;241m.\u001b[39mtrain_dataloader()))[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 38\u001b[0m prepared_model \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_to_quantize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqconfig_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# print(\"prepared model:\", prepared_model)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m quantized_model \u001b[38;5;241m=\u001b[39m convert_fx(prepared_model)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize_fx.py:382\u001b[0m, in \u001b[0;36mprepare_fx\u001b[0;34m(model, qconfig_mapping, example_inputs, prepare_custom_config, _equalization_config, backend_config)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\" Prepare a model for post training quantization\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;124;03m    calibrate(prepared_model, sample_inference_data)\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    381\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_once(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_api.quantize_fx.prepare_fx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_prepare_fx\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqconfig_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# is_qat\u001b[39;49;00m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprepare_custom_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_equalization_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize_fx.py:135\u001b[0m, in \u001b[0;36m_prepare_fx\u001b[0;34m(model, qconfig_mapping, is_qat, example_inputs, prepare_custom_config, _equalization_config, backend_config, is_standalone_module)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# symbolically trace the model\u001b[39;00m\n\u001b[1;32m    134\u001b[0m tracer \u001b[38;5;241m=\u001b[39m QuantizationTracer(skipped_module_names, skipped_module_classes)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m graph_module \u001b[38;5;241m=\u001b[39m GraphModule(model, \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    136\u001b[0m _attach_meta_to_node_if_not_exist(graph_module)\n\u001b[1;32m    138\u001b[0m fuse_custom_config \u001b[38;5;241m=\u001b[39m FuseCustomConfig()\u001b[38;5;241m.\u001b[39mset_preserved_attributes(prepare_custom_config\u001b[38;5;241m.\u001b[39mpreserved_attributes)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/fx/_symbolic_trace.py:784\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    775\u001b[0m patcher\u001b[38;5;241m.\u001b[39mpatch_method(\n\u001b[1;32m    776\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getattr__\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    778\u001b[0m     module_getattr_wrapper,\n\u001b[1;32m    779\u001b[0m     deduplicate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    780\u001b[0m )\n\u001b[1;32m    781\u001b[0m patcher\u001b[38;5;241m.\u001b[39mpatch_method(\n\u001b[1;32m    782\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__call__\u001b[39m\u001b[38;5;124m\"\u001b[39m, module_call_wrapper, deduplicate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    783\u001b[0m )\n\u001b[0;32m--> 784\u001b[0m \u001b[43m_patch_wrapped_functions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    785\u001b[0m _autowrap_check(patcher, fn_globals, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids)\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_search:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/fx/_symbolic_trace.py:1054\u001b[0m, in \u001b[0;36m_patch_wrapped_functions\u001b[0;34m(patcher)\u001b[0m\n\u001b[1;32m   1052\u001b[0m         orig_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(builtins, name)\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1054\u001b[0m         orig_fn \u001b[38;5;241m=\u001b[39m \u001b[43mframe_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1055\u001b[0m     patcher\u001b[38;5;241m.\u001b[39mpatch(frame_dict, name, _create_wrapped_func(orig_fn))\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m, name \u001b[38;5;129;01min\u001b[39;00m _wrapped_methods_to_patch:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'arange'"
     ]
    }
   ],
   "source": [
    "from torch.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "from torch.ao.quantization import (\n",
    "    default_dynamic_qconfig, \n",
    "    float_qparams_weight_only_qconfig, \n",
    "    QConfigMapping, \n",
    "    per_channel_dynamic_qconfig,\n",
    "    float16_static_qconfig\n",
    ")\n",
    "from ptls.nn.trx_encoder.noisy_embedding import NoisyEmbedding\n",
    "# Full docs for supported qconfig for floating point modules/ops can be found in `quantization docs <https://pytorch.org/docs/stable/quantization.html#module-torch.quantization>`_\n",
    "# Full docs for `QConfigMapping <https://pytorch.org/docs/stable/generated/torch.ao.quantization.qconfig_mapping.QConfigMapping.html#torch.ao.quantization.qconfig_mapping.QConfigMapping>`_\n",
    "qconfig_mapping = (QConfigMapping()\n",
    "                   .set_object_type(torch.nn.Embedding, float_qparams_weight_only_qconfig)\n",
    "                #    .set_global(float_qparams_weight_only_qconfig)\n",
    "    # .set_object_type(nn.Embedding, float_qparams_weight_only_qconfig)\n",
    "    # .set_object_type(nn.LSTM, default_dynamic_qconfig)\n",
    "    # .set_module_name(nn.Conv2d, float16_static_qconfig)\n",
    "    # .set_object_type(nn.Linear, default_dynamic_qconfig)\n",
    "    # .set_object_type(TestLinear, float_qparams_weight_only_qconfig)\n",
    ")\n",
    "# Load model to create the original model because quantization api changes the model inplace and we want\n",
    "# to keep the original model for future comparison\n",
    "\n",
    "\n",
    "model_to_quantize = model\n",
    "\n",
    "# model_to_quantize.load_state_dict(\n",
    "#     torch.load(\n",
    "#         model_data_filepath + 'word_language_model_quantize.pth',\n",
    "#         map_location=torch.device('cpu')\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "model_to_quantize.eval()\n",
    "\n",
    "example_inputs = (next(iter(data_module.train_dataloader()))[0])\n",
    "\n",
    "prepared_model = prepare_fx(model_to_quantize, qconfig_mapping, example_inputs)\n",
    "\n",
    "# print(\"prepared model:\", prepared_model)\n",
    "quantized_model = convert_fx(prepared_model)\n",
    "# print(\"quantized model\", quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedcore.models.network_impl.layers import IDecomposed\n",
    "\n",
    "def _recreate_embedding(module):\n",
    "        assert isinstance(module, torch.nn.Embedding)\n",
    "        new = torch.nn.Embedding(\n",
    "            module.num_embeddings,\n",
    "            module.embedding_dim,\n",
    "            module.padding_idx,\n",
    "            module.max_norm,\n",
    "            module.norm_type,\n",
    "            module.scale_grad_by_freq,\n",
    "            module.sparse,\n",
    "            module.weight,\n",
    "            getattr(module, 'freeze', None),\n",
    "            getattr(module.weight, 'device', torch.device('cpu')),\n",
    "            getattr(module.weight, 'dtype', torch.float32)\n",
    "        )\n",
    "        return new\n",
    "\n",
    "def _recreate_linear(module):\n",
    "    assert isinstance(module, torch.nn.Linear)\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "class ParentalReassembler:    \n",
    "    supported_layers = {torch.nn.Embedding: _recreate_embedding,\n",
    "                        torch.nn.Linear: _recreate_linear}\n",
    "    \n",
    "    @classmethod\n",
    "    def set_layer(cls, model, name, module):\n",
    "        ### TODO check Sequential\n",
    "        *path, name = name.split('.')\n",
    "        current_module = model\n",
    "        for child in path:\n",
    "            current_module = getattr(current_module, child)\n",
    "        setattr(current_module, name, module)\n",
    "            \n",
    "    \n",
    "    @classmethod\n",
    "    def fetch_module(cls, module):\n",
    "        is_decomposed = isinstance(module, IDecomposed)\n",
    "        for supported in cls.supported_layers:\n",
    "            if isinstance(module, supported):\n",
    "                return supported, is_decomposed\n",
    "        return None, is_decomposed\n",
    "    \n",
    "    @staticmethod\n",
    "    def decomposed_handle(*args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @classmethod\n",
    "    def handle(cls, module, type):\n",
    "        return cls.supported_layers[type](module)\n",
    "\n",
    "    @classmethod\n",
    "    def convert(cls, module):\n",
    "        associated, is_decomp = cls.fetch_module(module)\n",
    "        if associated is None:\n",
    "            print('failed to fetch:', module.__class__.__name__)\n",
    "            return None\n",
    "        if is_decomp:\n",
    "            new_module = cls.decomposed_handle(module)\n",
    "        else:\n",
    "            new_module = cls.handle(module, associated)\n",
    "        return new_module\n",
    "    \n",
    "    @classmethod\n",
    "    def reassemble(cls, model):\n",
    "        for name, module in model.named_modules():\n",
    "            print(name)\n",
    "            new_module = cls.convert(module)\n",
    "            if new_module is None:\n",
    "                continue\n",
    "            cls.set_layer(model, name, new_module)\n",
    "        return model\n",
    "\n",
    "\n",
    "ParentalReassembler.reassemble(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.fedcore_compression.fc_setups import TEST\n",
    "QUANT = TEST\n",
    "qkey = 'training_aware_quant'\n",
    "# qkey = 'post_training_quant'\n",
    "QUANT['initial_assumption'] = [qkey]\n",
    "QUANT['model_params'][qkey] = {'epochs': 1}\n",
    "QUANT['distributed_compression'] = False\n",
    "\n",
    "QUANT['common']['save_each'] = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fedcore.api.main.FedCore at 0x7fe762047f70>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fedcore.api.main import FedCore\n",
    "from fedcore.api.utils.data import get_compression_input\n",
    "\n",
    "compinp = get_compression_input(model, train_dataloader=data_module.train_dataloader(), \n",
    "                                calib_dataloader=data_module.train_dataloader(), num_classes=10)\n",
    "# fcomp = FedCore(compression_task='composite_compression',\n",
    "#                 need_evo_opt=False,\n",
    "                \n",
    "#                 initial_assumption=['training_aware_quant'])\n",
    "fcomp = FedCore(**QUANT, need_evo_opt=False)\n",
    "\n",
    "fcomp  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcomp.fit((compinp, model), manually_done=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedcore.neural_compressor.config import PostTrainingQuantConfig\n",
    "from fedcore.neural_compressor import quantization\n",
    "conf = PostTrainingQuantConfig(\n",
    "    # approach=\"weight_only\",\n",
    "    # op_type_dict={\n",
    "    #     \".*\": {  # re.match\n",
    "    #         \"weight\": {\n",
    "    #             \"bits\": 8,  # 1-8 bit\n",
    "    #             \"group_size\": -1,  # -1 (per-channel)\n",
    "    #             \"scheme\": \"sym\",\n",
    "    #             \"algorithm\": \"RTN\",\n",
    "    #         },\n",
    "    #     },\n",
    "    # },\n",
    "    # recipes={\n",
    "    #     # 'rtn_args':{'enable_full_range': True, 'enable_mse_search': True},\n",
    "    #     # 'gptq_args':{'percdamp': 0.01, 'actorder':True, 'block_size': 128, 'nsamples': 128, 'use_full_length': False},\n",
    "    #     # 'awq_args':{'enable_auto_scale': True, 'enable_mse_search': True, 'n_blocks': 5},\n",
    "    # },\n",
    ")\n",
    "eval_func = lambda *args, **kwargs: 0.9\n",
    "q_model = quantization.fit(model, conf, eval_func=eval_func, calib_dataloader=trainloader)\n",
    "q_model.save(\"saved_results\")\n",
    "compressed_model = q_model.export_compressed_model()\n",
    "torch.save(compressed_model, \"compressed_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedcore.neural_compressor.config import PostTrainingQuantConfig\n",
    "from fedcore.neural_compressor import quantization\n",
    "conf = PostTrainingQuantConfig(\n",
    "    # approach=\"weight_only\",\n",
    "    # op_type_dict={\n",
    "    #     \".*\": {  # re.match\n",
    "    #         \"weight\": {\n",
    "    #             \"bits\": 8,  # 1-8 bit\n",
    "    #             \"group_size\": -1,  # -1 (per-channel)\n",
    "    #             \"scheme\": \"sym\",\n",
    "    #             \"algorithm\": \"RTN\",\n",
    "    #         },\n",
    "    #     },\n",
    "    # },\n",
    "    # recipes={\n",
    "    #     # 'rtn_args':{'enable_full_range': True, 'enable_mse_search': True},\n",
    "    #     # 'gptq_args':{'percdamp': 0.01, 'actorder':True, 'block_size': 128, 'nsamples': 128, 'use_full_length': False},\n",
    "    #     # 'awq_args':{'enable_auto_scale': True, 'enable_mse_search': True, 'n_blocks': 5},\n",
    "    # },\n",
    ")\n",
    "eval_func = lambda *args, **kwargs: 0.9\n",
    "q_model = quantization.fit(model, conf, eval_func=eval_func, calib_dataloader=data_module.train_dataloader())\n",
    "q_model.save(\"saved_results\")\n",
    "compressed_model = q_model.export_compressed_model()\n",
    "torch.save(compressed_model, \"compressed_model.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptls-experiments-H-SwwRmK",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
