
>>> PEFT step 4: quant_qat
Creating Dask Server
Generations:   0%|          | 0/10000 [00:00<?, ?gen/s][QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[MODEL] Model initialized and copied for quantization.
Module: , qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: maxpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: avgpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: fc, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
[DATA] Example input shape: torch.Size([64, 3, 32, 32])
[PREPARE] Model prepared successfully.
[HOOK] Performing Dynamic PTQ hook operations.
[HOOK] Dynamic PTQ setup completed.
[FIT] Quantization performed successfully.
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[MODEL] Model initialized and copied for quantization.
Module: , qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: maxpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: avgpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: fc, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
[DATA] Example input shape: torch.Size([64, 3, 32, 32])
[PREPARE] Model prepared successfully.
[FIT ERROR] Exception during quantization:
Traceback (most recent call last):
  File "/ptls-experiments/FedCore/fedcore/algorithm/quantization/quantizers.py", line 213, in fit
    convert(self.quant_model, inplace=True)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 563, in convert
    _convert(
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 604, in _convert
    _convert(mod, mapping, True,  # inplace
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 607, in _convert
    reassign[name] = swap_module(mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 642, in swap_module
    new_mod = qmod.from_float(mod, use_precomputed_fake_quant=use_precomputed_fake_quant)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/nn/quantized/modules/batchnorm.py", line 77, in from_float
    return _BatchNorm.from_float(cls, mod, use_precomputed_fake_quant=use_precomputed_fake_quant)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/nn/quantized/modules/batchnorm.py", line 22, in from_float
    scale, zero_point = activation_post_process.calculate_qparams()
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/observer.py", line 1459, in calculate_qparams
    raise Exception(  # noqa: TRY002
Exception: calculate_qparams should not be called for PlaceholderObserver
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[MODEL] Model initialized and copied for quantization.
Module: , qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: maxpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: avgpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: fc, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
[DATA] Example input shape: torch.Size([64, 3, 32, 32])
[PREPARE] Model prepared successfully.
[HOOK] Performing Dynamic PTQ hook operations.
[HOOK] Dynamic PTQ setup completed.
[FIT] Quantization performed successfully.
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: qat, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})}
[MODEL] Model initialized and copied for quantization.
Module: , qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600c159940>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600c159940>})
Module: conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600e53f790>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600e53f790>})
Module: bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a183820>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a183820>})
Module: relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a183430>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a183430>})
Module: maxpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a183ca0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a183ca0>})
Module: layer1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a183c10>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a183c10>})
Module: layer1.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a183790>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a183790>})
Module: layer1.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a183af0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a183af0>})
Module: layer1.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f5ffc704700>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f5ffc704700>})
Module: layer1.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f5ffc704040>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f5ffc704040>})
Module: layer1.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f5ffc704160>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f5ffc704160>})
Module: layer1.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f61b3b78700>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f61b3b78700>})
Module: layer1.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b49040>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b49040>})
Module: layer1.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60127565e0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60127565e0>})
Module: layer1.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012756c10>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012756c10>})
Module: layer1.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a3a0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a3a0>})
Module: layer1.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a790>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a790>})
Module: layer1.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a430>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a430>})
Module: layer2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a550>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a550>})
Module: layer2.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72ac10>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72ac10>})
Module: layer2.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a0d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a0d0>})
Module: layer2.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72ae50>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72ae50>})
Module: layer2.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72adc0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72adc0>})
Module: layer2.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a940>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a940>})
Module: layer2.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a9d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a9d0>})
Module: layer2.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a700>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a700>})
Module: layer2.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a5e0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a5e0>})
Module: layer2.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a040>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a040>})
Module: layer2.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a1f0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a1f0>})
Module: layer2.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72aa60>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72aa60>})
Module: layer2.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72ad30>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72ad30>})
Module: layer2.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a310>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a310>})
Module: layer2.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72aee0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72aee0>})
Module: layer2.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72aca0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72aca0>})
Module: layer3, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a8b0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a8b0>})
Module: layer3.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a4c0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a4c0>})
Module: layer3.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b689d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b689d0>})
Module: layer3.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68790>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68790>})
Module: layer3.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b688b0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b688b0>})
Module: layer3.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68b80>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68b80>})
Module: layer3.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68670>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68670>})
Module: layer3.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68e50>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68e50>})
Module: layer3.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68940>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68940>})
Module: layer3.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68c10>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68c10>})
Module: layer3.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b681f0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b681f0>})
Module: layer3.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68700>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68700>})
Module: layer3.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68a60>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68a60>})
Module: layer3.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1430>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1430>})
Module: layer3.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1ca0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1ca0>})
Module: layer3.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1820>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1820>})
Module: layer4, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1670>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1670>})
Module: layer4.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e14c0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e14c0>})
Module: layer4.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1c10>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1c10>})
Module: layer4.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1b80>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1b80>})
Module: layer4.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1040>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1040>})
Module: layer4.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1a60>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1a60>})
Module: layer4.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1e50>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1e50>})
Module: layer4.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e19d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e19d0>})
Module: layer4.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e13a0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e13a0>})
Module: layer4.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1790>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1790>})
Module: layer4.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1940>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1940>})
Module: layer4.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1f70>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1f70>})
Module: layer4.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1ee0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1ee0>})
Module: layer4.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1af0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1af0>})
Module: layer4.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e18b0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e18b0>})
Module: layer4.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1160>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1160>})
Module: avgpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1310>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1310>})
Module: fc, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1dc0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1dc0>})
[DATA] Example input shape: torch.Size([64, 3, 32, 32])
[PREPARE] Model prepared successfully.
[HOOK] Performing QAT training hook.
[HOOK] QAT epoch 1/5 started.
[HOOK][Epoch: 1] Batch 1 Loss: 1.2999
[HOOK][Epoch: 1] Batch 51 Loss: 0.3033
[HOOK][Epoch: 1] Batch 101 Loss: 0.5700
[HOOK][Epoch: 1] Batch 151 Loss: 0.3721
[HOOK][Epoch: 1] Batch 201 Loss: 0.4178
[HOOK][Epoch: 1] Batch 251 Loss: 0.4867
[HOOK][Epoch: 1] Batch 301 Loss: 0.5227
[HOOK][Epoch: 1] Batch 351 Loss: 0.6051
[HOOK][Epoch: 1] Batch 401 Loss: 0.3540
[HOOK][Epoch: 1] Batch 451 Loss: 0.7001
[HOOK][Epoch: 1] Batch 501 Loss: 0.3930
[HOOK][Epoch: 1] Batch 551 Loss: 0.7452
[HOOK][Epoch: 1] Batch 601 Loss: 0.4874
[HOOK] QAT epoch 2/5 started.
[HOOK][Epoch: 2] Batch 1 Loss: 0.2060
[HOOK][Epoch: 2] Batch 51 Loss: 0.1502
[HOOK][Epoch: 2] Batch 101 Loss: 0.2041
[HOOK][Epoch: 2] Batch 151 Loss: 0.3298
[HOOK][Epoch: 2] Batch 201 Loss: 0.2159
[HOOK][Epoch: 2] Batch 251 Loss: 0.4516
[HOOK][Epoch: 2] Batch 301 Loss: 0.2350
[HOOK][Epoch: 2] Batch 351 Loss: 0.2712
[HOOK][Epoch: 2] Batch 401 Loss: 0.2615
[HOOK][Epoch: 2] Batch 451 Loss: 0.5491
[HOOK][Epoch: 2] Batch 501 Loss: 0.5014
[HOOK][Epoch: 2] Batch 551 Loss: 0.2596
[HOOK][Epoch: 2] Batch 601 Loss: 0.2030
[HOOK] QAT epoch 3/5 started.
[HOOK][Epoch: 3] Batch 1 Loss: 0.1713
[HOOK][Epoch: 3] Batch 51 Loss: 0.1464
[HOOK][Epoch: 3] Batch 101 Loss: 0.2267
[HOOK][Epoch: 3] Batch 151 Loss: 0.1593
[HOOK][Epoch: 3] Batch 201 Loss: 0.3333
[HOOK][Epoch: 3] Batch 251 Loss: 0.2197
[HOOK][Epoch: 3] Batch 301 Loss: 0.0985
[HOOK][Epoch: 3] Batch 351 Loss: 0.2284
[HOOK][Epoch: 3] Batch 401 Loss: 0.3295
[HOOK][Epoch: 3] Batch 451 Loss: 0.2345
[HOOK][Epoch: 3] Batch 501 Loss: 0.2874
[HOOK][Epoch: 3] Batch 551 Loss: 0.1965
[HOOK][Epoch: 3] Batch 601 Loss: 0.2386
[HOOK] QAT epoch 4/5 started.
[HOOK][Epoch: 4] Batch 1 Loss: 0.1795
[HOOK][Epoch: 4] Batch 51 Loss: 0.1265
[HOOK][Epoch: 4] Batch 101 Loss: 0.1927
[HOOK][Epoch: 4] Batch 151 Loss: 0.1428
[HOOK][Epoch: 4] Batch 201 Loss: 0.2679
[HOOK][Epoch: 4] Batch 251 Loss: 0.0600
[HOOK][Epoch: 4] Batch 301 Loss: 0.0541
[HOOK][Epoch: 4] Batch 351 Loss: 0.1273
[HOOK][Epoch: 4] Batch 401 Loss: 0.3034
[HOOK][Epoch: 4] Batch 451 Loss: 0.0995
[HOOK][Epoch: 4] Batch 501 Loss: 0.2602
[HOOK][Epoch: 4] Batch 551 Loss: 0.2665
[HOOK][Epoch: 4] Batch 601 Loss: 0.2570
[HOOK] QAT epoch 5/5 started.
[HOOK][Epoch: 5] Batch 1 Loss: 0.2235
[HOOK][Epoch: 5] Batch 51 Loss: 0.1239
[HOOK][Epoch: 5] Batch 101 Loss: 0.1142
[HOOK][Epoch: 5] Batch 151 Loss: 0.2039
[HOOK][Epoch: 5] Batch 201 Loss: 0.1002
[HOOK][Epoch: 5] Batch 251 Loss: 0.3508
[HOOK][Epoch: 5] Batch 301 Loss: 0.1465
[HOOK][Epoch: 5] Batch 351 Loss: 0.1520
[HOOK][Epoch: 5] Batch 401 Loss: 0.0988
[HOOK][Epoch: 5] Batch 451 Loss: 0.0536
[HOOK][Epoch: 5] Batch 501 Loss: 0.2907
[HOOK][Epoch: 5] Batch 551 Loss: 0.1564
[HOOK][Epoch: 5] Batch 601 Loss: 0.0675
[HOOK] QAT training completed.
[FIT] Quantization performed successfully.
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: qat, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})}
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[MODEL] Model initialized and copied for quantization.
Module: , qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: maxpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: avgpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: fc, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
[DATA] Example input shape: torch.Size([64, 3, 32, 32])
[PREPARE] Model prepared successfully.
[FIT ERROR] Exception during quantization:
Traceback (most recent call last):
  File "/ptls-experiments/FedCore/fedcore/algorithm/quantization/quantizers.py", line 213, in fit
    convert(self.quant_model, inplace=True)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 563, in convert
    _convert(
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 604, in _convert
    _convert(mod, mapping, True,  # inplace
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 607, in _convert
    reassign[name] = swap_module(mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 642, in swap_module
    new_mod = qmod.from_float(mod, use_precomputed_fake_quant=use_precomputed_fake_quant)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/nn/quantized/modules/batchnorm.py", line 77, in from_float
    return _BatchNorm.from_float(cls, mod, use_precomputed_fake_quant=use_precomputed_fake_quant)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/nn/quantized/modules/batchnorm.py", line 22, in from_float
    scale, zero_point = activation_post_process.calculate_qparams()
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/observer.py", line 1459, in calculate_qparams
    raise Exception(  # noqa: TRY002
Exception: calculate_qparams should not be called for PlaceholderObserver
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[MODEL] Model initialized and copied for quantization.
Module: , qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: maxpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: avgpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: fc, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
[DATA] Example input shape: torch.Size([64, 3, 32, 32])
[PREPARE] Model prepared successfully.
[HOOK] Performing Dynamic PTQ hook operations.
[HOOK] Dynamic PTQ setup completed.
[FIT] Quantization performed successfully.
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[MODEL] Model initialized and copied for quantization.
Module: , qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: maxpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: avgpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: fc, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
[DATA] Example input shape: torch.Size([64, 3, 32, 32])
[PREPARE] Model prepared successfully.
[FIT ERROR] Exception during quantization:
Traceback (most recent call last):
  File "/ptls-experiments/FedCore/fedcore/algorithm/quantization/quantizers.py", line 213, in fit
    convert(self.quant_model, inplace=True)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 563, in convert
    _convert(
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 604, in _convert
    _convert(mod, mapping, True,  # inplace
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 607, in _convert
    reassign[name] = swap_module(mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 642, in swap_module
    new_mod = qmod.from_float(mod, use_precomputed_fake_quant=use_precomputed_fake_quant)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/nn/quantized/modules/batchnorm.py", line 77, in from_float
    return _BatchNorm.from_float(cls, mod, use_precomputed_fake_quant=use_precomputed_fake_quant)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/nn/quantized/modules/batchnorm.py", line 22, in from_float
    scale, zero_point = activation_post_process.calculate_qparams()
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/observer.py", line 1459, in calculate_qparams
    raise Exception(  # noqa: TRY002
Exception: calculate_qparams should not be called for PlaceholderObserver
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: qat, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})}
[MODEL] Model initialized and copied for quantization.
Module: , qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1ee0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1ee0>})
Module: conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1790>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1790>})
Module: bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1670>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1670>})
Module: relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1dc0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1dc0>})
Module: maxpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e6665e0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e6665e0>})
Module: layer1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600e3403a0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600e3403a0>})
Module: layer1.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1160>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1160>})
Module: layer1.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1ee0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1ee0>})
Module: layer1.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1040>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1040>})
Module: layer1.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1550>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1550>})
Module: layer1.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1430>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1430>})
Module: layer1.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1af0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1af0>})
Module: layer1.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d10d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d10d0>})
Module: layer1.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1820>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1820>})
Module: layer1.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1790>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1790>})
Module: layer1.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1a60>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1a60>})
Module: layer1.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1160>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1160>})
Module: layer1.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60923bcb80>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60923bcb80>})
Module: layer2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60923bcaf0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60923bcaf0>})
Module: layer2.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e666430>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e666430>})
Module: layer2.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e666af0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e666af0>})
Module: layer2.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e15e0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e15e0>})
Module: layer2.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1820>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1820>})
Module: layer2.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1f70>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1f70>})
Module: layer2.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1e50>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1e50>})
Module: layer2.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1940>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1940>})
Module: layer2.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1c10>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1c10>})
Module: layer2.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1b80>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1b80>})
Module: layer2.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a1f0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a1f0>})
Module: layer2.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a9d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a9d0>})
Module: layer2.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72ae50>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72ae50>})
Module: layer2.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a8b0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a8b0>})
Module: layer2.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72aa60>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72aa60>})
Module: layer2.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a550>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a550>})
Module: layer3, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60127565e0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60127565e0>})
Module: layer3.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1ca0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1ca0>})
Module: layer3.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a183c10>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a183c10>})
Module: layer3.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a183ca0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a183ca0>})
Module: layer3.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a183820>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a183820>})
Module: layer3.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d19d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d19d0>})
Module: layer3.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1f70>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1f70>})
Module: layer3.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1a60>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1a60>})
Module: layer3.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d18b0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d18b0>})
Module: layer3.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1ca0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1ca0>})
Module: layer3.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d15e0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d15e0>})
Module: layer3.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d14c0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d14c0>})
Module: layer3.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1b80>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1b80>})
Module: layer3.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1940>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1940>})
Module: layer3.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1c10>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1c10>})
Module: layer3.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1e50>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1e50>})
Module: layer4, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f615a1b4ca0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f615a1b4ca0>})
Module: layer4.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f615a1b4dc0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f615a1b4dc0>})
Module: layer4.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f615a1b4940>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f615a1b4940>})
Module: layer4.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356280>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356280>})
Module: layer4.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356d30>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356d30>})
Module: layer4.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356b80>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356b80>})
Module: layer4.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60123565e0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60123565e0>})
Module: layer4.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60123568b0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60123568b0>})
Module: layer4.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356700>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356700>})
Module: layer4.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356f70>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356f70>})
Module: layer4.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356310>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356310>})
Module: layer4.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60123564c0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60123564c0>})
Module: layer4.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356940>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356940>})
Module: layer4.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356af0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356af0>})
Module: layer4.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356c10>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356c10>})
Module: layer4.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356dc0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356dc0>})
Module: avgpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356160>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356160>})
Module: fc, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60123560d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60123560d0>})
[DATA] Example input shape: torch.Size([64, 3, 32, 32])
[PREPARE] Model prepared successfully.
[HOOK] Performing QAT training hook.
[HOOK] QAT epoch 1/5 started.
[HOOK][Epoch: 1] Batch 1 Loss: 1.2833
[HOOK][Epoch: 1] Batch 51 Loss: 0.5557
[HOOK][Epoch: 1] Batch 101 Loss: 0.9963
[HOOK][Epoch: 1] Batch 151 Loss: 0.4431
[HOOK][Epoch: 1] Batch 201 Loss: 0.7282
[HOOK][Epoch: 1] Batch 251 Loss: 0.4292
[HOOK][Epoch: 1] Batch 301 Loss: 0.5519
[HOOK][Epoch: 1] Batch 351 Loss: 0.4402
[HOOK][Epoch: 1] Batch 401 Loss: 0.4901
[HOOK][Epoch: 1] Batch 451 Loss: 0.3448
[HOOK][Epoch: 1] Batch 501 Loss: 0.2673
[HOOK][Epoch: 1] Batch 551 Loss: 0.3550
[HOOK][Epoch: 1] Batch 601 Loss: 0.3695
[HOOK] QAT epoch 2/5 started.
[HOOK][Epoch: 2] Batch 1 Loss: 0.3148
[HOOK][Epoch: 2] Batch 51 Loss: 0.1391
[HOOK][Epoch: 2] Batch 101 Loss: 0.2215
[HOOK][Epoch: 2] Batch 151 Loss: 0.2394
[HOOK][Epoch: 2] Batch 201 Loss: 0.2357
[HOOK][Epoch: 2] Batch 251 Loss: 0.2246
[HOOK][Epoch: 2] Batch 301 Loss: 0.5482
[HOOK][Epoch: 2] Batch 351 Loss: 0.3883
[HOOK][Epoch: 2] Batch 401 Loss: 0.1690
[HOOK][Epoch: 2] Batch 451 Loss: 0.1442
[HOOK][Epoch: 2] Batch 501 Loss: 0.6547
[HOOK][Epoch: 2] Batch 551 Loss: 0.3586
[HOOK][Epoch: 2] Batch 601 Loss: 0.4003
[HOOK] QAT epoch 3/5 started.
[HOOK][Epoch: 3] Batch 1 Loss: 0.0961
[HOOK][Epoch: 3] Batch 51 Loss: 0.1818
[HOOK][Epoch: 3] Batch 101 Loss: 0.2119
[HOOK][Epoch: 3] Batch 151 Loss: 0.3796
[HOOK][Epoch: 3] Batch 201 Loss: 0.2402
[HOOK][Epoch: 3] Batch 251 Loss: 0.2327
[HOOK][Epoch: 3] Batch 301 Loss: 0.0968
[HOOK][Epoch: 3] Batch 351 Loss: 0.2045
[HOOK][Epoch: 3] Batch 401 Loss: 0.1520
[HOOK][Epoch: 3] Batch 451 Loss: 0.2761
[HOOK][Epoch: 3] Batch 501 Loss: 0.2393
[HOOK][Epoch: 3] Batch 551 Loss: 0.1314
[HOOK][Epoch: 3] Batch 601 Loss: 0.1116
[HOOK] QAT epoch 4/5 started.
[HOOK][Epoch: 4] Batch 1 Loss: 0.4581
[HOOK][Epoch: 4] Batch 51 Loss: 0.1348
[HOOK][Epoch: 4] Batch 101 Loss: 0.1683
[HOOK][Epoch: 4] Batch 151 Loss: 0.1860
[HOOK][Epoch: 4] Batch 201 Loss: 0.0840
[HOOK][Epoch: 4] Batch 251 Loss: 0.1721
[HOOK][Epoch: 4] Batch 301 Loss: 0.1299
[HOOK][Epoch: 4] Batch 351 Loss: 0.1797
[HOOK][Epoch: 4] Batch 401 Loss: 0.1556
[HOOK][Epoch: 4] Batch 451 Loss: 0.2439
[HOOK][Epoch: 4] Batch 501 Loss: 0.2348
[HOOK][Epoch: 4] Batch 551 Loss: 0.3973
[HOOK][Epoch: 4] Batch 601 Loss: 0.1523
[HOOK] QAT epoch 5/5 started.
[HOOK][Epoch: 5] Batch 1 Loss: 0.1032
[HOOK][Epoch: 5] Batch 51 Loss: 0.2453
[HOOK][Epoch: 5] Batch 101 Loss: 0.3051
[HOOK][Epoch: 5] Batch 151 Loss: 0.0496
[HOOK][Epoch: 5] Batch 201 Loss: 0.1643
[HOOK][Epoch: 5] Batch 251 Loss: 0.3073
[HOOK][Epoch: 5] Batch 301 Loss: 0.0459
[HOOK][Epoch: 5] Batch 351 Loss: 0.0850
[HOOK][Epoch: 5] Batch 401 Loss: 0.1833
[HOOK][Epoch: 5] Batch 451 Loss: 0.1876
[HOOK][Epoch: 5] Batch 501 Loss: 0.1505
[HOOK][Epoch: 5] Batch 551 Loss: 0.2732
[HOOK][Epoch: 5] Batch 601 Loss: 0.1885
[HOOK] QAT training completed.
[FIT] Quantization performed successfully.
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: qat, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})}
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: qat, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})}
[MODEL] Model initialized and copied for quantization.
Module: , qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600e3403a0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600e3403a0>})
Module: conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60923bcaf0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60923bcaf0>})
Module: bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f615a1b4940>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f615a1b4940>})
Module: relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f615a1b4ca0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f615a1b4ca0>})
Module: maxpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60127565e0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60127565e0>})
Module: layer1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a183ca0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a183ca0>})
Module: layer1.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a183c10>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a183c10>})
Module: layer1.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a1f0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a1f0>})
Module: layer1.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72aa60>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72aa60>})
Module: layer1.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a9d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a9d0>})
Module: layer1.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72ae50>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72ae50>})
Module: layer1.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e666430>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e666430>})
Module: layer1.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e6665e0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e6665e0>})
Module: layer1.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e666af0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e666af0>})
Module: layer1.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d15e0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d15e0>})
Module: layer1.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1c10>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1c10>})
Module: layer1.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d18b0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d18b0>})
Module: layer1.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1b80>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1b80>})
Module: layer2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d14c0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d14c0>})
Module: layer2.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1f70>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1f70>})
Module: layer2.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d19d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d19d0>})
Module: layer2.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d10d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d10d0>})
Module: layer2.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1160>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1160>})
Module: layer2.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1430>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1430>})
Module: layer2.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1790>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1790>})
Module: layer2.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1820>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1820>})
Module: layer2.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1040>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1040>})
Module: layer2.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1ee0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1ee0>})
Module: layer2.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1ee0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1ee0>})
Module: layer2.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1ca0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1ca0>})
Module: layer2.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1c10>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1c10>})
Module: layer2.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1e50>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1e50>})
Module: layer2.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1f70>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1f70>})
Module: layer2.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e15e0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e15e0>})
Module: layer3, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1160>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1160>})
Module: layer3.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1dc0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1dc0>})
Module: layer3.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1670>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1670>})
Module: layer3.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1790>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1790>})
Module: layer3.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60123565e0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60123565e0>})
Module: layer3.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60123564c0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60123564c0>})
Module: layer3.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356940>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356940>})
Module: layer3.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356b80>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356b80>})
Module: layer3.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356280>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356280>})
Module: layer3.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356310>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356310>})
Module: layer3.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356ca0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356ca0>})
Module: layer3.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356f70>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356f70>})
Module: layer3.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356dc0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356dc0>})
Module: layer3.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60123560d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60123560d0>})
Module: layer3.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6136f85dc0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6136f85dc0>})
Module: layer3.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6136f85700>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6136f85700>})
Module: layer4, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6136f85940>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6136f85940>})
Module: layer4.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6136f853a0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6136f853a0>})
Module: layer4.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6136f85040>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6136f85040>})
Module: layer4.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6136f858b0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6136f858b0>})
Module: layer4.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6136f850d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6136f850d0>})
Module: layer4.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6136f85af0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6136f85af0>})
Module: layer4.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6030493700>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6030493700>})
Module: layer4.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6030493790>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6030493790>})
Module: layer4.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60304930d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60304930d0>})
Module: layer4.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60304933a0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60304933a0>})
Module: layer4.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6030493e50>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6030493e50>})
Module: layer4.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6030493a60>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6030493a60>})
Module: layer4.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6030493b80>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6030493b80>})
Module: layer4.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6030493ca0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6030493ca0>})
Module: layer4.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6030493f70>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6030493f70>})
Module: layer4.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6030493040>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6030493040>})
Module: avgpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6030493430>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6030493430>})
Module: fc, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e65d310>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e65d310>})
[DATA] Example input shape: torch.Size([64, 3, 32, 32])
[PREPARE] Model prepared successfully.
[HOOK] Performing QAT training hook.
[HOOK] QAT epoch 1/5 started.
[HOOK][Epoch: 1] Batch 1 Loss: 1.8221
[HOOK][Epoch: 1] Batch 51 Loss: 0.5907
[HOOK][Epoch: 1] Batch 101 Loss: 0.4062
[HOOK][Epoch: 1] Batch 151 Loss: 0.4129
[HOOK][Epoch: 1] Batch 201 Loss: 0.3748
[HOOK][Epoch: 1] Batch 251 Loss: 0.5098
[HOOK][Epoch: 1] Batch 301 Loss: 0.6616
[HOOK][Epoch: 1] Batch 351 Loss: 0.4049
[HOOK][Epoch: 1] Batch 401 Loss: 0.4890
[HOOK][Epoch: 1] Batch 451 Loss: 0.5036
[HOOK][Epoch: 1] Batch 501 Loss: 0.4770
[HOOK][Epoch: 1] Batch 551 Loss: 0.5435
[HOOK][Epoch: 1] Batch 601 Loss: 0.7206
[HOOK] QAT epoch 2/5 started.
[HOOK][Epoch: 2] Batch 1 Loss: 0.2462
[HOOK][Epoch: 2] Batch 51 Loss: 0.2764
[HOOK][Epoch: 2] Batch 101 Loss: 0.3183
[HOOK][Epoch: 2] Batch 151 Loss: 0.6735
[HOOK][Epoch: 2] Batch 201 Loss: 0.4945
[HOOK][Epoch: 2] Batch 251 Loss: 0.3555
[HOOK][Epoch: 2] Batch 301 Loss: 0.2765
[HOOK][Epoch: 2] Batch 351 Loss: 0.2978
[HOOK][Epoch: 2] Batch 401 Loss: 0.3357
[HOOK][Epoch: 2] Batch 451 Loss: 0.4032
[HOOK][Epoch: 2] Batch 501 Loss: 0.2412
[HOOK][Epoch: 2] Batch 551 Loss: 0.0954
[HOOK][Epoch: 2] Batch 601 Loss: 0.2350
[HOOK] QAT epoch 3/5 started.
[HOOK][Epoch: 3] Batch 1 Loss: 0.1647
[HOOK][Epoch: 3] Batch 51 Loss: 0.0930
[HOOK][Epoch: 3] Batch 101 Loss: 0.1853
[HOOK][Epoch: 3] Batch 151 Loss: 0.3702
[HOOK][Epoch: 3] Batch 201 Loss: 0.2632
[HOOK][Epoch: 3] Batch 251 Loss: 0.1144
[HOOK][Epoch: 3] Batch 301 Loss: 0.1967
[HOOK][Epoch: 3] Batch 351 Loss: 0.1223
[HOOK][Epoch: 3] Batch 401 Loss: 0.1732
[HOOK][Epoch: 3] Batch 451 Loss: 0.1453
[HOOK][Epoch: 3] Batch 501 Loss: 0.3170
[HOOK][Epoch: 3] Batch 551 Loss: 0.2022
[HOOK][Epoch: 3] Batch 601 Loss: 0.3975
[HOOK] QAT epoch 4/5 started.
[HOOK][Epoch: 4] Batch 1 Loss: 0.1456
[HOOK][Epoch: 4] Batch 51 Loss: 0.2407
[HOOK][Epoch: 4] Batch 101 Loss: 0.1437
[HOOK][Epoch: 4] Batch 151 Loss: 0.2369
[HOOK][Epoch: 4] Batch 201 Loss: 0.1158
[HOOK][Epoch: 4] Batch 251 Loss: 0.1734
[HOOK][Epoch: 4] Batch 301 Loss: 0.3314
[HOOK][Epoch: 4] Batch 351 Loss: 0.3198
[HOOK][Epoch: 4] Batch 401 Loss: 0.2669
[HOOK][Epoch: 4] Batch 451 Loss: 0.1437
[HOOK][Epoch: 4] Batch 501 Loss: 0.1249
[HOOK][Epoch: 4] Batch 551 Loss: 0.3451
[HOOK][Epoch: 4] Batch 601 Loss: 0.2120
[HOOK] QAT epoch 5/5 started.
[HOOK][Epoch: 5] Batch 1 Loss: 0.0830
[HOOK][Epoch: 5] Batch 51 Loss: 0.0407
[HOOK][Epoch: 5] Batch 101 Loss: 0.0609
[HOOK][Epoch: 5] Batch 151 Loss: 0.1902
[HOOK][Epoch: 5] Batch 201 Loss: 0.3045
[HOOK][Epoch: 5] Batch 251 Loss: 0.1956
[HOOK][Epoch: 5] Batch 301 Loss: 0.0768
[HOOK][Epoch: 5] Batch 351 Loss: 0.0927
[HOOK][Epoch: 5] Batch 401 Loss: 0.0681
[HOOK][Epoch: 5] Batch 451 Loss: 0.2585
[HOOK][Epoch: 5] Batch 501 Loss: 0.1879
[HOOK][Epoch: 5] Batch 551 Loss: 0.0933
[HOOK][Epoch: 5] Batch 601 Loss: 0.0692
[HOOK] QAT training completed.
[FIT] Quantization performed successfully.
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: qat, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})}
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[MODEL] Model initialized and copied for quantization.
Module: , qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: maxpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: avgpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: fc, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
[DATA] Example input shape: torch.Size([64, 3, 32, 32])
[PREPARE] Model prepared successfully.
[HOOK] Performing Dynamic PTQ hook operations.
[HOOK] Dynamic PTQ setup completed.
[FIT] Quantization performed successfully.
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: qat, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})}
[MODEL] Model initialized and copied for quantization.
Module: , qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600e3403a0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600e3403a0>})
Module: conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6136f85940>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6136f85940>})
Module: bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6136f85700>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6136f85700>})
Module: relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6136f85af0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6136f85af0>})
Module: maxpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6136f858b0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6136f858b0>})
Module: layer1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6136f85040>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6136f85040>})
Module: layer1.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6136f85dc0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6136f85dc0>})
Module: layer1.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356280>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356280>})
Module: layer1.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356dc0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356dc0>})
Module: layer1.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356f70>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356f70>})
Module: layer1.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356940>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356940>})
Module: layer1.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60123564c0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60123564c0>})
Module: layer1.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60127565e0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60127565e0>})
Module: layer1.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1160>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1160>})
Module: layer1.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1ee0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1ee0>})
Module: layer1.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1dc0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1dc0>})
Module: layer1.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e15e0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e15e0>})
Module: layer1.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1f70>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1f70>})
Module: layer2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1790>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1790>})
Module: layer2.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1670>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1670>})
Module: layer2.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1c10>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1c10>})
Module: layer2.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1ca0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1ca0>})
Module: layer2.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a183ca0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a183ca0>})
Module: layer2.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a183c10>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a183c10>})
Module: layer2.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1040>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1040>})
Module: layer2.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1f70>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1f70>})
Module: layer2.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1790>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1790>})
Module: layer2.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1430>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d1430>})
Module: layer2.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d10d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d10d0>})
Module: layer2.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d19d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d19d0>})
Module: layer2.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d18b0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d18b0>})
Module: layer2.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d15e0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60301d15e0>})
Module: layer2.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e6665e0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e6665e0>})
Module: layer2.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e666af0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e666af0>})
Module: layer3, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72ae50>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72ae50>})
Module: layer3.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72aa60>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72aa60>})
Module: layer3.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a1f0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60eb72a1f0>})
Module: layer3.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6030493b80>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6030493b80>})
Module: layer3.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6030493a60>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6030493a60>})
Module: layer3.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6030493700>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6030493700>})
Module: layer3.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6030493040>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6030493040>})
Module: layer3.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60304933a0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60304933a0>})
Module: layer3.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6030493e50>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6030493e50>})
Module: layer3.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f615a1b4ca0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f615a1b4ca0>})
Module: layer3.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f615a1b4940>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f615a1b4940>})
Module: layer3.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60923bcaf0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60923bcaf0>})
Module: layer3.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e65dd30>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e65dd30>})
Module: layer3.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e65d310>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e65d310>})
Module: layer3.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60303bfc10>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60303bfc10>})
Module: layer3.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60303bf550>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60303bf550>})
Module: layer4, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60303bfca0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60303bfca0>})
Module: layer4.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60303bf0d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60303bf0d0>})
Module: layer4.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60303bf9d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60303bf9d0>})
Module: layer4.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60303bf310>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60303bf310>})
Module: layer4.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60303bf1f0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60303bf1f0>})
Module: layer4.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb280>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb280>})
Module: layer4.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addba60>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addba60>})
Module: layer4.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb550>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb550>})
Module: layer4.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c790>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c790>})
Module: layer4.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c040>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c040>})
Module: layer4.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017cd30>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017cd30>})
Module: layer4.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c1f0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c1f0>})
Module: layer4.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c280>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c280>})
Module: layer4.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017caf0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017caf0>})
Module: layer4.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c820>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c820>})
Module: layer4.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c700>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c700>})
Module: avgpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c4c0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c4c0>})
Module: fc, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c9d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c9d0>})
[DATA] Example input shape: torch.Size([64, 3, 32, 32])
[PREPARE] Model prepared successfully.
[HOOK] Performing QAT training hook.
[HOOK] QAT epoch 1/5 started.
[HOOK][Epoch: 1] Batch 1 Loss: 1.6941
[HOOK][Epoch: 1] Batch 51 Loss: 0.6318
[HOOK][Epoch: 1] Batch 101 Loss: 0.6490
[HOOK][Epoch: 1] Batch 151 Loss: 0.6332
[HOOK][Epoch: 1] Batch 201 Loss: 0.4015
[HOOK][Epoch: 1] Batch 251 Loss: 0.4144
[HOOK][Epoch: 1] Batch 301 Loss: 0.2746
[HOOK][Epoch: 1] Batch 351 Loss: 0.4138
[HOOK][Epoch: 1] Batch 401 Loss: 0.6769
[HOOK][Epoch: 1] Batch 451 Loss: 0.4194
[HOOK][Epoch: 1] Batch 501 Loss: 0.3460
[HOOK][Epoch: 1] Batch 551 Loss: 0.5233
[HOOK][Epoch: 1] Batch 601 Loss: 0.5958
[HOOK] QAT epoch 2/5 started.
[HOOK][Epoch: 2] Batch 1 Loss: 0.2870
[HOOK][Epoch: 2] Batch 51 Loss: 0.1585
[HOOK][Epoch: 2] Batch 101 Loss: 0.1546
[HOOK][Epoch: 2] Batch 151 Loss: 0.5126
[HOOK][Epoch: 2] Batch 201 Loss: 0.3446
[HOOK][Epoch: 2] Batch 251 Loss: 0.3502
[HOOK][Epoch: 2] Batch 301 Loss: 0.4663
[HOOK][Epoch: 2] Batch 351 Loss: 0.2589
[HOOK][Epoch: 2] Batch 401 Loss: 0.4167
[HOOK][Epoch: 2] Batch 451 Loss: 0.4459
[HOOK][Epoch: 2] Batch 501 Loss: 0.1959
[HOOK][Epoch: 2] Batch 551 Loss: 0.2093
[HOOK][Epoch: 2] Batch 601 Loss: 0.4313
[HOOK] QAT epoch 3/5 started.
[HOOK][Epoch: 3] Batch 1 Loss: 0.2332
[HOOK][Epoch: 3] Batch 51 Loss: 0.1828
[HOOK][Epoch: 3] Batch 101 Loss: 0.1268
[HOOK][Epoch: 3] Batch 151 Loss: 0.2008
[HOOK][Epoch: 3] Batch 201 Loss: 0.1613
[HOOK][Epoch: 3] Batch 251 Loss: 0.0837
[HOOK][Epoch: 3] Batch 301 Loss: 0.2083
[HOOK][Epoch: 3] Batch 351 Loss: 0.3884
[HOOK][Epoch: 3] Batch 401 Loss: 0.1146
[HOOK][Epoch: 3] Batch 451 Loss: 0.1921
[HOOK][Epoch: 3] Batch 501 Loss: 0.1411
[HOOK][Epoch: 3] Batch 551 Loss: 0.6186
[HOOK][Epoch: 3] Batch 601 Loss: 0.2331
[HOOK] QAT epoch 4/5 started.
[HOOK][Epoch: 4] Batch 1 Loss: 0.1387
[HOOK][Epoch: 4] Batch 51 Loss: 0.1269
[HOOK][Epoch: 4] Batch 101 Loss: 0.0898
[HOOK][Epoch: 4] Batch 151 Loss: 0.1422
[HOOK][Epoch: 4] Batch 201 Loss: 0.1829
[HOOK][Epoch: 4] Batch 251 Loss: 0.1199
[HOOK][Epoch: 4] Batch 301 Loss: 0.0128
[HOOK][Epoch: 4] Batch 351 Loss: 0.0739
[HOOK][Epoch: 4] Batch 401 Loss: 0.1642
[HOOK][Epoch: 4] Batch 451 Loss: 0.2378
[HOOK][Epoch: 4] Batch 501 Loss: 0.2772
[HOOK][Epoch: 4] Batch 551 Loss: 0.1383
[HOOK][Epoch: 4] Batch 601 Loss: 0.2822
[HOOK] QAT epoch 5/5 started.
[HOOK][Epoch: 5] Batch 1 Loss: 0.1814
[HOOK][Epoch: 5] Batch 51 Loss: 0.0936
[HOOK][Epoch: 5] Batch 101 Loss: 0.1714
[HOOK][Epoch: 5] Batch 151 Loss: 0.1625
[HOOK][Epoch: 5] Batch 201 Loss: 0.2554
[HOOK][Epoch: 5] Batch 251 Loss: 0.1219
[HOOK][Epoch: 5] Batch 301 Loss: 0.1899
[HOOK][Epoch: 5] Batch 351 Loss: 0.2515
[HOOK][Epoch: 5] Batch 401 Loss: 0.1154
[HOOK][Epoch: 5] Batch 451 Loss: 0.0666
[HOOK][Epoch: 5] Batch 501 Loss: 0.3156
[HOOK][Epoch: 5] Batch 551 Loss: 0.3144
[HOOK][Epoch: 5] Batch 601 Loss: 0.2713
[HOOK] QAT training completed.
[FIT] Quantization performed successfully.
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: qat, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})}
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[MODEL] Model initialized and copied for quantization.
Module: , qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: maxpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: avgpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: fc, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
[DATA] Example input shape: torch.Size([64, 3, 32, 32])
[PREPARE] Model prepared successfully.
[FIT ERROR] Exception during quantization:
Traceback (most recent call last):
  File "/ptls-experiments/FedCore/fedcore/algorithm/quantization/quantizers.py", line 213, in fit
    convert(self.quant_model, inplace=True)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 563, in convert
    _convert(
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 604, in _convert
    _convert(mod, mapping, True,  # inplace
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 607, in _convert
    reassign[name] = swap_module(mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 642, in swap_module
    new_mod = qmod.from_float(mod, use_precomputed_fake_quant=use_precomputed_fake_quant)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/nn/quantized/modules/batchnorm.py", line 77, in from_float
    return _BatchNorm.from_float(cls, mod, use_precomputed_fake_quant=use_precomputed_fake_quant)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/nn/quantized/modules/batchnorm.py", line 22, in from_float
    scale, zero_point = activation_post_process.calculate_qparams()
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/observer.py", line 1459, in calculate_qparams
    raise Exception(  # noqa: TRY002
Exception: calculate_qparams should not be called for PlaceholderObserver
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[MODEL] Model initialized and copied for quantization.
Module: , qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: maxpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: avgpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: fc, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
[DATA] Example input shape: torch.Size([64, 3, 32, 32])
[PREPARE] Model prepared successfully.
[HOOK] Performing Dynamic PTQ hook operations.
[HOOK] Dynamic PTQ setup completed.
[FIT] Quantization performed successfully.
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[MODEL] Model initialized and copied for quantization.
Module: , qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: maxpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: avgpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: fc, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
[DATA] Example input shape: torch.Size([64, 3, 32, 32])
[PREPARE] Model prepared successfully.
[FIT ERROR] Exception during quantization:
Traceback (most recent call last):
  File "/ptls-experiments/FedCore/fedcore/algorithm/quantization/quantizers.py", line 213, in fit
    convert(self.quant_model, inplace=True)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 563, in convert
    _convert(
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 604, in _convert
    _convert(mod, mapping, True,  # inplace
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 607, in _convert
    reassign[name] = swap_module(mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 642, in swap_module
    new_mod = qmod.from_float(mod, use_precomputed_fake_quant=use_precomputed_fake_quant)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/nn/quantized/modules/batchnorm.py", line 77, in from_float
    return _BatchNorm.from_float(cls, mod, use_precomputed_fake_quant=use_precomputed_fake_quant)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/nn/quantized/modules/batchnorm.py", line 22, in from_float
    scale, zero_point = activation_post_process.calculate_qparams()
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/observer.py", line 1459, in calculate_qparams
    raise Exception(  # noqa: TRY002
Exception: calculate_qparams should not be called for PlaceholderObserver
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[MODEL] Model initialized and copied for quantization.
Module: , qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: maxpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: avgpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: fc, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
[DATA] Example input shape: torch.Size([64, 3, 32, 32])
[PREPARE] Model prepared successfully.
[HOOK] Performing Dynamic PTQ hook operations.
[HOOK] Dynamic PTQ setup completed.
[FIT] Quantization performed successfully.
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: qat, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})}
[MODEL] Model initialized and copied for quantization.
Module: , qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f05f6f70>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f05f6f70>})
Module: conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaeef70>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaeef70>})
Module: bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee670>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee670>})
Module: relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee040>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee040>})
Module: maxpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaeeaf0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaeeaf0>})
Module: layer1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaeeee0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaeeee0>})
Module: layer1.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee9d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee9d0>})
Module: layer1.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee310>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee310>})
Module: layer1.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee4c0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee4c0>})
Module: layer1.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee940>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee940>})
Module: layer1.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee3a0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee3a0>})
Module: layer1.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaeeb80>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaeeb80>})
Module: layer1.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c280>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c280>})
Module: layer1.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c1f0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c1f0>})
Module: layer1.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c700>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c700>})
Module: layer1.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c040>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c040>})
Module: layer1.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c9d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c9d0>})
Module: layer1.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c790>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c790>})
Module: layer2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c820>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c820>})
Module: layer2.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c4c0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c4c0>})
Module: layer2.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c5e0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c5e0>})
Module: layer2.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b24820>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b24820>})
Module: layer2.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b24d30>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b24d30>})
Module: layer2.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311c10>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311c10>})
Module: layer2.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311700>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311700>})
Module: layer2.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311310>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311310>})
Module: layer2.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311a60>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311a60>})
Module: layer2.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311e50>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311e50>})
Module: layer2.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311af0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311af0>})
Module: layer2.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60063111f0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60063111f0>})
Module: layer2.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311ca0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311ca0>})
Module: layer2.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311160>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311160>})
Module: layer2.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60063110d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60063110d0>})
Module: layer2.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311820>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311820>})
Module: layer3, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311550>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311550>})
Module: layer3.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311940>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311940>})
Module: layer3.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311b80>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311b80>})
Module: layer3.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311790>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311790>})
Module: layer3.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311040>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311040>})
Module: layer3.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60063114c0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60063114c0>})
Module: layer3.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311430>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311430>})
Module: layer3.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311d30>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311d30>})
Module: layer3.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311ee0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311ee0>})
Module: layer3.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60063115e0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60063115e0>})
Module: layer3.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addba60>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addba60>})
Module: layer3.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb280>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb280>})
Module: layer3.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb3a0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb3a0>})
Module: layer3.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb790>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb790>})
Module: layer3.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb670>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb670>})
Module: layer3.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb310>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb310>})
Module: layer4, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb040>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb040>})
Module: layer4.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb1f0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb1f0>})
Module: layer4.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addbf70>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addbf70>})
Module: layer4.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addbaf0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addbaf0>})
Module: layer4.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb820>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb820>})
Module: layer4.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb430>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb430>})
Module: layer4.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addbc10>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addbc10>})
Module: layer4.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addbd30>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addbd30>})
Module: layer4.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addbb80>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addbb80>})
Module: layer4.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addbe50>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addbe50>})
Module: layer4.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb0d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb0d0>})
Module: layer4.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb4c0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb4c0>})
Module: layer4.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addbdc0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addbdc0>})
Module: layer4.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb5e0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb5e0>})
Module: layer4.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addbca0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addbca0>})
Module: layer4.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb700>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb700>})
Module: avgpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addbee0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addbee0>})
Module: fc, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb160>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb160>})
[DATA] Example input shape: torch.Size([64, 3, 32, 32])
[PREPARE] Model prepared successfully.
[HOOK] Performing QAT training hook.
[HOOK] QAT epoch 1/5 started.
[HOOK][Epoch: 1] Batch 1 Loss: 1.6431
[HOOK][Epoch: 1] Batch 51 Loss: 0.5008
[HOOK][Epoch: 1] Batch 101 Loss: 0.5237
[HOOK][Epoch: 1] Batch 151 Loss: 0.6486
[HOOK][Epoch: 1] Batch 201 Loss: 0.4452
[HOOK][Epoch: 1] Batch 251 Loss: 0.2986
[HOOK][Epoch: 1] Batch 301 Loss: 0.3645
[HOOK][Epoch: 1] Batch 351 Loss: 0.4836
[HOOK][Epoch: 1] Batch 401 Loss: 0.2960
[HOOK][Epoch: 1] Batch 451 Loss: 0.4515
[HOOK][Epoch: 1] Batch 501 Loss: 0.5042
[HOOK][Epoch: 1] Batch 551 Loss: 0.3094
[HOOK][Epoch: 1] Batch 601 Loss: 0.3429
[HOOK] QAT epoch 2/5 started.
[HOOK][Epoch: 2] Batch 1 Loss: 0.3179
[HOOK][Epoch: 2] Batch 51 Loss: 0.3091
[HOOK][Epoch: 2] Batch 101 Loss: 0.1339
[HOOK][Epoch: 2] Batch 151 Loss: 0.3506
[HOOK][Epoch: 2] Batch 201 Loss: 0.2100
[HOOK][Epoch: 2] Batch 251 Loss: 0.1674
[HOOK][Epoch: 2] Batch 301 Loss: 0.1986
[HOOK][Epoch: 2] Batch 351 Loss: 0.1933
[HOOK][Epoch: 2] Batch 401 Loss: 0.2833
[HOOK][Epoch: 2] Batch 451 Loss: 0.2565
[HOOK][Epoch: 2] Batch 501 Loss: 0.5623
[HOOK][Epoch: 2] Batch 551 Loss: 0.3328
[HOOK][Epoch: 2] Batch 601 Loss: 0.1615
[HOOK] QAT epoch 3/5 started.
[HOOK][Epoch: 3] Batch 1 Loss: 0.2149
[HOOK][Epoch: 3] Batch 51 Loss: 0.2501
[HOOK][Epoch: 3] Batch 101 Loss: 0.2243
[HOOK][Epoch: 3] Batch 151 Loss: 0.1516
[HOOK][Epoch: 3] Batch 201 Loss: 0.1553
[HOOK][Epoch: 3] Batch 251 Loss: 0.1388
[HOOK][Epoch: 3] Batch 301 Loss: 0.3335
[HOOK][Epoch: 3] Batch 351 Loss: 0.1558
[HOOK][Epoch: 3] Batch 401 Loss: 0.2025
[HOOK][Epoch: 3] Batch 451 Loss: 0.1664
[HOOK][Epoch: 3] Batch 501 Loss: 0.3233
[HOOK][Epoch: 3] Batch 551 Loss: 0.1414
[HOOK][Epoch: 3] Batch 601 Loss: 0.2135
[HOOK] QAT epoch 4/5 started.
[HOOK][Epoch: 4] Batch 1 Loss: 0.0724
[HOOK][Epoch: 4] Batch 51 Loss: 0.1420
[HOOK][Epoch: 4] Batch 101 Loss: 0.1904
[HOOK][Epoch: 4] Batch 151 Loss: 0.1404
[HOOK][Epoch: 4] Batch 201 Loss: 0.1707
[HOOK][Epoch: 4] Batch 251 Loss: 0.1972
[HOOK][Epoch: 4] Batch 301 Loss: 0.1365
[HOOK][Epoch: 4] Batch 351 Loss: 0.2359
[HOOK][Epoch: 4] Batch 401 Loss: 0.2615
[HOOK][Epoch: 4] Batch 451 Loss: 0.1233
[HOOK][Epoch: 4] Batch 501 Loss: 0.1298
[HOOK][Epoch: 4] Batch 551 Loss: 0.1579
[HOOK][Epoch: 4] Batch 601 Loss: 0.2384
[HOOK] QAT epoch 5/5 started.
[HOOK][Epoch: 5] Batch 1 Loss: 0.1317
[HOOK][Epoch: 5] Batch 51 Loss: 0.2141
[HOOK][Epoch: 5] Batch 101 Loss: 0.1443
[HOOK][Epoch: 5] Batch 151 Loss: 0.0781
[HOOK][Epoch: 5] Batch 201 Loss: 0.1317
[HOOK][Epoch: 5] Batch 251 Loss: 0.1997
[HOOK][Epoch: 5] Batch 301 Loss: 0.1548
[HOOK][Epoch: 5] Batch 351 Loss: 0.1233
[HOOK][Epoch: 5] Batch 401 Loss: 0.0875
[HOOK][Epoch: 5] Batch 451 Loss: 0.0929
[HOOK][Epoch: 5] Batch 501 Loss: 0.1077
[HOOK][Epoch: 5] Batch 551 Loss: 0.2757
[HOOK][Epoch: 5] Batch 601 Loss: 0.1271
[HOOK] QAT training completed.
[FIT] Quantization performed successfully.
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: qat, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})}
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[MODEL] Model initialized and copied for quantization.
Module: , qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: maxpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: avgpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: fc, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
[DATA] Example input shape: torch.Size([64, 3, 32, 32])
[PREPARE] Model prepared successfully.
[FIT ERROR] Exception during quantization:
Traceback (most recent call last):
  File "/ptls-experiments/FedCore/fedcore/algorithm/quantization/quantizers.py", line 213, in fit
    convert(self.quant_model, inplace=True)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 563, in convert
    _convert(
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 604, in _convert
    _convert(mod, mapping, True,  # inplace
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 607, in _convert
    reassign[name] = swap_module(mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 642, in swap_module
    new_mod = qmod.from_float(mod, use_precomputed_fake_quant=use_precomputed_fake_quant)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/nn/quantized/modules/batchnorm.py", line 77, in from_float
    return _BatchNorm.from_float(cls, mod, use_precomputed_fake_quant=use_precomputed_fake_quant)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/nn/quantized/modules/batchnorm.py", line 22, in from_float
    scale, zero_point = activation_post_process.calculate_qparams()
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/observer.py", line 1459, in calculate_qparams
    raise Exception(  # noqa: TRY002
Exception: calculate_qparams should not be called for PlaceholderObserver
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[MODEL] Model initialized and copied for quantization.
Module: , qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: maxpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: avgpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: fc, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
[DATA] Example input shape: torch.Size([64, 3, 32, 32])
[PREPARE] Model prepared successfully.
[HOOK] Performing Dynamic PTQ hook operations.
[HOOK] Dynamic PTQ setup completed.
[FIT] Quantization performed successfully.
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[MODEL] Model initialized and copied for quantization.
Module: , qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: maxpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: avgpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: fc, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
[DATA] Example input shape: torch.Size([64, 3, 32, 32])
[PREPARE] Model prepared successfully.
[FIT ERROR] Exception during quantization:
Traceback (most recent call last):
  File "/ptls-experiments/FedCore/fedcore/algorithm/quantization/quantizers.py", line 213, in fit
    convert(self.quant_model, inplace=True)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 563, in convert
    _convert(
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 604, in _convert
    _convert(mod, mapping, True,  # inplace
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 607, in _convert
    reassign[name] = swap_module(mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 642, in swap_module
    new_mod = qmod.from_float(mod, use_precomputed_fake_quant=use_precomputed_fake_quant)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/nn/quantized/modules/batchnorm.py", line 77, in from_float
    return _BatchNorm.from_float(cls, mod, use_precomputed_fake_quant=use_precomputed_fake_quant)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/nn/quantized/modules/batchnorm.py", line 22, in from_float
    scale, zero_point = activation_post_process.calculate_qparams()
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/observer.py", line 1459, in calculate_qparams
    raise Exception(  # noqa: TRY002
Exception: calculate_qparams should not be called for PlaceholderObserver
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: qat, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})}
[MODEL] Model initialized and copied for quantization.
Module: , qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f05f6f70>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f05f6f70>})
Module: conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60063111f0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60063111f0>})
Module: bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311c10>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311c10>})
Module: relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311700>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311700>})
Module: maxpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e65d040>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e65d040>})
Module: layer1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b24820>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b24820>})
Module: layer1.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c820>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c820>})
Module: layer1.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c5e0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c5e0>})
Module: layer1.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c4c0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c4c0>})
Module: layer1.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c280>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c280>})
Module: layer1.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c9d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c9d0>})
Module: layer1.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c700>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c700>})
Module: layer1.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c1f0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c1f0>})
Module: layer1.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaeeaf0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaeeaf0>})
Module: layer1.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaeeee0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaeeee0>})
Module: layer1.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee9d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee9d0>})
Module: layer1.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee040>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee040>})
Module: layer1.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee3a0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee3a0>})
Module: layer2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee4c0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee4c0>})
Module: layer2.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee310>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee310>})
Module: layer2.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee670>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee670>})
Module: layer2.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaeef70>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaeef70>})
Module: layer2.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb040>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb040>})
Module: layer2.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addba60>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addba60>})
Module: layer2.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb310>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb310>})
Module: layer2.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addbdc0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addbdc0>})
Module: layer2.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb4c0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb4c0>})
Module: layer2.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addbc10>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addbc10>})
Module: layer2.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb670>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb670>})
Module: layer2.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb430>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb430>})
Module: layer2.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addbaf0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addbaf0>})
Module: layer2.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addbf70>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addbf70>})
Module: layer2.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb3a0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb3a0>})
Module: layer2.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb280>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb280>})
Module: layer3, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addbe50>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addbe50>})
Module: layer3.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb0d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb0d0>})
Module: layer3.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb700>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb700>})
Module: layer3.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb160>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb160>})
Module: layer3.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e5434c0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e5434c0>})
Module: layer3.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e5439d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e5439d0>})
Module: layer3.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e543dc0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e543dc0>})
Module: layer3.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e543940>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e543940>})
Module: layer3.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e543550>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e543550>})
Module: layer3.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e5430d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e5430d0>})
Module: layer3.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609aff30d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609aff30d0>})
Module: layer3.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6166619790>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6166619790>})
Module: layer3.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f61666193a0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f61666193a0>})
Module: layer3.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df940>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df940>})
Module: layer3.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df700>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df700>})
Module: layer3.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921dfd30>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921dfd30>})
Module: layer4, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921dfaf0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921dfaf0>})
Module: layer4.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df5e0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df5e0>})
Module: layer4.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df790>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df790>})
Module: layer4.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921dfdc0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921dfdc0>})
Module: layer4.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df9d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df9d0>})
Module: layer4.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df1f0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df1f0>})
Module: layer4.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df8b0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df8b0>})
Module: layer4.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df0d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df0d0>})
Module: layer4.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df4c0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df4c0>})
Module: layer4.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df160>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df160>})
Module: layer4.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df310>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df310>})
Module: layer4.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921dff70>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921dff70>})
Module: layer4.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921dfa60>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921dfa60>})
Module: layer4.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356940>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356940>})
Module: layer4.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356dc0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356dc0>})
Module: layer4.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356280>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356280>})
Module: avgpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356310>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356310>})
Module: fc, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356ca0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356ca0>})
[DATA] Example input shape: torch.Size([64, 3, 32, 32])
[PREPARE] Model prepared successfully.
[HOOK] Performing QAT training hook.
[HOOK] QAT epoch 1/5 started.
[HOOK][Epoch: 1] Batch 1 Loss: 1.6665
[HOOK][Epoch: 1] Batch 51 Loss: 0.7977
[HOOK][Epoch: 1] Batch 101 Loss: 0.3588
[HOOK][Epoch: 1] Batch 151 Loss: 0.4396
[HOOK][Epoch: 1] Batch 201 Loss: 0.4204
[HOOK][Epoch: 1] Batch 251 Loss: 0.3668
[HOOK][Epoch: 1] Batch 301 Loss: 0.3847
[HOOK][Epoch: 1] Batch 351 Loss: 0.5330
[HOOK][Epoch: 1] Batch 401 Loss: 0.4629
[HOOK][Epoch: 1] Batch 451 Loss: 0.3977
[HOOK][Epoch: 1] Batch 501 Loss: 0.3867
[HOOK][Epoch: 1] Batch 551 Loss: 0.6053
[HOOK][Epoch: 1] Batch 601 Loss: 0.5787
[HOOK] QAT epoch 2/5 started.
[HOOK][Epoch: 2] Batch 1 Loss: 0.1998
[HOOK][Epoch: 2] Batch 51 Loss: 0.1407
[HOOK][Epoch: 2] Batch 101 Loss: 0.2693
[HOOK][Epoch: 2] Batch 151 Loss: 0.3771
[HOOK][Epoch: 2] Batch 201 Loss: 0.3310
[HOOK][Epoch: 2] Batch 251 Loss: 0.3461
[HOOK][Epoch: 2] Batch 301 Loss: 0.7500
[HOOK][Epoch: 2] Batch 351 Loss: 0.3067
[HOOK][Epoch: 2] Batch 401 Loss: 0.1785
[HOOK][Epoch: 2] Batch 451 Loss: 0.1949
[HOOK][Epoch: 2] Batch 501 Loss: 0.3282
[HOOK][Epoch: 2] Batch 551 Loss: 0.3374
[HOOK][Epoch: 2] Batch 601 Loss: 0.4604
[HOOK] QAT epoch 3/5 started.
[HOOK][Epoch: 3] Batch 1 Loss: 0.1304
[HOOK][Epoch: 3] Batch 51 Loss: 0.0913
[HOOK][Epoch: 3] Batch 101 Loss: 0.2227
[HOOK][Epoch: 3] Batch 151 Loss: 0.1904
[HOOK][Epoch: 3] Batch 201 Loss: 0.3301
[HOOK][Epoch: 3] Batch 251 Loss: 0.1197
[HOOK][Epoch: 3] Batch 301 Loss: 0.1241
[HOOK][Epoch: 3] Batch 351 Loss: 0.1911
[HOOK][Epoch: 3] Batch 401 Loss: 0.2050
[HOOK][Epoch: 3] Batch 451 Loss: 0.3942
[HOOK][Epoch: 3] Batch 501 Loss: 0.2760
[HOOK][Epoch: 3] Batch 551 Loss: 0.0659
[HOOK][Epoch: 3] Batch 601 Loss: 0.3357
[HOOK] QAT epoch 4/5 started.
[HOOK][Epoch: 4] Batch 1 Loss: 0.1959
[HOOK][Epoch: 4] Batch 51 Loss: 0.0434
[HOOK][Epoch: 4] Batch 101 Loss: 0.1799
[HOOK][Epoch: 4] Batch 151 Loss: 0.1048
[HOOK][Epoch: 4] Batch 201 Loss: 0.1300
[HOOK][Epoch: 4] Batch 251 Loss: 0.1021
[HOOK][Epoch: 4] Batch 301 Loss: 0.1211
[HOOK][Epoch: 4] Batch 351 Loss: 0.1536
[HOOK][Epoch: 4] Batch 401 Loss: 0.2967
[HOOK][Epoch: 4] Batch 451 Loss: 0.1878
[HOOK][Epoch: 4] Batch 501 Loss: 0.1954
[HOOK][Epoch: 4] Batch 551 Loss: 0.3014
[HOOK][Epoch: 4] Batch 601 Loss: 0.2799
[HOOK] QAT epoch 5/5 started.
[HOOK][Epoch: 5] Batch 1 Loss: 0.0919
[HOOK][Epoch: 5] Batch 51 Loss: 0.1532
[HOOK][Epoch: 5] Batch 101 Loss: 0.2042
[HOOK][Epoch: 5] Batch 151 Loss: 0.0976
[HOOK][Epoch: 5] Batch 201 Loss: 0.1977
[HOOK][Epoch: 5] Batch 251 Loss: 0.0970
[HOOK][Epoch: 5] Batch 301 Loss: 0.1763
[HOOK][Epoch: 5] Batch 351 Loss: 0.1500
[HOOK][Epoch: 5] Batch 401 Loss: 0.1563
[HOOK][Epoch: 5] Batch 451 Loss: 0.1631
[HOOK][Epoch: 5] Batch 501 Loss: 0.0785
[HOOK][Epoch: 5] Batch 551 Loss: 0.2864
[HOOK][Epoch: 5] Batch 601 Loss: 0.2224
[HOOK] QAT training completed.
[FIT] Quantization performed successfully.
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: qat, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})}
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: qat, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})}
[MODEL] Model initialized and copied for quantization.
Module: , qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f05f6f70>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f05f6f70>})
Module: conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609aff30d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609aff30d0>})
Module: bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b24820>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b24820>})
Module: relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f61666193a0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f61666193a0>})
Module: maxpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6166619790>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6166619790>})
Module: layer1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e543550>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e543550>})
Module: layer1.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e543dc0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e543dc0>})
Module: layer1.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e5439d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e5439d0>})
Module: layer1.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e65d040>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e65d040>})
Module: layer1.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addbe50>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addbe50>})
Module: layer1.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb0d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb0d0>})
Module: layer1.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb670>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb670>})
Module: layer1.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb4c0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb4c0>})
Module: layer1.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb160>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb160>})
Module: layer1.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb700>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb700>})
Module: layer1.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb3a0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb3a0>})
Module: layer1.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addbaf0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addbaf0>})
Module: layer1.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb430>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb430>})
Module: layer2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb310>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb310>})
Module: layer2.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addba60>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addba60>})
Module: layer2.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee4c0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee4c0>})
Module: layer2.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee310>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee310>})
Module: layer2.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaeef70>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaeef70>})
Module: layer2.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee670>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee670>})
Module: layer2.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee040>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee040>})
Module: layer2.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaeeee0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaeeee0>})
Module: layer2.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaeeaf0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaeeaf0>})
Module: layer2.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c1f0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c1f0>})
Module: layer2.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c820>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c820>})
Module: layer2.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c9d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c9d0>})
Module: layer2.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c4c0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c4c0>})
Module: layer2.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c5e0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c5e0>})
Module: layer2.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921dfa60>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921dfa60>})
Module: layer2.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921dff70>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921dff70>})
Module: layer3, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df8b0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df8b0>})
Module: layer3.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df1f0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df1f0>})
Module: layer3.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921dfdc0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921dfdc0>})
Module: layer3.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df790>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df790>})
Module: layer3.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df700>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df700>})
Module: layer3.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df160>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df160>})
Module: layer3.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921dfaf0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921dfaf0>})
Module: layer3.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df310>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df310>})
Module: layer3.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311700>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311700>})
Module: layer3.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311c10>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311c10>})
Module: layer3.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60063111f0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60063111f0>})
Module: layer3.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356280>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356280>})
Module: layer3.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356b80>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356b80>})
Module: layer3.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356ca0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356ca0>})
Module: layer3.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6004422d30>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6004422d30>})
Module: layer3.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b689d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b689d0>})
Module: layer4, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68b80>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68b80>})
Module: layer4.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68940>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68940>})
Module: layer4.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68670>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68670>})
Module: layer4.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68790>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68790>})
Module: layer4.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b688b0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b688b0>})
Module: layer4.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68e50>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68e50>})
Module: layer4.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b681f0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b681f0>})
Module: layer4.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68c10>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68c10>})
Module: layer4.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1790>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1790>})
Module: layer4.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1670>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1670>})
Module: layer4.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1ca0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1ca0>})
Module: layer4.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1c10>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1c10>})
Module: layer4.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e15e0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e15e0>})
Module: layer4.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1ee0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1ee0>})
Module: layer4.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1160>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1160>})
Module: layer4.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1e50>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1e50>})
Module: avgpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1b80>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1b80>})
Module: fc, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1940>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1940>})
[DATA] Example input shape: torch.Size([64, 3, 32, 32])
[PREPARE] Model prepared successfully.
[HOOK] Performing QAT training hook.
[HOOK] QAT epoch 1/5 started.
[HOOK][Epoch: 1] Batch 1 Loss: 1.2667
[HOOK][Epoch: 1] Batch 51 Loss: 0.5097
[HOOK][Epoch: 1] Batch 101 Loss: 0.7717
[HOOK][Epoch: 1] Batch 151 Loss: 0.5095
[HOOK][Epoch: 1] Batch 201 Loss: 0.5683
[HOOK][Epoch: 1] Batch 251 Loss: 0.2953
[HOOK][Epoch: 1] Batch 301 Loss: 0.4811
[HOOK][Epoch: 1] Batch 351 Loss: 0.2738
[HOOK][Epoch: 1] Batch 401 Loss: 0.3910
[HOOK][Epoch: 1] Batch 451 Loss: 0.2968
[HOOK][Epoch: 1] Batch 501 Loss: 0.4340
[HOOK][Epoch: 1] Batch 551 Loss: 0.5240
[HOOK][Epoch: 1] Batch 601 Loss: 0.4318
[HOOK] QAT epoch 2/5 started.
[HOOK][Epoch: 2] Batch 1 Loss: 0.3299
[HOOK][Epoch: 2] Batch 51 Loss: 0.4017
[HOOK][Epoch: 2] Batch 101 Loss: 0.3283
[HOOK][Epoch: 2] Batch 151 Loss: 0.1963
[HOOK][Epoch: 2] Batch 201 Loss: 0.3183
[HOOK][Epoch: 2] Batch 251 Loss: 0.1874
[HOOK][Epoch: 2] Batch 301 Loss: 0.2868
[HOOK][Epoch: 2] Batch 351 Loss: 0.5965
[HOOK][Epoch: 2] Batch 401 Loss: 0.3002
[HOOK][Epoch: 2] Batch 451 Loss: 0.2681
[HOOK][Epoch: 2] Batch 501 Loss: 0.4216
[HOOK][Epoch: 2] Batch 551 Loss: 0.3648
[HOOK][Epoch: 2] Batch 601 Loss: 0.1895
[HOOK] QAT epoch 3/5 started.
[HOOK][Epoch: 3] Batch 1 Loss: 0.3575
[HOOK][Epoch: 3] Batch 51 Loss: 0.0865
[HOOK][Epoch: 3] Batch 101 Loss: 0.1429
[HOOK][Epoch: 3] Batch 151 Loss: 0.1531
[HOOK][Epoch: 3] Batch 201 Loss: 0.2823
[HOOK][Epoch: 3] Batch 251 Loss: 0.1502
[HOOK][Epoch: 3] Batch 301 Loss: 0.3373
[HOOK][Epoch: 3] Batch 351 Loss: 0.2558
[HOOK][Epoch: 3] Batch 401 Loss: 0.1192
[HOOK][Epoch: 3] Batch 451 Loss: 0.4078
[HOOK][Epoch: 3] Batch 501 Loss: 0.2108
[HOOK][Epoch: 3] Batch 551 Loss: 0.3566
[HOOK][Epoch: 3] Batch 601 Loss: 0.2175
[HOOK] QAT epoch 4/5 started.
[HOOK][Epoch: 4] Batch 1 Loss: 0.1897
[HOOK][Epoch: 4] Batch 51 Loss: 0.1182
[HOOK][Epoch: 4] Batch 101 Loss: 0.1281
[HOOK][Epoch: 4] Batch 151 Loss: 0.1669
[HOOK][Epoch: 4] Batch 201 Loss: 0.1184
[HOOK][Epoch: 4] Batch 251 Loss: 0.0662
[HOOK][Epoch: 4] Batch 301 Loss: 0.2360
[HOOK][Epoch: 4] Batch 351 Loss: 0.2364
[HOOK][Epoch: 4] Batch 401 Loss: 0.3226
[HOOK][Epoch: 4] Batch 451 Loss: 0.2428
[HOOK][Epoch: 4] Batch 501 Loss: 0.4173
[HOOK][Epoch: 4] Batch 551 Loss: 0.1681
[HOOK][Epoch: 4] Batch 601 Loss: 0.1829
[HOOK] QAT epoch 5/5 started.
[HOOK][Epoch: 5] Batch 1 Loss: 0.1989
[HOOK][Epoch: 5] Batch 51 Loss: 0.0968
[HOOK][Epoch: 5] Batch 101 Loss: 0.0684
[HOOK][Epoch: 5] Batch 151 Loss: 0.1965
[HOOK][Epoch: 5] Batch 201 Loss: 0.0844
[HOOK][Epoch: 5] Batch 251 Loss: 0.0947
[HOOK][Epoch: 5] Batch 301 Loss: 0.1764
[HOOK][Epoch: 5] Batch 351 Loss: 0.1239
[HOOK][Epoch: 5] Batch 401 Loss: 0.1658
[HOOK][Epoch: 5] Batch 451 Loss: 0.1384
[HOOK][Epoch: 5] Batch 501 Loss: 0.0894
[HOOK][Epoch: 5] Batch 551 Loss: 0.2276
[HOOK][Epoch: 5] Batch 601 Loss: 0.0475
[HOOK] QAT training completed.
[FIT] Quantization performed successfully.
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: qat, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})}
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[MODEL] Model initialized and copied for quantization.
Module: , qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: maxpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: avgpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: fc, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
[DATA] Example input shape: torch.Size([64, 3, 32, 32])
[PREPARE] Model prepared successfully.
[HOOK] Performing Dynamic PTQ hook operations.
[HOOK] Dynamic PTQ setup completed.
[FIT] Quantization performed successfully.
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: qat, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})}
[MODEL] Model initialized and copied for quantization.
Module: , qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f05f6f70>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f05f6f70>})
Module: conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311700>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311700>})
Module: bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68b80>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68b80>})
Module: relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b689d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b689d0>})
Module: maxpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b681f0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b681f0>})
Module: layer1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68e50>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68e50>})
Module: layer1.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68790>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68790>})
Module: layer1.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68670>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68670>})
Module: layer1.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6004422d30>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6004422d30>})
Module: layer1.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356b80>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356b80>})
Module: layer1.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356280>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356280>})
Module: layer1.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df8b0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df8b0>})
Module: layer1.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df1f0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df1f0>})
Module: layer1.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921dff70>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921dff70>})
Module: layer1.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921dfaf0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921dfaf0>})
Module: layer1.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df160>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df160>})
Module: layer1.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df790>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df790>})
Module: layer1.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921dfdc0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921dfdc0>})
Module: layer2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921dfa60>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921dfa60>})
Module: layer2.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c4c0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c4c0>})
Module: layer2.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c9d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c9d0>})
Module: layer2.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaeeaf0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaeeaf0>})
Module: layer2.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee040>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee040>})
Module: layer2.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee670>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee670>})
Module: layer2.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee310>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee310>})
Module: layer2.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee4c0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60ebaee4c0>})
Module: layer2.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6166619790>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6166619790>})
Module: layer2.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f61666193a0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f61666193a0>})
Module: layer2.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb310>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb310>})
Module: layer2.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addba60>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addba60>})
Module: layer2.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb4c0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb4c0>})
Module: layer2.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addbaf0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addbaf0>})
Module: layer2.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb0d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb0d0>})
Module: layer2.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb700>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb700>})
Module: layer3, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb160>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb160>})
Module: layer3.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e65d040>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e65d040>})
Module: layer3.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e543550>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e543550>})
Module: layer3.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e543dc0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e543dc0>})
Module: layer3.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e5439d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f608e5439d0>})
Module: layer3.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b24820>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b24820>})
Module: layer3.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609aff30d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609aff30d0>})
Module: layer3.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e15e0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e15e0>})
Module: layer3.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1c10>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1c10>})
Module: layer3.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1670>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1670>})
Module: layer3.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e13a0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e13a0>})
Module: layer3.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1ca0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1ca0>})
Module: layer3.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1e50>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1e50>})
Module: layer3.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1940>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60105e1940>})
Module: layer3.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3790>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3790>})
Module: layer3.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3040>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3040>})
Module: layer4, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3310>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3310>})
Module: layer4.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3700>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3700>})
Module: layer4.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3160>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3160>})
Module: layer4.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3e50>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3e50>})
Module: layer4.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d35e0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d35e0>})
Module: layer4.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3820>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3820>})
Module: layer4.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3b80>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3b80>})
Module: layer4.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d38b0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d38b0>})
Module: layer4.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d30d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d30d0>})
Module: layer4.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d39d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d39d0>})
Module: layer4.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d33a0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d33a0>})
Module: layer4.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3670>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3670>})
Module: layer4.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3280>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3280>})
Module: layer4.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f614d73e0d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f614d73e0d0>})
Module: layer4.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f614d73e9d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f614d73e9d0>})
Module: layer4.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f614d73ee50>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f614d73ee50>})
Module: avgpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f614d73ed30>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f614d73ed30>})
Module: fc, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f614d73e820>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f614d73e820>})
[DATA] Example input shape: torch.Size([64, 3, 32, 32])
[PREPARE] Model prepared successfully.
[HOOK] Performing QAT training hook.
[HOOK] QAT epoch 1/5 started.
[HOOK][Epoch: 1] Batch 1 Loss: 1.3759
[HOOK][Epoch: 1] Batch 51 Loss: 0.3522
[HOOK][Epoch: 1] Batch 101 Loss: 0.5915
[HOOK][Epoch: 1] Batch 151 Loss: 0.5151
[HOOK][Epoch: 1] Batch 201 Loss: 0.6754
[HOOK][Epoch: 1] Batch 251 Loss: 0.3703
[HOOK][Epoch: 1] Batch 301 Loss: 0.3668
[HOOK][Epoch: 1] Batch 351 Loss: 0.8077
[HOOK][Epoch: 1] Batch 401 Loss: 0.6594
[HOOK][Epoch: 1] Batch 451 Loss: 0.5139
[HOOK][Epoch: 1] Batch 501 Loss: 0.1936
[HOOK][Epoch: 1] Batch 551 Loss: 0.6112
[HOOK][Epoch: 1] Batch 601 Loss: 0.5646
[HOOK] QAT epoch 2/5 started.
[HOOK][Epoch: 2] Batch 1 Loss: 0.2346
[HOOK][Epoch: 2] Batch 51 Loss: 0.2192
[HOOK][Epoch: 2] Batch 101 Loss: 0.3255
[HOOK][Epoch: 2] Batch 151 Loss: 0.2165
[HOOK][Epoch: 2] Batch 201 Loss: 0.2965
[HOOK][Epoch: 2] Batch 251 Loss: 0.1571
[HOOK][Epoch: 2] Batch 301 Loss: 0.2003
[HOOK][Epoch: 2] Batch 351 Loss: 0.2906
[HOOK][Epoch: 2] Batch 401 Loss: 0.1798
[HOOK][Epoch: 2] Batch 451 Loss: 0.3416
[HOOK][Epoch: 2] Batch 501 Loss: 0.2719
[HOOK][Epoch: 2] Batch 551 Loss: 0.2093
[HOOK][Epoch: 2] Batch 601 Loss: 0.1551
[HOOK] QAT epoch 3/5 started.
[HOOK][Epoch: 3] Batch 1 Loss: 0.2328
[HOOK][Epoch: 3] Batch 51 Loss: 0.1075
[HOOK][Epoch: 3] Batch 101 Loss: 0.1852
[HOOK][Epoch: 3] Batch 151 Loss: 0.1535
[HOOK][Epoch: 3] Batch 201 Loss: 0.1722
[HOOK][Epoch: 3] Batch 251 Loss: 0.2801
[HOOK][Epoch: 3] Batch 301 Loss: 0.1045
[HOOK][Epoch: 3] Batch 351 Loss: 0.5287
[HOOK][Epoch: 3] Batch 401 Loss: 0.1328
[HOOK][Epoch: 3] Batch 451 Loss: 0.1107
[HOOK][Epoch: 3] Batch 501 Loss: 0.4149
[HOOK][Epoch: 3] Batch 551 Loss: 0.1431
[HOOK][Epoch: 3] Batch 601 Loss: 0.3193
[HOOK] QAT epoch 4/5 started.
[HOOK][Epoch: 4] Batch 1 Loss: 0.2140
[HOOK][Epoch: 4] Batch 51 Loss: 0.1728
[HOOK][Epoch: 4] Batch 101 Loss: 0.3045
[HOOK][Epoch: 4] Batch 151 Loss: 0.2581
[HOOK][Epoch: 4] Batch 201 Loss: 0.3909
[HOOK][Epoch: 4] Batch 251 Loss: 0.2551
[HOOK][Epoch: 4] Batch 301 Loss: 0.2891
[HOOK][Epoch: 4] Batch 351 Loss: 0.1241
[HOOK][Epoch: 4] Batch 401 Loss: 0.0904
[HOOK][Epoch: 4] Batch 451 Loss: 0.1381
[HOOK][Epoch: 4] Batch 501 Loss: 0.1067
[HOOK][Epoch: 4] Batch 551 Loss: 0.1713
[HOOK][Epoch: 4] Batch 601 Loss: 0.1712
[HOOK] QAT epoch 5/5 started.
[HOOK][Epoch: 5] Batch 1 Loss: 0.0728
[HOOK][Epoch: 5] Batch 51 Loss: 0.1481
[HOOK][Epoch: 5] Batch 101 Loss: 0.3020
[HOOK][Epoch: 5] Batch 151 Loss: 0.0924
[HOOK][Epoch: 5] Batch 201 Loss: 0.1115
[HOOK][Epoch: 5] Batch 251 Loss: 0.2164
[HOOK][Epoch: 5] Batch 301 Loss: 0.1456
[HOOK][Epoch: 5] Batch 351 Loss: 0.1454
[HOOK][Epoch: 5] Batch 401 Loss: 0.0484
[HOOK][Epoch: 5] Batch 451 Loss: 0.0998
[HOOK][Epoch: 5] Batch 501 Loss: 0.1757
[HOOK][Epoch: 5] Batch 551 Loss: 0.1945
[HOOK][Epoch: 5] Batch 601 Loss: 0.1500
[HOOK] QAT training completed.
[FIT] Quantization performed successfully.
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: qat, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})}
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[MODEL] Model initialized and copied for quantization.
Module: , qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: maxpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: avgpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: fc, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
[DATA] Example input shape: torch.Size([64, 3, 32, 32])
[PREPARE] Model prepared successfully.
[FIT ERROR] Exception during quantization:
Traceback (most recent call last):
  File "/ptls-experiments/FedCore/fedcore/algorithm/quantization/quantizers.py", line 213, in fit
    convert(self.quant_model, inplace=True)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 563, in convert
    _convert(
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 604, in _convert
    _convert(mod, mapping, True,  # inplace
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 607, in _convert
    reassign[name] = swap_module(mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 642, in swap_module
    new_mod = qmod.from_float(mod, use_precomputed_fake_quant=use_precomputed_fake_quant)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/nn/quantized/modules/batchnorm.py", line 77, in from_float
    return _BatchNorm.from_float(cls, mod, use_precomputed_fake_quant=use_precomputed_fake_quant)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/nn/quantized/modules/batchnorm.py", line 22, in from_float
    scale, zero_point = activation_post_process.calculate_qparams()
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/observer.py", line 1459, in calculate_qparams
    raise Exception(  # noqa: TRY002
Exception: calculate_qparams should not be called for PlaceholderObserver
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[MODEL] Model initialized and copied for quantization.
Module: , qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: maxpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: avgpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: fc, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
[DATA] Example input shape: torch.Size([64, 3, 32, 32])
[PREPARE] Model prepared successfully.
[HOOK] Performing Dynamic PTQ hook operations.
[HOOK] Dynamic PTQ setup completed.
[FIT] Quantization performed successfully.
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[MODEL] Model initialized and copied for quantization.
Module: , qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: maxpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: avgpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: fc, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
[DATA] Example input shape: torch.Size([64, 3, 32, 32])
[PREPARE] Model prepared successfully.
[FIT ERROR] Exception during quantization:
Traceback (most recent call last):
  File "/ptls-experiments/FedCore/fedcore/algorithm/quantization/quantizers.py", line 213, in fit
    convert(self.quant_model, inplace=True)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 563, in convert
    _convert(
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 604, in _convert
    _convert(mod, mapping, True,  # inplace
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 607, in _convert
    reassign[name] = swap_module(mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/quantize.py", line 642, in swap_module
    new_mod = qmod.from_float(mod, use_precomputed_fake_quant=use_precomputed_fake_quant)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/nn/quantized/modules/batchnorm.py", line 77, in from_float
    return _BatchNorm.from_float(cls, mod, use_precomputed_fake_quant=use_precomputed_fake_quant)
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/nn/quantized/modules/batchnorm.py", line 22, in from_float
    scale, zero_point = activation_post_process.calculate_qparams()
  File "/root/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/torch/ao/quantization/observer.py", line 1459, in calculate_qparams
    raise Exception(  # noqa: TRY002
Exception: calculate_qparams should not be called for PlaceholderObserver
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[MODEL] Model initialized and copied for quantization.
Module: , qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: maxpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer1.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer2.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer3.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: layer4.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: avgpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
Module: fc, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
[DATA] Example input shape: torch.Size([64, 3, 32, 32])
[PREPARE] Model prepared successfully.
[HOOK] Performing Dynamic PTQ hook operations.
[HOOK] Dynamic PTQ setup completed.
[FIT] Quantization performed successfully.
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: qat, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})}
[MODEL] Model initialized and copied for quantization.
Module: , qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb0d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f609addb0d0>})
Module: conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c9d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f603017c9d0>})
Module: bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600e340ee0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600e340ee0>})
Module: relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df1f0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df1f0>})
Module: maxpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921dfa60>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921dfa60>})
Module: layer1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df790>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921df790>})
Module: layer1.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921dfaf0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921dfaf0>})
Module: layer1.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921dff70>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60921dff70>})
Module: layer1.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356280>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6012356280>})
Module: layer1.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6004422d30>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6004422d30>})
Module: layer1.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3040>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3040>})
Module: layer1.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3280>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3280>})
Module: layer1.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3670>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3670>})
Module: layer1.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3b80>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3b80>})
Module: layer1.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3820>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3820>})
Module: layer1.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3e50>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3e50>})
Module: layer1.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3160>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3160>})
Module: layer1.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3790>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3790>})
Module: layer2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d39d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d39d0>})
Module: layer2.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3310>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d3310>})
Module: layer2.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d33a0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600a7d33a0>})
Module: layer2.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68e50>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68e50>})
Module: layer2.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b681f0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b681f0>})
Module: layer2.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68790>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68790>})
Module: layer2.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b689d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b689d0>})
Module: layer2.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68670>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68670>})
Module: layer2.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68b80>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60f0b68b80>})
Module: layer2.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311700>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f6006311700>})
Module: layer2.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f614d73ee50>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f614d73ee50>})
Module: layer2.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f614d73e820>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f614d73e820>})
Module: layer2.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600c08de50>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600c08de50>})
Module: layer2.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600c08df70>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600c08df70>})
Module: layer2.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600c08dc10>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600c08dc10>})
Module: layer2.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600c08d160>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600c08d160>})
Module: layer3, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600c08d5e0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600c08d5e0>})
Module: layer3.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600c08d820>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600c08d820>})
Module: layer3.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600c08d280>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600c08d280>})
Module: layer3.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600c08dca0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600c08dca0>})
Module: layer3.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600c08d8b0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600c08d8b0>})
Module: layer3.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600c08ddc0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600c08ddc0>})
Module: layer3.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600c08da60>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600c08da60>})
Module: layer3.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600c08d700>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600c08d700>})
Module: layer3.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600c08db80>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600c08db80>})
Module: layer3.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600c08d790>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600c08d790>})
Module: layer3.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600c08d670>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600c08d670>})
Module: layer3.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600c08d550>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600c08d550>})
Module: layer3.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600c08daf0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f600c08daf0>})
Module: layer3.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60307214c0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f60307214c0>})
Module: layer3.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f61575a9940>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f61575a9940>})
Module: layer3.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f61575a93a0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f61575a93a0>})
Module: layer4, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f601275c160>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f601275c160>})
Module: layer4.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f601275c8b0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f601275c8b0>})
Module: layer4.0.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f601275cee0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f601275cee0>})
Module: layer4.0.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f601275cdc0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f601275cdc0>})
Module: layer4.0.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f601275c5e0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f601275c5e0>})
Module: layer4.0.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f601275c4c0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f601275c4c0>})
Module: layer4.0.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f601275c790>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f601275c790>})
Module: layer4.0.downsample, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f601275c040>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f601275c040>})
Module: layer4.0.downsample.0, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f601275c280>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f601275c280>})
Module: layer4.0.downsample.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f601275ca60>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f601275ca60>})
Module: layer4.1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f601275cc10>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f601275cc10>})
Module: layer4.1.conv1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f601275c9d0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f601275c9d0>})
Module: layer4.1.bn1, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f601275c700>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f601275c700>})
Module: layer4.1.relu, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f601275c820>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f601275c820>})
Module: layer4.1.conv2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f601275c310>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f601275c310>})
Module: layer4.1.bn2, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f601275c550>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f601275c550>})
Module: avgpool, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f601275ce50>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f601275ce50>})
Module: fc, qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f601275cca0>}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7f601275cca0>})
[DATA] Example input shape: torch.Size([64, 3, 32, 32])
[PREPARE] Model prepared successfully.
[HOOK] Performing QAT training hook.
[HOOK] QAT epoch 1/5 started.
[HOOK][Epoch: 1] Batch 1 Loss: 1.6500
[HOOK][Epoch: 1] Batch 51 Loss: 0.8434
[HOOK][Epoch: 1] Batch 101 Loss: 0.4847
[HOOK][Epoch: 1] Batch 151 Loss: 0.3574
[HOOK][Epoch: 1] Batch 201 Loss: 0.4718
[HOOK][Epoch: 1] Batch 251 Loss: 0.6557
[HOOK][Epoch: 1] Batch 301 Loss: 0.4854
[HOOK][Epoch: 1] Batch 351 Loss: 0.3042
[HOOK][Epoch: 1] Batch 401 Loss: 0.5021
[HOOK][Epoch: 1] Batch 451 Loss: 0.4496
[HOOK][Epoch: 1] Batch 501 Loss: 0.2493
[HOOK][Epoch: 1] Batch 551 Loss: 0.3809
[HOOK][Epoch: 1] Batch 601 Loss: 0.7637
[HOOK] QAT epoch 2/5 started.
[HOOK][Epoch: 2] Batch 1 Loss: 0.1817
[HOOK][Epoch: 2] Batch 51 Loss: 0.5280
[HOOK][Epoch: 2] Batch 101 Loss: 0.1556
[HOOK][Epoch: 2] Batch 151 Loss: 0.3406
[HOOK][Epoch: 2] Batch 201 Loss: 0.2364
[HOOK][Epoch: 2] Batch 251 Loss: 0.3701
[HOOK][Epoch: 2] Batch 301 Loss: 0.2310
[HOOK][Epoch: 2] Batch 351 Loss: 0.1648
[HOOK][Epoch: 2] Batch 401 Loss: 0.4051
[HOOK][Epoch: 2] Batch 451 Loss: 0.2440
[HOOK][Epoch: 2] Batch 501 Loss: 0.2520
[HOOK][Epoch: 2] Batch 551 Loss: 0.4211
[HOOK][Epoch: 2] Batch 601 Loss: 0.2427
[HOOK] QAT epoch 3/5 started.
[HOOK][Epoch: 3] Batch 1 Loss: 0.1185
[HOOK][Epoch: 3] Batch 51 Loss: 0.2423
[HOOK][Epoch: 3] Batch 101 Loss: 0.1125
[HOOK][Epoch: 3] Batch 151 Loss: 0.2575
[HOOK][Epoch: 3] Batch 201 Loss: 0.2322
[HOOK][Epoch: 3] Batch 251 Loss: 0.0891
[HOOK][Epoch: 3] Batch 301 Loss: 0.1548
[HOOK][Epoch: 3] Batch 351 Loss: 0.2497
[HOOK][Epoch: 3] Batch 401 Loss: 0.1731
[HOOK][Epoch: 3] Batch 451 Loss: 0.2865
[HOOK][Epoch: 3] Batch 501 Loss: 0.2375
[HOOK][Epoch: 3] Batch 551 Loss: 0.2304
[HOOK][Epoch: 3] Batch 601 Loss: 0.1651
[HOOK] QAT epoch 4/5 started.
[HOOK][Epoch: 4] Batch 1 Loss: 0.1871
[HOOK][Epoch: 4] Batch 51 Loss: 0.0929
[HOOK][Epoch: 4] Batch 101 Loss: 0.2215
[HOOK][Epoch: 4] Batch 151 Loss: 0.0944
[HOOK][Epoch: 4] Batch 201 Loss: 0.3056
[HOOK][Epoch: 4] Batch 251 Loss: 0.1301
[HOOK][Epoch: 4] Batch 301 Loss: 0.1882
[HOOK][Epoch: 4] Batch 351 Loss: 0.0491
[HOOK][Epoch: 4] Batch 401 Loss: 0.1353
[HOOK][Epoch: 4] Batch 451 Loss: 0.1037
[HOOK][Epoch: 4] Batch 501 Loss: 0.1498
[HOOK][Epoch: 4] Batch 551 Loss: 0.1037
[HOOK][Epoch: 4] Batch 601 Loss: 0.2210
[HOOK] QAT epoch 5/5 started.
[HOOK][Epoch: 5] Batch 1 Loss: 0.1276
[HOOK][Epoch: 5] Batch 51 Loss: 0.0750
[HOOK][Epoch: 5] Batch 101 Loss: 0.2307
[HOOK][Epoch: 5] Batch 151 Loss: 0.1026
[HOOK][Epoch: 5] Batch 201 Loss: 0.1393
[HOOK][Epoch: 5] Batch 251 Loss: 0.1525
[HOOK][Epoch: 5] Batch 301 Loss: 0.1666
[HOOK][Epoch: 5] Batch 351 Loss: 0.2120
[HOOK][Epoch: 5] Batch 401 Loss: 0.2600
[HOOK][Epoch: 5] Batch 451 Loss: 0.1274
[HOOK][Epoch: 5] Batch 501 Loss: 0.1315
[HOOK][Epoch: 5] Batch 551 Loss: 0.1170
[HOOK][Epoch: 5] Batch 601 Loss: 0.2127
[HOOK] QAT training completed.
[FIT] Quantization performed successfully.
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: qat, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})}
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|4         | 7/157 [00:00<00:02, 60.93it/s][A
  9%|8         | 14/157 [00:00<00:02, 60.06it/s][A
 13%|#3        | 21/157 [00:00<00:02, 61.11it/s][A
 18%|#7        | 28/157 [00:00<00:02, 61.20it/s][A
 22%|##2       | 35/157 [00:00<00:01, 61.83it/s][A
 27%|##6       | 42/157 [00:00<00:01, 62.40it/s][A
 31%|###1      | 49/157 [00:00<00:01, 62.88it/s][A
 36%|###5      | 56/157 [00:00<00:01, 63.31it/s][A
 40%|####      | 63/157 [00:01<00:01, 64.27it/s][A
 45%|####4     | 70/157 [00:01<00:01, 63.44it/s][A
 49%|####9     | 77/157 [00:01<00:01, 63.29it/s][A
 54%|#####3    | 84/157 [00:01<00:01, 63.34it/s][A
 58%|#####7    | 91/157 [00:01<00:01, 63.17it/s][A
 62%|######2   | 98/157 [00:01<00:00, 63.19it/s][A
 67%|######6   | 105/157 [00:01<00:00, 62.62it/s][A
 71%|#######1  | 112/157 [00:01<00:00, 62.98it/s][A
 76%|#######5  | 119/157 [00:01<00:00, 62.80it/s][A
 80%|########  | 126/157 [00:02<00:00, 62.68it/s][A
 85%|########4 | 133/157 [00:02<00:00, 62.77it/s][A
 89%|########9 | 140/157 [00:02<00:00, 62.02it/s][A
 94%|#########3| 147/157 [00:02<00:00, 62.15it/s][A
 98%|#########8| 154/157 [00:02<00:00, 62.51it/s][A100%|##########| 157/157 [00:02<00:00, 62.70it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|4         | 7/157 [00:00<00:02, 60.84it/s][A
  9%|8         | 14/157 [00:00<00:02, 62.10it/s][A
 13%|#3        | 21/157 [00:00<00:02, 62.63it/s][A
 18%|#7        | 28/157 [00:00<00:02, 63.05it/s][A
 22%|##2       | 35/157 [00:00<00:01, 63.35it/s][A
 27%|##6       | 42/157 [00:00<00:01, 62.67it/s][A
 31%|###1      | 49/157 [00:00<00:01, 62.90it/s][A
 36%|###5      | 56/157 [00:00<00:01, 62.36it/s][A
 40%|####      | 63/157 [00:01<00:01, 62.65it/s][A
 45%|####4     | 70/157 [00:01<00:01, 62.61it/s][A
 49%|####9     | 77/157 [00:01<00:01, 62.32it/s][A
 54%|#####3    | 84/157 [00:01<00:01, 62.41it/s][A
 58%|#####7    | 91/157 [00:01<00:01, 62.74it/s][A
 62%|######2   | 98/157 [00:01<00:00, 62.86it/s][A
 67%|######6   | 105/157 [00:01<00:00, 62.11it/s][A
 71%|#######1  | 112/157 [00:01<00:00, 62.33it/s][A
 76%|#######5  | 119/157 [00:01<00:00, 62.45it/s][A
 80%|########  | 126/157 [00:02<00:00, 62.63it/s][A
 85%|########4 | 133/157 [00:02<00:00, 62.60it/s][A
 89%|########9 | 140/157 [00:02<00:00, 63.07it/s][A
 94%|#########3| 147/157 [00:02<00:00, 63.12it/s][A
 98%|#########8| 154/157 [00:02<00:00, 62.27it/s][A100%|##########| 157/157 [00:02<00:00, 62.60it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 55.69it/s][A
  8%|8         | 13/157 [00:00<00:02, 58.94it/s][A
 13%|#2        | 20/157 [00:00<00:02, 60.53it/s][A
 17%|#7        | 27/157 [00:00<00:02, 61.42it/s][A
 22%|##1       | 34/157 [00:00<00:01, 61.66it/s][A
 26%|##6       | 41/157 [00:00<00:01, 61.76it/s][A
 31%|###       | 48/157 [00:00<00:01, 62.33it/s][A
 35%|###5      | 55/157 [00:00<00:01, 62.35it/s][A
 39%|###9      | 62/157 [00:01<00:01, 63.01it/s][A
 44%|####3     | 69/157 [00:01<00:01, 62.43it/s][A
 48%|####8     | 76/157 [00:01<00:01, 62.68it/s][A
 53%|#####2    | 83/157 [00:01<00:01, 62.62it/s][A
 57%|#####7    | 90/157 [00:01<00:01, 63.65it/s][A
 62%|######1   | 97/157 [00:01<00:00, 63.30it/s][A
 66%|######6   | 104/157 [00:01<00:00, 63.48it/s][A
 71%|#######   | 111/157 [00:01<00:00, 63.47it/s][A
 75%|#######5  | 118/157 [00:02<00:01, 23.48it/s][A
 79%|#######8  | 124/157 [00:02<00:01, 27.90it/s][A
 83%|########3 | 131/157 [00:02<00:00, 33.73it/s][A
 88%|########7 | 138/157 [00:02<00:00, 39.40it/s][A
 92%|#########2| 145/157 [00:02<00:00, 44.48it/s][A
 97%|#########6| 152/157 [00:03<00:00, 49.00it/s][A100%|##########| 157/157 [00:03<00:00, 50.04it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: qat, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 57.14it/s][A
  8%|8         | 13/157 [00:00<00:02, 59.00it/s][A
 12%|#2        | 19/157 [00:00<00:02, 58.56it/s][A
 17%|#6        | 26/157 [00:00<00:02, 60.28it/s][A
 21%|##1       | 33/157 [00:00<00:02, 61.15it/s][A
 25%|##5       | 40/157 [00:00<00:01, 61.57it/s][A
 30%|##9       | 47/157 [00:00<00:01, 61.93it/s][A
 34%|###4      | 54/157 [00:00<00:01, 62.18it/s][A
 39%|###8      | 61/157 [00:00<00:01, 62.34it/s][A
 43%|####3     | 68/157 [00:01<00:01, 61.96it/s][A
 48%|####7     | 75/157 [00:01<00:01, 61.59it/s][A
 52%|#####2    | 82/157 [00:01<00:01, 60.94it/s][A
 57%|#####6    | 89/157 [00:01<00:01, 61.47it/s][A
 61%|######1   | 96/157 [00:01<00:00, 62.02it/s][A
 66%|######5   | 103/157 [00:01<00:00, 62.73it/s][A
 70%|#######   | 110/157 [00:01<00:00, 63.18it/s][A
 75%|#######4  | 117/157 [00:01<00:00, 63.28it/s][A
 79%|#######8  | 124/157 [00:02<00:00, 62.90it/s][A
 83%|########3 | 131/157 [00:02<00:00, 63.36it/s][A
 88%|########7 | 138/157 [00:02<00:00, 62.70it/s][A
 92%|#########2| 145/157 [00:02<00:00, 62.38it/s][A
 97%|#########6| 152/157 [00:02<00:00, 63.16it/s][A100%|##########| 157/157 [00:02<00:00, 62.05it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 57.65it/s][A
  8%|7         | 12/157 [00:00<00:02, 57.89it/s][A
 12%|#2        | 19/157 [00:00<00:02, 59.27it/s][A
 17%|#6        | 26/157 [00:00<00:02, 59.92it/s][A
 21%|##1       | 33/157 [00:00<00:02, 60.63it/s][A
 25%|##5       | 40/157 [00:00<00:01, 59.60it/s][A
 30%|##9       | 47/157 [00:00<00:01, 59.85it/s][A
 34%|###4      | 54/157 [00:00<00:01, 60.84it/s][A
 39%|###8      | 61/157 [00:01<00:01, 60.64it/s][A
 43%|####3     | 68/157 [00:01<00:01, 60.72it/s][A
 48%|####7     | 75/157 [00:01<00:01, 61.10it/s][A
 52%|#####2    | 82/157 [00:01<00:01, 61.62it/s][A
 57%|#####6    | 89/157 [00:01<00:01, 61.39it/s][A
 61%|######1   | 96/157 [00:01<00:00, 61.77it/s][A
 66%|######5   | 103/157 [00:01<00:00, 60.38it/s][A
 70%|#######   | 110/157 [00:01<00:00, 60.81it/s][A
 75%|#######4  | 117/157 [00:01<00:00, 60.86it/s][A
 79%|#######8  | 124/157 [00:02<00:00, 61.25it/s][A
 83%|########3 | 131/157 [00:02<00:00, 62.14it/s][A
 88%|########7 | 138/157 [00:02<00:00, 62.83it/s][A
 92%|#########2| 145/157 [00:02<00:00, 63.36it/s][A
 97%|#########6| 152/157 [00:02<00:00, 63.77it/s][A100%|##########| 157/157 [00:02<00:00, 61.48it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 56.46it/s][A
  8%|8         | 13/157 [00:00<00:02, 59.72it/s][A
 13%|#2        | 20/157 [00:00<00:02, 62.27it/s][A
 17%|#7        | 27/157 [00:00<00:02, 63.14it/s][A
 22%|##1       | 34/157 [00:00<00:01, 64.04it/s][A
 26%|##6       | 41/157 [00:00<00:01, 64.62it/s][A
 31%|###       | 48/157 [00:00<00:01, 64.80it/s][A
 35%|###5      | 55/157 [00:00<00:01, 65.18it/s][A
 39%|###9      | 62/157 [00:00<00:01, 65.58it/s][A
 44%|####3     | 69/157 [00:01<00:01, 63.58it/s][A
 48%|####8     | 76/157 [00:01<00:01, 63.81it/s][A
 53%|#####2    | 83/157 [00:01<00:01, 63.98it/s][A
 57%|#####7    | 90/157 [00:01<00:01, 64.29it/s][A
 62%|######1   | 97/157 [00:01<00:00, 63.51it/s][A
 66%|######6   | 104/157 [00:01<00:00, 63.84it/s][A
 71%|#######   | 111/157 [00:01<00:00, 64.25it/s][A
 75%|#######5  | 118/157 [00:01<00:00, 65.03it/s][A
 80%|#######9  | 125/157 [00:01<00:00, 65.22it/s][A
 84%|########4 | 132/157 [00:02<00:00, 63.88it/s][A
 89%|########8 | 139/157 [00:02<00:00, 64.30it/s][A
 93%|#########2| 146/157 [00:02<00:00, 64.73it/s][A
 97%|#########7| 153/157 [00:02<00:00, 65.25it/s][A100%|##########| 157/157 [00:02<00:00, 64.27it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|4         | 7/157 [00:00<00:02, 62.31it/s][A
  9%|8         | 14/157 [00:00<00:02, 62.57it/s][A
 13%|#3        | 21/157 [00:00<00:02, 62.56it/s][A
 18%|#7        | 28/157 [00:00<00:02, 62.94it/s][A
 22%|##2       | 35/157 [00:00<00:02, 60.95it/s][A
 27%|##6       | 42/157 [00:00<00:01, 60.65it/s][A
 31%|###1      | 49/157 [00:00<00:01, 60.82it/s][A
 36%|###5      | 56/157 [00:00<00:01, 60.80it/s][A
 40%|####      | 63/157 [00:01<00:01, 61.45it/s][A
 45%|####4     | 70/157 [00:01<00:01, 61.82it/s][A
 49%|####9     | 77/157 [00:01<00:01, 62.02it/s][A
 54%|#####3    | 84/157 [00:01<00:01, 62.70it/s][A
 58%|#####7    | 91/157 [00:01<00:01, 62.92it/s][A
 62%|######2   | 98/157 [00:01<00:00, 60.48it/s][A
 67%|######6   | 105/157 [00:01<00:00, 59.28it/s][A
 71%|#######1  | 112/157 [00:01<00:00, 60.10it/s][A
 76%|#######5  | 119/157 [00:01<00:00, 60.80it/s][A
 80%|########  | 126/157 [00:02<00:00, 61.88it/s][A
 85%|########4 | 133/157 [00:02<00:00, 62.38it/s][A
 89%|########9 | 140/157 [00:02<00:00, 62.98it/s][A
 94%|#########3| 147/157 [00:02<00:00, 63.32it/s][A
 98%|#########8| 154/157 [00:02<00:00, 63.75it/s][A100%|##########| 157/157 [00:02<00:00, 61.80it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: qat, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 57.87it/s][A
  8%|8         | 13/157 [00:00<00:02, 60.54it/s][A
 13%|#2        | 20/157 [00:00<00:02, 61.88it/s][A
 17%|#7        | 27/157 [00:00<00:02, 62.44it/s][A
 22%|##1       | 34/157 [00:00<00:01, 62.75it/s][A
 26%|##6       | 41/157 [00:00<00:01, 62.91it/s][A
 31%|###       | 48/157 [00:00<00:01, 63.14it/s][A
 35%|###5      | 55/157 [00:00<00:01, 62.20it/s][A
 39%|###9      | 62/157 [00:01<00:01, 60.82it/s][A
 44%|####3     | 69/157 [00:01<00:01, 61.68it/s][A
 48%|####8     | 76/157 [00:01<00:01, 62.20it/s][A
 53%|#####2    | 83/157 [00:01<00:01, 63.08it/s][A
 57%|#####7    | 90/157 [00:01<00:01, 63.50it/s][A
 62%|######1   | 97/157 [00:01<00:00, 63.60it/s][A
 66%|######6   | 104/157 [00:01<00:00, 62.88it/s][A
 71%|#######   | 111/157 [00:01<00:00, 62.75it/s][A
 75%|#######5  | 118/157 [00:01<00:00, 62.88it/s][A
 80%|#######9  | 125/157 [00:02<00:00, 61.90it/s][A
 84%|########4 | 132/157 [00:02<00:00, 62.16it/s][A
 89%|########8 | 139/157 [00:02<00:00, 62.64it/s][A
 93%|#########2| 146/157 [00:02<00:00, 62.77it/s][A
 97%|#########7| 153/157 [00:02<00:00, 62.09it/s][A100%|##########| 157/157 [00:02<00:00, 62.39it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: qat, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 58.72it/s][A
  8%|8         | 13/157 [00:00<00:02, 60.86it/s][A
 13%|#2        | 20/157 [00:00<00:02, 59.33it/s][A
 17%|#7        | 27/157 [00:00<00:02, 60.03it/s][A
 22%|##1       | 34/157 [00:00<00:02, 61.14it/s][A
 26%|##6       | 41/157 [00:00<00:01, 61.98it/s][A
 31%|###       | 48/157 [00:00<00:01, 62.59it/s][A
 35%|###5      | 55/157 [00:00<00:01, 63.53it/s][A
 39%|###9      | 62/157 [00:00<00:01, 64.27it/s][A
 44%|####3     | 69/157 [00:01<00:01, 63.92it/s][A
 48%|####8     | 76/157 [00:01<00:01, 63.97it/s][A
 53%|#####2    | 83/157 [00:01<00:01, 62.47it/s][A
 57%|#####7    | 90/157 [00:01<00:01, 61.65it/s][A
 62%|######1   | 97/157 [00:01<00:00, 62.01it/s][A
 66%|######6   | 104/157 [00:01<00:00, 62.36it/s][A
 71%|#######   | 111/157 [00:01<00:00, 61.81it/s][A
 75%|#######5  | 118/157 [00:01<00:00, 62.30it/s][A
 80%|#######9  | 125/157 [00:02<00:00, 62.77it/s][A
 84%|########4 | 132/157 [00:02<00:00, 63.26it/s][A
 89%|########8 | 139/157 [00:02<00:00, 63.70it/s][A
 93%|#########2| 146/157 [00:02<00:00, 62.66it/s][A
 97%|#########7| 153/157 [00:02<00:00, 62.47it/s][A100%|##########| 157/157 [00:02<00:00, 62.43it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|4         | 7/157 [00:00<00:02, 61.02it/s][A
  9%|8         | 14/157 [00:00<00:02, 61.96it/s][A
 13%|#3        | 21/157 [00:00<00:02, 62.44it/s][A
 18%|#7        | 28/157 [00:00<00:02, 62.99it/s][A
 22%|##2       | 35/157 [00:00<00:01, 63.08it/s][A
 27%|##6       | 42/157 [00:00<00:01, 63.23it/s][A
 31%|###1      | 49/157 [00:00<00:01, 61.31it/s][A
 36%|###5      | 56/157 [00:00<00:01, 61.36it/s][A
 40%|####      | 63/157 [00:01<00:01, 62.02it/s][A
 45%|####4     | 70/157 [00:01<00:01, 63.12it/s][A
 49%|####9     | 77/157 [00:01<00:01, 63.79it/s][A
 54%|#####3    | 84/157 [00:01<00:01, 64.93it/s][A
 58%|#####7    | 91/157 [00:01<00:01, 65.12it/s][A
 62%|######2   | 98/157 [00:01<00:00, 64.64it/s][A
 67%|######6   | 105/157 [00:01<00:00, 63.99it/s][A
 71%|#######1  | 112/157 [00:01<00:00, 63.10it/s][A
 76%|#######5  | 119/157 [00:01<00:00, 63.24it/s][A
 80%|########  | 126/157 [00:01<00:00, 63.83it/s][A
 85%|########4 | 133/157 [00:02<00:00, 64.06it/s][A
 89%|########9 | 140/157 [00:02<00:00, 64.38it/s][A
 94%|#########3| 147/157 [00:02<00:00, 64.17it/s][A
 98%|#########8| 154/157 [00:02<00:00, 64.53it/s][A100%|##########| 157/157 [00:02<00:00, 63.55it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: qat, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 59.77it/s][A
  8%|7         | 12/157 [00:00<00:02, 58.64it/s][A
 11%|#1        | 18/157 [00:00<00:02, 57.97it/s][A
 16%|#5        | 25/157 [00:00<00:02, 59.59it/s][A
 20%|#9        | 31/157 [00:00<00:02, 59.28it/s][A
 24%|##3       | 37/157 [00:00<00:02, 58.82it/s][A
 28%|##8       | 44/157 [00:00<00:01, 59.35it/s][A
 32%|###2      | 51/157 [00:00<00:01, 60.05it/s][A
 37%|###6      | 58/157 [00:00<00:01, 60.81it/s][A
 41%|####1     | 65/157 [00:01<00:01, 61.16it/s][A
 46%|####5     | 72/157 [00:01<00:01, 62.16it/s][A
 50%|#####     | 79/157 [00:01<00:01, 60.31it/s][A
 55%|#####4    | 86/157 [00:01<00:01, 61.01it/s][A
 59%|#####9    | 93/157 [00:01<00:01, 61.52it/s][A
 64%|######3   | 100/157 [00:01<00:00, 61.72it/s][A
 68%|######8   | 107/157 [00:01<00:00, 62.13it/s][A
 73%|#######2  | 114/157 [00:01<00:00, 62.49it/s][A
 77%|#######7  | 121/157 [00:01<00:00, 62.60it/s][A
 82%|########1 | 128/157 [00:02<00:00, 63.03it/s][A
 86%|########5 | 135/157 [00:02<00:00, 62.78it/s][A
 90%|######### | 142/157 [00:02<00:00, 61.64it/s][A
 95%|#########4| 149/157 [00:02<00:00, 62.04it/s][A
 99%|#########9| 156/157 [00:02<00:00, 62.11it/s][A100%|##########| 157/157 [00:02<00:00, 61.27it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|4         | 7/157 [00:00<00:02, 61.19it/s][A
  9%|8         | 14/157 [00:00<00:02, 62.09it/s][A
 13%|#3        | 21/157 [00:00<00:02, 62.53it/s][A
 18%|#7        | 28/157 [00:00<00:02, 62.81it/s][A
 22%|##2       | 35/157 [00:00<00:01, 61.34it/s][A
 27%|##6       | 42/157 [00:00<00:01, 60.38it/s][A
 31%|###1      | 49/157 [00:00<00:01, 61.12it/s][A
 36%|###5      | 56/157 [00:00<00:01, 61.72it/s][A
 40%|####      | 63/157 [00:01<00:01, 62.40it/s][A
 45%|####4     | 70/157 [00:01<00:01, 62.09it/s][A
 49%|####9     | 77/157 [00:01<00:01, 62.56it/s][A
 54%|#####3    | 84/157 [00:01<00:01, 63.25it/s][A
 58%|#####7    | 91/157 [00:01<00:01, 63.55it/s][A
 62%|######2   | 98/157 [00:01<00:00, 63.67it/s][A
 67%|######6   | 105/157 [00:01<00:00, 61.63it/s][A
 71%|#######1  | 112/157 [00:01<00:00, 62.18it/s][A
 76%|#######5  | 119/157 [00:01<00:00, 62.49it/s][A
 80%|########  | 126/157 [00:02<00:00, 62.44it/s][A
 85%|########4 | 133/157 [00:02<00:00, 62.76it/s][A
 89%|########9 | 140/157 [00:02<00:00, 63.04it/s][A
 94%|#########3| 147/157 [00:02<00:00, 63.43it/s][A
 98%|#########8| 154/157 [00:02<00:00, 63.64it/s][A100%|##########| 157/157 [00:02<00:00, 62.60it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 54.14it/s][A
  8%|7         | 12/157 [00:00<00:02, 56.98it/s][A
 11%|#1        | 18/157 [00:00<00:02, 57.47it/s][A
 16%|#5        | 25/157 [00:00<00:02, 58.95it/s][A
 20%|##        | 32/157 [00:00<00:02, 60.24it/s][A
 25%|##4       | 39/157 [00:00<00:01, 60.52it/s][A
 29%|##9       | 46/157 [00:00<00:01, 60.95it/s][A
 34%|###3      | 53/157 [00:00<00:01, 61.45it/s][A
 38%|###8      | 60/157 [00:00<00:01, 61.35it/s][A
 43%|####2     | 67/157 [00:01<00:01, 60.09it/s][A
 47%|####7     | 74/157 [00:01<00:01, 60.38it/s][A
 52%|#####1    | 81/157 [00:01<00:01, 61.11it/s][A
 56%|#####6    | 88/157 [00:01<00:01, 61.70it/s][A
 61%|######    | 95/157 [00:01<00:01, 61.91it/s][A
 65%|######4   | 102/157 [00:01<00:00, 61.73it/s][A
 69%|######9   | 109/157 [00:01<00:00, 62.01it/s][A
 74%|#######3  | 116/157 [00:01<00:00, 62.28it/s][A
 78%|#######8  | 123/157 [00:02<00:00, 61.59it/s][A
 83%|########2 | 130/157 [00:02<00:00, 60.94it/s][A
 87%|########7 | 137/157 [00:02<00:00, 61.53it/s][A
 92%|#########1| 144/157 [00:02<00:00, 62.17it/s][A
 96%|#########6| 151/157 [00:02<00:00, 62.69it/s][A100%|##########| 157/157 [00:02<00:00, 61.29it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|4         | 7/157 [00:00<00:02, 61.04it/s][A
  9%|8         | 14/157 [00:00<00:02, 61.48it/s][A
 13%|#3        | 21/157 [00:00<00:02, 62.13it/s][A
 18%|#7        | 28/157 [00:00<00:02, 60.43it/s][A
 22%|##2       | 35/157 [00:00<00:01, 61.48it/s][A
 27%|##6       | 42/157 [00:00<00:01, 61.88it/s][A
 31%|###1      | 49/157 [00:00<00:01, 62.51it/s][A
 36%|###5      | 56/157 [00:00<00:01, 62.81it/s][A
 40%|####      | 63/157 [00:01<00:01, 63.12it/s][A
 45%|####4     | 70/157 [00:01<00:01, 63.36it/s][A
 49%|####9     | 77/157 [00:01<00:01, 62.70it/s][A
 54%|#####3    | 84/157 [00:01<00:01, 62.68it/s][A
 58%|#####7    | 91/157 [00:01<00:01, 61.24it/s][A
 62%|######2   | 98/157 [00:01<00:00, 61.31it/s][A
 67%|######6   | 105/157 [00:01<00:00, 62.16it/s][A
 71%|#######1  | 112/157 [00:01<00:00, 63.13it/s][A
 76%|#######5  | 119/157 [00:01<00:00, 62.95it/s][A
 80%|########  | 126/157 [00:02<00:00, 63.23it/s][A
 85%|########4 | 133/157 [00:02<00:00, 63.38it/s][A
 89%|########9 | 140/157 [00:02<00:00, 63.72it/s][A
 94%|#########3| 147/157 [00:02<00:00, 63.98it/s][A
 98%|#########8| 154/157 [00:02<00:00, 62.24it/s][A100%|##########| 157/157 [00:02<00:00, 62.52it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 58.79it/s][A
  8%|7         | 12/157 [00:00<00:02, 59.00it/s][A
 12%|#2        | 19/157 [00:00<00:02, 59.88it/s][A
 17%|#6        | 26/157 [00:00<00:02, 60.37it/s][A
 21%|##1       | 33/157 [00:00<00:02, 60.86it/s][A
 25%|##5       | 40/157 [00:00<00:01, 61.28it/s][A
 30%|##9       | 47/157 [00:00<00:01, 61.52it/s][A
 34%|###4      | 54/157 [00:00<00:01, 59.99it/s][A
 39%|###8      | 61/157 [00:01<00:01, 60.65it/s][A
 43%|####3     | 68/157 [00:01<00:01, 61.02it/s][A
 48%|####7     | 75/157 [00:01<00:01, 61.66it/s][A
 52%|#####2    | 82/157 [00:01<00:01, 62.00it/s][A
 57%|#####6    | 89/157 [00:01<00:01, 62.25it/s][A
 61%|######1   | 96/157 [00:01<00:00, 61.92it/s][A
 66%|######5   | 103/157 [00:01<00:00, 61.98it/s][A
 70%|#######   | 110/157 [00:01<00:00, 61.63it/s][A
 75%|#######4  | 117/157 [00:01<00:00, 60.60it/s][A
 79%|#######8  | 124/157 [00:02<00:00, 61.06it/s][A
 83%|########3 | 131/157 [00:02<00:00, 61.38it/s][A
 88%|########7 | 138/157 [00:02<00:00, 61.91it/s][A
 92%|#########2| 145/157 [00:02<00:00, 62.33it/s][A
 97%|#########6| 152/157 [00:02<00:00, 62.92it/s][A100%|##########| 157/157 [00:02<00:00, 61.60it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: qat, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 59.83it/s][A
  8%|7         | 12/157 [00:00<00:02, 57.50it/s][A
 11%|#1        | 18/157 [00:00<00:02, 58.48it/s][A
 16%|#5        | 25/157 [00:00<00:02, 59.88it/s][A
 20%|##        | 32/157 [00:00<00:02, 60.70it/s][A
 25%|##4       | 39/157 [00:00<00:01, 61.49it/s][A
 29%|##9       | 46/157 [00:00<00:01, 62.16it/s][A
 34%|###3      | 53/157 [00:00<00:01, 62.11it/s][A
 38%|###8      | 60/157 [00:00<00:01, 62.11it/s][A
 43%|####2     | 67/157 [00:01<00:01, 62.48it/s][A
 47%|####7     | 74/157 [00:01<00:01, 61.19it/s][A
 52%|#####1    | 81/157 [00:01<00:01, 60.33it/s][A
 56%|#####6    | 88/157 [00:01<00:01, 60.54it/s][A
 61%|######    | 95/157 [00:01<00:01, 61.18it/s][A
 65%|######4   | 102/157 [00:01<00:00, 60.96it/s][A
 69%|######9   | 109/157 [00:01<00:00, 61.40it/s][A
 74%|#######3  | 116/157 [00:01<00:00, 61.80it/s][A
 78%|#######8  | 123/157 [00:02<00:00, 61.78it/s][A
 83%|########2 | 130/157 [00:02<00:00, 62.29it/s][A
 87%|########7 | 137/157 [00:02<00:00, 61.32it/s][A
 92%|#########1| 144/157 [00:02<00:00, 61.32it/s][A
 96%|#########6| 151/157 [00:02<00:00, 60.92it/s][A100%|##########| 157/157 [00:02<00:00, 61.29it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 58.05it/s][A
  8%|7         | 12/157 [00:00<00:02, 58.60it/s][A
 12%|#2        | 19/157 [00:00<00:02, 59.54it/s][A
 17%|#6        | 26/157 [00:00<00:02, 60.75it/s][A
 21%|##1       | 33/157 [00:00<00:02, 61.06it/s][A
 25%|##5       | 40/157 [00:00<00:01, 60.05it/s][A
 30%|##9       | 47/157 [00:00<00:01, 59.90it/s][A
 34%|###4      | 54/157 [00:00<00:01, 60.34it/s][A
 39%|###8      | 61/157 [00:01<00:01, 60.47it/s][A
 43%|####3     | 68/157 [00:01<00:01, 61.10it/s][A
 48%|####7     | 75/157 [00:01<00:01, 61.49it/s][A
 52%|#####2    | 82/157 [00:01<00:01, 61.99it/s][A
 57%|#####6    | 89/157 [00:01<00:01, 62.57it/s][A
 61%|######1   | 96/157 [00:01<00:00, 61.71it/s][A
 66%|######5   | 103/157 [00:01<00:00, 60.58it/s][A
 70%|#######   | 110/157 [00:01<00:00, 60.57it/s][A
 75%|#######4  | 117/157 [00:01<00:00, 61.38it/s][A
 79%|#######8  | 124/157 [00:02<00:00, 62.06it/s][A
 83%|########3 | 131/157 [00:02<00:00, 62.42it/s][A
 88%|########7 | 138/157 [00:02<00:00, 62.93it/s][A
 92%|#########2| 145/157 [00:02<00:00, 62.91it/s][A
 97%|#########6| 152/157 [00:02<00:00, 62.78it/s][A100%|##########| 157/157 [00:02<00:00, 61.47it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 58.69it/s][A
  8%|7         | 12/157 [00:00<00:02, 58.67it/s][A
 11%|#1        | 18/157 [00:00<00:02, 58.61it/s][A
 16%|#5        | 25/157 [00:00<00:02, 60.13it/s][A
 20%|##        | 32/157 [00:00<00:02, 60.95it/s][A
 25%|##4       | 39/157 [00:00<00:01, 61.44it/s][A
 29%|##9       | 46/157 [00:00<00:01, 61.87it/s][A
 34%|###3      | 53/157 [00:00<00:01, 62.38it/s][A
 38%|###8      | 60/157 [00:00<00:01, 60.87it/s][A
 43%|####2     | 67/157 [00:01<00:01, 60.80it/s][A
 47%|####7     | 74/157 [00:01<00:01, 61.20it/s][A
 52%|#####1    | 81/157 [00:01<00:01, 61.84it/s][A
 56%|#####6    | 88/157 [00:01<00:01, 62.31it/s][A
 61%|######    | 95/157 [00:01<00:00, 62.44it/s][A
 65%|######4   | 102/157 [00:01<00:00, 62.88it/s][A
 69%|######9   | 109/157 [00:01<00:00, 63.08it/s][A
 74%|#######3  | 116/157 [00:01<00:00, 63.44it/s][A
 78%|#######8  | 123/157 [00:01<00:00, 61.90it/s][A
 83%|########2 | 130/157 [00:02<00:00, 61.94it/s][A
 87%|########7 | 137/157 [00:02<00:00, 62.51it/s][A
 92%|#########1| 144/157 [00:02<00:00, 63.09it/s][A
 96%|#########6| 151/157 [00:02<00:00, 63.24it/s][A100%|##########| 157/157 [00:02<00:00, 62.06it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 57.63it/s][A
  8%|7         | 12/157 [00:00<00:02, 57.40it/s][A
 11%|#1        | 18/157 [00:00<00:02, 57.03it/s][A
 15%|#5        | 24/157 [00:00<00:02, 57.18it/s][A
 20%|#9        | 31/157 [00:00<00:02, 59.11it/s][A
 24%|##4       | 38/157 [00:00<00:01, 60.01it/s][A
 29%|##8       | 45/157 [00:00<00:01, 60.76it/s][A
 33%|###3      | 52/157 [00:00<00:01, 60.79it/s][A
 38%|###7      | 59/157 [00:00<00:01, 61.19it/s][A
 42%|####2     | 66/157 [00:01<00:01, 61.34it/s][A
 46%|####6     | 73/157 [00:01<00:01, 61.55it/s][A
 51%|#####     | 80/157 [00:01<00:01, 61.90it/s][A
 55%|#####5    | 87/157 [00:01<00:01, 60.67it/s][A
 60%|#####9    | 94/157 [00:01<00:01, 60.86it/s][A
 64%|######4   | 101/157 [00:01<00:00, 61.35it/s][A
 69%|######8   | 108/157 [00:01<00:00, 61.73it/s][A
 73%|#######3  | 115/157 [00:01<00:00, 62.27it/s][A
 78%|#######7  | 122/157 [00:02<00:00, 62.64it/s][A
 82%|########2 | 129/157 [00:02<00:00, 62.90it/s][A
 87%|########6 | 136/157 [00:02<00:00, 63.00it/s][A
 91%|#########1| 143/157 [00:02<00:00, 62.44it/s][A
 96%|#########5| 150/157 [00:02<00:00, 61.64it/s][A
100%|##########| 157/157 [00:02<00:00, 62.55it/s][A100%|##########| 157/157 [00:02<00:00, 61.25it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: qat, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 58.17it/s][A
  8%|7         | 12/157 [00:00<00:02, 59.01it/s][A
 11%|#1        | 18/157 [00:00<00:02, 59.40it/s][A
 16%|#5        | 25/157 [00:00<00:02, 60.32it/s][A
 20%|##        | 32/157 [00:00<00:02, 60.72it/s][A
 25%|##4       | 39/157 [00:00<00:01, 61.58it/s][A
 29%|##9       | 46/157 [00:00<00:01, 60.18it/s][A
 34%|###3      | 53/157 [00:00<00:01, 59.25it/s][A
 38%|###7      | 59/157 [00:00<00:01, 59.41it/s][A
 41%|####1     | 65/157 [00:01<00:01, 59.48it/s][A
 46%|####5     | 72/157 [00:01<00:01, 59.92it/s][A
 50%|#####     | 79/157 [00:01<00:01, 60.80it/s][A
 55%|#####4    | 86/157 [00:01<00:01, 61.38it/s][A
 59%|#####9    | 93/157 [00:01<00:01, 62.06it/s][A
 64%|######3   | 100/157 [00:01<00:00, 62.32it/s][A
 68%|######8   | 107/157 [00:01<00:00, 61.13it/s][A
 73%|#######2  | 114/157 [00:01<00:00, 60.94it/s][A
 77%|#######7  | 121/157 [00:01<00:00, 60.68it/s][A
 82%|########1 | 128/157 [00:02<00:00, 61.12it/s][A
 86%|########5 | 135/157 [00:02<00:00, 61.60it/s][A
 90%|######### | 142/157 [00:02<00:00, 61.87it/s][A
 95%|#########4| 149/157 [00:02<00:00, 62.00it/s][A
 99%|#########9| 156/157 [00:02<00:00, 62.36it/s][A100%|##########| 157/157 [00:02<00:00, 61.03it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: qat, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 54.22it/s][A
  8%|7         | 12/157 [00:00<00:02, 55.79it/s][A
 11%|#1        | 18/157 [00:00<00:02, 56.02it/s][A
 16%|#5        | 25/157 [00:00<00:02, 58.34it/s][A
 20%|##        | 32/157 [00:00<00:02, 59.47it/s][A
 25%|##4       | 39/157 [00:00<00:01, 60.03it/s][A
 29%|##9       | 46/157 [00:00<00:01, 60.78it/s][A
 34%|###3      | 53/157 [00:00<00:01, 60.75it/s][A
 38%|###8      | 60/157 [00:01<00:01, 61.02it/s][A
 43%|####2     | 67/157 [00:01<00:01, 59.50it/s][A
 47%|####7     | 74/157 [00:01<00:01, 59.99it/s][A
 52%|#####1    | 81/157 [00:01<00:01, 60.64it/s][A
 56%|#####6    | 88/157 [00:01<00:01, 61.13it/s][A
 61%|######    | 95/157 [00:01<00:01, 61.22it/s][A
 65%|######4   | 102/157 [00:01<00:00, 61.22it/s][A
 69%|######9   | 109/157 [00:01<00:00, 61.67it/s][A
 74%|#######3  | 116/157 [00:01<00:00, 61.95it/s][A
 78%|#######8  | 123/157 [00:02<00:00, 61.99it/s][A
 83%|########2 | 130/157 [00:02<00:00, 60.91it/s][A
 87%|########7 | 137/157 [00:02<00:00, 61.31it/s][A
 92%|#########1| 144/157 [00:02<00:00, 61.07it/s][A
 96%|#########6| 151/157 [00:02<00:00, 61.13it/s][A100%|##########| 157/157 [00:02<00:00, 60.53it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 58.92it/s][A
  8%|8         | 13/157 [00:00<00:02, 60.36it/s][A
 13%|#2        | 20/157 [00:00<00:02, 61.62it/s][A
 17%|#7        | 27/157 [00:00<00:02, 60.96it/s][A
 22%|##1       | 34/157 [00:00<00:01, 61.50it/s][A
 26%|##6       | 41/157 [00:00<00:01, 62.29it/s][A
 31%|###       | 48/157 [00:00<00:01, 62.74it/s][A
 35%|###5      | 55/157 [00:00<00:01, 63.27it/s][A
 39%|###9      | 62/157 [00:00<00:01, 63.59it/s][A
 44%|####3     | 69/157 [00:01<00:01, 63.23it/s][A
 48%|####8     | 76/157 [00:01<00:01, 63.32it/s][A
 53%|#####2    | 83/157 [00:01<00:01, 63.37it/s][A
 57%|#####7    | 90/157 [00:01<00:01, 62.47it/s][A
 62%|######1   | 97/157 [00:01<00:00, 61.30it/s][A
 66%|######6   | 104/157 [00:01<00:00, 60.83it/s][A
 71%|#######   | 111/157 [00:01<00:00, 61.42it/s][A
 75%|#######5  | 118/157 [00:01<00:00, 61.91it/s][A
 80%|#######9  | 125/157 [00:02<00:00, 62.09it/s][A
 84%|########4 | 132/157 [00:02<00:00, 62.47it/s][A
 89%|########8 | 139/157 [00:02<00:00, 62.37it/s][A
 93%|#########2| 146/157 [00:02<00:00, 62.09it/s][A
 97%|#########7| 153/157 [00:02<00:00, 59.73it/s][A100%|##########| 157/157 [00:02<00:00, 61.75it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: qat, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 58.76it/s][A
  8%|7         | 12/157 [00:00<00:02, 58.86it/s][A
 12%|#2        | 19/157 [00:00<00:02, 60.59it/s][A
 17%|#6        | 26/157 [00:00<00:02, 62.52it/s][A
 21%|##1       | 33/157 [00:00<00:01, 63.54it/s][A
 25%|##5       | 40/157 [00:00<00:01, 62.32it/s][A
 30%|##9       | 47/157 [00:00<00:01, 62.30it/s][A
 34%|###4      | 54/157 [00:00<00:01, 60.44it/s][A
 39%|###8      | 61/157 [00:00<00:01, 60.70it/s][A
 43%|####3     | 68/157 [00:01<00:01, 61.14it/s][A
 48%|####7     | 75/157 [00:01<00:01, 61.60it/s][A
 52%|#####2    | 82/157 [00:01<00:01, 61.88it/s][A
 57%|#####6    | 89/157 [00:01<00:01, 61.76it/s][A
 61%|######1   | 96/157 [00:01<00:00, 61.51it/s][A
 66%|######5   | 103/157 [00:01<00:00, 61.03it/s][A
 70%|#######   | 110/157 [00:01<00:00, 61.13it/s][A
 75%|#######4  | 117/157 [00:01<00:00, 60.04it/s][A
 79%|#######8  | 124/157 [00:02<00:00, 60.33it/s][A
 83%|########3 | 131/157 [00:02<00:00, 60.83it/s][A
 88%|########7 | 138/157 [00:02<00:00, 60.69it/s][A
 92%|#########2| 145/157 [00:02<00:00, 61.32it/s][A
 97%|#########6| 152/157 [00:02<00:00, 60.81it/s][A100%|##########| 157/157 [00:02<00:00, 61.16it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 57.20it/s][A
  8%|7         | 12/157 [00:00<00:02, 56.34it/s][A
 11%|#1        | 18/157 [00:00<00:02, 57.04it/s][A
 16%|#5        | 25/157 [00:00<00:02, 58.39it/s][A
 20%|##        | 32/157 [00:00<00:02, 58.96it/s][A
 25%|##4       | 39/157 [00:00<00:01, 59.86it/s][A
 29%|##9       | 46/157 [00:00<00:01, 61.07it/s][A
 34%|###3      | 53/157 [00:00<00:01, 62.05it/s][A
 38%|###8      | 60/157 [00:00<00:01, 62.75it/s][A
 43%|####2     | 67/157 [00:01<00:01, 62.88it/s][A
 47%|####7     | 74/157 [00:01<00:01, 61.23it/s][A
 52%|#####1    | 81/157 [00:01<00:01, 61.48it/s][A
 56%|#####6    | 88/157 [00:01<00:01, 62.38it/s][A
 61%|######    | 95/157 [00:01<00:00, 62.78it/s][A
 65%|######4   | 102/157 [00:01<00:00, 63.22it/s][A
 69%|######9   | 109/157 [00:01<00:00, 63.09it/s][A
 74%|#######3  | 116/157 [00:01<00:00, 63.38it/s][A
 78%|#######8  | 123/157 [00:01<00:00, 63.57it/s][A
 83%|########2 | 130/157 [00:02<00:00, 63.47it/s][A
 87%|########7 | 137/157 [00:02<00:00, 62.12it/s][A
 92%|#########1| 144/157 [00:02<00:00, 62.16it/s][A
 96%|#########6| 151/157 [00:02<00:00, 61.87it/s][A100%|##########| 157/157 [00:02<00:00, 61.77it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 59.69it/s][A
  8%|7         | 12/157 [00:00<00:02, 59.06it/s][A
 11%|#1        | 18/157 [00:00<00:02, 59.12it/s][A
 16%|#5        | 25/157 [00:00<00:02, 60.16it/s][A
 20%|##        | 32/157 [00:00<00:02, 60.30it/s][A
 25%|##4       | 39/157 [00:00<00:02, 58.59it/s][A
 29%|##9       | 46/157 [00:00<00:01, 59.35it/s][A
 34%|###3      | 53/157 [00:00<00:01, 59.50it/s][A
 38%|###8      | 60/157 [00:01<00:01, 60.36it/s][A
 43%|####2     | 67/157 [00:01<00:01, 61.10it/s][A
 47%|####7     | 74/157 [00:01<00:01, 61.78it/s][A
 52%|#####1    | 81/157 [00:01<00:01, 62.09it/s][A
 56%|#####6    | 88/157 [00:01<00:01, 62.26it/s][A
 61%|######    | 95/157 [00:01<00:01, 61.36it/s][A
 65%|######4   | 102/157 [00:01<00:00, 60.14it/s][A
 69%|######9   | 109/157 [00:01<00:00, 60.46it/s][A
 74%|#######3  | 116/157 [00:01<00:00, 61.34it/s][A
 78%|#######8  | 123/157 [00:02<00:00, 61.72it/s][A
 83%|########2 | 130/157 [00:02<00:00, 62.10it/s][A
 87%|########7 | 137/157 [00:02<00:00, 61.74it/s][A
 92%|#########1| 144/157 [00:02<00:00, 61.56it/s][A
 96%|#########6| 151/157 [00:02<00:00, 61.91it/s][A100%|##########| 157/157 [00:02<00:00, 60.99it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 58.57it/s][A
  8%|8         | 13/157 [00:00<00:02, 59.82it/s][A
 13%|#2        | 20/157 [00:00<00:02, 60.05it/s][A
 17%|#7        | 27/157 [00:00<00:02, 60.91it/s][A
 22%|##1       | 34/157 [00:00<00:01, 61.59it/s][A
 26%|##6       | 41/157 [00:00<00:01, 61.92it/s][A
 31%|###       | 48/157 [00:00<00:01, 62.03it/s][A
 35%|###5      | 55/157 [00:00<00:01, 62.24it/s][A
 39%|###9      | 62/157 [00:01<00:01, 60.56it/s][A
 44%|####3     | 69/157 [00:01<00:01, 59.31it/s][A
 48%|####8     | 76/157 [00:01<00:01, 60.05it/s][A
 53%|#####2    | 83/157 [00:01<00:01, 60.16it/s][A
 57%|#####7    | 90/157 [00:01<00:01, 60.34it/s][A
 62%|######1   | 97/157 [00:01<00:00, 60.87it/s][A
 66%|######6   | 104/157 [00:01<00:00, 61.21it/s][A
 71%|#######   | 111/157 [00:01<00:00, 60.78it/s][A
 75%|#######5  | 118/157 [00:01<00:00, 60.17it/s][A
 80%|#######9  | 125/157 [00:02<00:00, 59.74it/s][A
 84%|########4 | 132/157 [00:02<00:00, 60.27it/s][A
 89%|########8 | 139/157 [00:02<00:00, 61.16it/s][A
 93%|#########2| 146/157 [00:02<00:00, 61.96it/s][A
 97%|#########7| 153/157 [00:02<00:00, 62.21it/s][A100%|##########| 157/157 [00:02<00:00, 61.00it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|4         | 7/157 [00:00<00:02, 60.20it/s][A
  9%|8         | 14/157 [00:00<00:02, 59.42it/s][A
 13%|#2        | 20/157 [00:00<00:02, 57.21it/s][A
 17%|#6        | 26/157 [00:00<00:02, 57.76it/s][A
 21%|##1       | 33/157 [00:00<00:02, 59.06it/s][A
 25%|##5       | 40/157 [00:00<00:01, 59.39it/s][A
 30%|##9       | 47/157 [00:00<00:01, 60.20it/s][A
 34%|###4      | 54/157 [00:00<00:01, 60.67it/s][A
 39%|###8      | 61/157 [00:01<00:01, 60.78it/s][A
 43%|####3     | 68/157 [00:01<00:03, 25.91it/s][A
 48%|####7     | 75/157 [00:01<00:02, 31.52it/s][A
 52%|#####2    | 82/157 [00:01<00:02, 37.06it/s][A
 57%|#####6    | 89/157 [00:01<00:01, 42.32it/s][A
 61%|######1   | 96/157 [00:02<00:01, 46.86it/s][A
 65%|######4   | 102/157 [00:02<00:01, 49.71it/s][A
 69%|######9   | 109/157 [00:02<00:00, 52.97it/s][A
 74%|#######3  | 116/157 [00:02<00:00, 55.55it/s][A
 78%|#######8  | 123/157 [00:02<00:00, 57.44it/s][A
 83%|########2 | 130/157 [00:02<00:00, 56.85it/s][A
 87%|########7 | 137/157 [00:02<00:00, 58.62it/s][A
 92%|#########1| 144/157 [00:02<00:00, 59.93it/s][A
 96%|#########6| 151/157 [00:02<00:00, 60.99it/s][A100%|##########| 157/157 [00:03<00:00, 51.07it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: qat, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|4         | 7/157 [00:00<00:02, 60.74it/s][A
  9%|8         | 14/157 [00:00<00:02, 61.58it/s][A
 13%|#3        | 21/157 [00:00<00:02, 60.52it/s][A
 18%|#7        | 28/157 [00:00<00:02, 60.27it/s][A
 22%|##2       | 35/157 [00:00<00:02, 60.19it/s][A
 27%|##6       | 42/157 [00:00<00:01, 60.45it/s][A
 31%|###1      | 49/157 [00:00<00:01, 61.26it/s][A
 36%|###5      | 56/157 [00:00<00:01, 61.81it/s][A
 40%|####      | 63/157 [00:01<00:01, 61.73it/s][A
 45%|####4     | 70/157 [00:01<00:01, 61.83it/s][A
 49%|####9     | 77/157 [00:01<00:01, 61.62it/s][A
 54%|#####3    | 84/157 [00:01<00:01, 62.32it/s][A
 58%|#####7    | 91/157 [00:01<00:01, 60.65it/s][A
 62%|######2   | 98/157 [00:01<00:00, 61.33it/s][A
 67%|######6   | 105/157 [00:01<00:00, 62.18it/s][A
 71%|#######1  | 112/157 [00:01<00:00, 63.33it/s][A
 76%|#######5  | 119/157 [00:01<00:00, 64.17it/s][A
 80%|########  | 126/157 [00:02<00:00, 64.31it/s][A
 85%|########4 | 133/157 [00:02<00:00, 64.89it/s][A
 89%|########9 | 140/157 [00:02<00:00, 65.29it/s][A
 94%|#########3| 147/157 [00:02<00:00, 65.49it/s][A
 98%|#########8| 154/157 [00:02<00:00, 65.53it/s][A100%|##########| 157/157 [00:02<00:00, 62.56it/s]

  0%|          | 0/313 [00:00<?, ?it/s][A  0%|          | 0/313 [00:00<?, ?it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 59.23it/s][A
  8%|7         | 12/157 [00:00<00:02, 59.46it/s][A
 11%|#1        | 18/157 [00:00<00:02, 59.49it/s][A
 16%|#5        | 25/157 [00:00<00:02, 60.72it/s][A
 20%|##        | 32/157 [00:00<00:02, 61.31it/s][A
 25%|##4       | 39/157 [00:00<00:01, 61.39it/s][A
 29%|##9       | 46/157 [00:00<00:01, 61.16it/s][A
 34%|###3      | 53/157 [00:00<00:01, 59.40it/s][A
 38%|###8      | 60/157 [00:00<00:01, 60.07it/s][A
 43%|####2     | 67/157 [00:01<00:01, 60.61it/s][A
 47%|####7     | 74/157 [00:01<00:01, 61.17it/s][A
 52%|#####1    | 81/157 [00:01<00:01, 61.30it/s][A
 56%|#####6    | 88/157 [00:01<00:01, 61.69it/s][A
 61%|######    | 95/157 [00:01<00:00, 62.07it/s][A
 65%|######4   | 102/157 [00:01<00:00, 61.93it/s][A
 69%|######9   | 109/157 [00:01<00:00, 61.89it/s][A
 74%|#######3  | 116/157 [00:01<00:00, 59.91it/s][A
 78%|#######8  | 123/157 [00:02<00:00, 60.46it/s][A
 83%|########2 | 130/157 [00:02<00:00, 60.80it/s][A
 87%|########7 | 137/157 [00:02<00:00, 61.58it/s][A
 92%|#########1| 144/157 [00:02<00:00, 61.86it/s][A
 96%|#########6| 151/157 [00:02<00:00, 62.00it/s][A100%|##########| 157/157 [00:02<00:00, 61.24it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|4         | 7/157 [00:00<00:02, 60.55it/s][A
  9%|8         | 14/157 [00:00<00:02, 62.55it/s][A
 13%|#3        | 21/157 [00:00<00:02, 60.13it/s][A
 18%|#7        | 28/157 [00:00<00:02, 61.60it/s][A
 22%|##2       | 35/157 [00:00<00:01, 62.20it/s][A
 27%|##6       | 42/157 [00:00<00:01, 62.44it/s][A
 31%|###1      | 49/157 [00:00<00:01, 62.08it/s][A
 36%|###5      | 56/157 [00:00<00:01, 62.45it/s][A
 40%|####      | 63/157 [00:01<00:01, 62.31it/s][A
 45%|####4     | 70/157 [00:01<00:01, 62.79it/s][A
 49%|####9     | 77/157 [00:01<00:01, 62.80it/s][A
 54%|#####3    | 84/157 [00:01<00:01, 61.46it/s][A
 58%|#####7    | 91/157 [00:01<00:01, 62.19it/s][A
 62%|######2   | 98/157 [00:01<00:00, 62.01it/s][A
 67%|######6   | 105/157 [00:01<00:00, 62.36it/s][A
 71%|#######1  | 112/157 [00:01<00:00, 62.68it/s][A
 76%|#######5  | 119/157 [00:01<00:00, 62.78it/s][A
 80%|########  | 126/157 [00:02<00:00, 63.41it/s][A
 85%|########4 | 133/157 [00:02<00:00, 64.08it/s][A
 89%|########9 | 140/157 [00:02<00:00, 63.49it/s][A
 94%|#########3| 147/157 [00:02<00:00, 62.10it/s][A
 98%|#########8| 154/157 [00:02<00:00, 62.64it/s][A100%|##########| 157/157 [00:02<00:00, 62.52it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|4         | 7/157 [00:00<00:02, 60.91it/s][A
  9%|8         | 14/157 [00:00<00:02, 60.53it/s][A
 13%|#3        | 21/157 [00:00<00:02, 60.61it/s][A
 18%|#7        | 28/157 [00:00<00:02, 60.33it/s][A
 22%|##2       | 35/157 [00:00<00:02, 60.95it/s][A
 27%|##6       | 42/157 [00:00<00:01, 60.06it/s][A
 31%|###1      | 49/157 [00:00<00:01, 60.47it/s][A
 36%|###5      | 56/157 [00:00<00:01, 60.83it/s][A
 40%|####      | 63/157 [00:01<00:01, 61.44it/s][A
 45%|####4     | 70/157 [00:01<00:01, 61.90it/s][A
 49%|####9     | 77/157 [00:01<00:01, 62.44it/s][A
 54%|#####3    | 84/157 [00:01<00:01, 62.58it/s][A
 58%|#####7    | 91/157 [00:01<00:01, 62.15it/s][A
 62%|######2   | 98/157 [00:01<00:00, 62.13it/s][A
 67%|######6   | 105/157 [00:01<00:00, 60.34it/s][A
 71%|#######1  | 112/157 [00:01<00:00, 60.75it/s][A
 76%|#######5  | 119/157 [00:01<00:00, 61.35it/s][A
 80%|########  | 126/157 [00:02<00:00, 61.81it/s][A
 85%|########4 | 133/157 [00:02<00:00, 62.35it/s][A
 89%|########9 | 140/157 [00:02<00:00, 62.22it/s][A
 94%|#########3| 147/157 [00:02<00:00, 62.30it/s][A
 98%|#########8| 154/157 [00:02<00:00, 62.55it/s][A100%|##########| 157/157 [00:02<00:00, 61.58it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: qat, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 53.78it/s][A
  8%|8         | 13/157 [00:00<00:02, 58.82it/s][A
 13%|#2        | 20/157 [00:00<00:02, 60.80it/s][A
 17%|#7        | 27/157 [00:00<00:02, 61.66it/s][A
 22%|##1       | 34/157 [00:00<00:01, 62.96it/s][A
 26%|##6       | 41/157 [00:00<00:01, 62.85it/s][A
 31%|###       | 48/157 [00:00<00:01, 62.39it/s][A
 35%|###5      | 55/157 [00:00<00:01, 62.11it/s][A
 39%|###9      | 62/157 [00:01<00:01, 62.28it/s][A
 44%|####3     | 69/157 [00:01<00:01, 59.93it/s][A
 48%|####8     | 76/157 [00:01<00:01, 60.90it/s][A
 53%|#####2    | 83/157 [00:01<00:01, 61.58it/s][A
 57%|#####7    | 90/157 [00:01<00:01, 62.16it/s][A
 62%|######1   | 97/157 [00:01<00:00, 62.36it/s][A
 66%|######6   | 104/157 [00:01<00:00, 62.32it/s][A
 71%|#######   | 111/157 [00:01<00:00, 62.53it/s][A
 75%|#######5  | 118/157 [00:01<00:00, 62.89it/s][A
 80%|#######9  | 125/157 [00:02<00:00, 63.22it/s][A
 84%|########4 | 132/157 [00:02<00:00, 61.55it/s][A
 89%|########8 | 139/157 [00:02<00:00, 62.32it/s][A
 93%|#########2| 146/157 [00:02<00:00, 62.97it/s][A
 97%|#########7| 153/157 [00:02<00:00, 63.51it/s][A100%|##########| 157/157 [00:02<00:00, 62.20it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|4         | 7/157 [00:00<00:02, 61.23it/s][A
  9%|8         | 14/157 [00:00<00:02, 60.45it/s][A
 13%|#3        | 21/157 [00:00<00:02, 60.37it/s][A
 18%|#7        | 28/157 [00:00<00:02, 60.07it/s][A
 22%|##2       | 35/157 [00:00<00:02, 59.44it/s][A
 27%|##6       | 42/157 [00:00<00:01, 59.79it/s][A
 31%|###1      | 49/157 [00:00<00:01, 60.11it/s][A
 36%|###5      | 56/157 [00:00<00:01, 60.25it/s][A
 40%|####      | 63/157 [00:01<00:01, 60.19it/s][A
 45%|####4     | 70/157 [00:01<00:01, 60.96it/s][A
 49%|####9     | 77/157 [00:01<00:01, 61.54it/s][A
 54%|#####3    | 84/157 [00:01<00:01, 62.01it/s][A
 58%|#####7    | 91/157 [00:01<00:01, 60.23it/s][A
 62%|######2   | 98/157 [00:01<00:00, 60.59it/s][A
 67%|######6   | 105/157 [00:01<00:00, 60.74it/s][A
 71%|#######1  | 112/157 [00:01<00:00, 61.36it/s][A
 76%|#######5  | 119/157 [00:01<00:00, 61.94it/s][A
 80%|########  | 126/157 [00:02<00:00, 62.12it/s][A
 85%|########4 | 133/157 [00:02<00:00, 62.24it/s][A
 89%|########9 | 140/157 [00:02<00:00, 62.87it/s][A
 94%|#########3| 147/157 [00:02<00:00, 63.01it/s][A
 98%|#########8| 154/157 [00:02<00:00, 61.37it/s][A100%|##########| 157/157 [00:02<00:00, 61.20it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|4         | 7/157 [00:00<00:02, 60.92it/s][A
  9%|8         | 14/157 [00:00<00:02, 62.03it/s][A
 13%|#3        | 21/157 [00:00<00:02, 62.32it/s][A
 18%|#7        | 28/157 [00:00<00:02, 62.72it/s][A
 22%|##2       | 35/157 [00:00<00:01, 62.81it/s][A
 27%|##6       | 42/157 [00:00<00:01, 62.81it/s][A
 31%|###1      | 49/157 [00:00<00:01, 62.31it/s][A
 36%|###5      | 56/157 [00:00<00:01, 60.44it/s][A
 40%|####      | 63/157 [00:01<00:01, 60.63it/s][A
 45%|####4     | 70/157 [00:01<00:01, 61.28it/s][A
 49%|####9     | 77/157 [00:01<00:01, 61.90it/s][A
 54%|#####3    | 84/157 [00:01<00:01, 62.65it/s][A
 58%|#####7    | 91/157 [00:01<00:01, 62.59it/s][A
 62%|######2   | 98/157 [00:01<00:00, 62.07it/s][A
 67%|######6   | 105/157 [00:01<00:00, 62.09it/s][A
 71%|#######1  | 112/157 [00:01<00:00, 62.85it/s][A
 76%|#######5  | 119/157 [00:01<00:00, 61.53it/s][A
 80%|########  | 126/157 [00:02<00:00, 62.94it/s][A
 85%|########4 | 133/157 [00:02<00:00, 63.34it/s][A
 89%|########9 | 140/157 [00:02<00:00, 63.80it/s][A
 94%|#########3| 147/157 [00:02<00:00, 64.18it/s][A
 98%|#########8| 154/157 [00:02<00:00, 64.06it/s][A100%|##########| 157/157 [00:02<00:00, 62.67it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 58.35it/s][A
  8%|8         | 13/157 [00:00<00:02, 60.29it/s][A
 13%|#2        | 20/157 [00:00<00:02, 57.53it/s][A
 17%|#7        | 27/157 [00:00<00:02, 58.81it/s][A
 22%|##1       | 34/157 [00:00<00:02, 59.82it/s][A
 26%|##6       | 41/157 [00:00<00:01, 61.10it/s][A
 31%|###       | 48/157 [00:00<00:01, 61.66it/s][A
 35%|###5      | 55/157 [00:00<00:01, 62.10it/s][A
 39%|###9      | 62/157 [00:01<00:01, 61.90it/s][A
 44%|####3     | 69/157 [00:01<00:01, 62.38it/s][A
 48%|####8     | 76/157 [00:01<00:01, 63.07it/s][A
 53%|#####2    | 83/157 [00:01<00:01, 61.09it/s][A
 57%|#####7    | 90/157 [00:01<00:01, 61.13it/s][A
 62%|######1   | 97/157 [00:01<00:00, 60.94it/s][A
 66%|######6   | 104/157 [00:01<00:00, 61.27it/s][A
 71%|#######   | 111/157 [00:01<00:00, 61.47it/s][A
 75%|#######5  | 118/157 [00:01<00:00, 61.93it/s][A
 80%|#######9  | 125/157 [00:02<00:00, 62.08it/s][A
 84%|########4 | 132/157 [00:02<00:00, 62.39it/s][A
 89%|########8 | 139/157 [00:02<00:00, 61.89it/s][A
 93%|#########2| 146/157 [00:02<00:00, 60.87it/s][A
 97%|#########7| 153/157 [00:02<00:00, 62.20it/s][A100%|##########| 157/157 [00:02<00:00, 61.50it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: qat, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|4         | 7/157 [00:00<00:02, 62.57it/s][A
  9%|8         | 14/157 [00:00<00:02, 63.87it/s][A
 13%|#3        | 21/157 [00:00<00:02, 64.37it/s][A
 18%|#7        | 28/157 [00:00<00:02, 64.13it/s][A
 22%|##2       | 35/157 [00:00<00:01, 62.25it/s][A
 27%|##6       | 42/157 [00:00<00:01, 60.36it/s][A
 31%|###1      | 49/157 [00:00<00:01, 60.26it/s][A
 36%|###5      | 56/157 [00:00<00:01, 60.96it/s][A
 40%|####      | 63/157 [00:01<00:01, 62.11it/s][A
 45%|####4     | 70/157 [00:01<00:01, 63.04it/s][A
 49%|####9     | 77/157 [00:01<00:01, 63.53it/s][A
 54%|#####3    | 84/157 [00:01<00:01, 63.96it/s][A
 58%|#####7    | 91/157 [00:01<00:01, 64.30it/s][A
 62%|######2   | 98/157 [00:01<00:00, 64.64it/s][A
 67%|######6   | 105/157 [00:01<00:00, 63.10it/s][A
 71%|#######1  | 112/157 [00:01<00:00, 62.87it/s][A
 76%|#######5  | 119/157 [00:01<00:00, 63.47it/s][A
 80%|########  | 126/157 [00:01<00:00, 64.00it/s][A
 85%|########4 | 133/157 [00:02<00:00, 63.95it/s][A
 89%|########9 | 140/157 [00:02<00:00, 63.76it/s][A
 94%|#########3| 147/157 [00:02<00:00, 63.85it/s][A
 98%|#########8| 154/157 [00:02<00:00, 63.29it/s][A100%|##########| 157/157 [00:02<00:00, 63.17it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: qat, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 57.75it/s][A
  8%|7         | 12/157 [00:00<00:02, 56.75it/s][A
 12%|#2        | 19/157 [00:00<00:02, 58.36it/s][A
 17%|#6        | 26/157 [00:00<00:02, 59.24it/s][A
 21%|##1       | 33/157 [00:00<00:02, 59.09it/s][A
 25%|##4       | 39/157 [00:00<00:02, 58.84it/s][A
 29%|##8       | 45/157 [00:00<00:01, 58.82it/s][A
 33%|###3      | 52/157 [00:00<00:01, 59.47it/s][A
 38%|###7      | 59/157 [00:00<00:01, 60.19it/s][A
 42%|####2     | 66/157 [00:01<00:01, 59.48it/s][A
 46%|####6     | 73/157 [00:01<00:01, 59.93it/s][A
 51%|#####     | 80/157 [00:01<00:01, 60.75it/s][A
 55%|#####5    | 87/157 [00:01<00:01, 61.31it/s][A
 60%|#####9    | 94/157 [00:01<00:01, 61.75it/s][A
 64%|######4   | 101/157 [00:01<00:00, 61.95it/s][A
 69%|######8   | 108/157 [00:01<00:00, 62.37it/s][A
 73%|#######3  | 115/157 [00:01<00:00, 62.48it/s][A
 78%|#######7  | 122/157 [00:02<00:00, 62.17it/s][A
 82%|########2 | 129/157 [00:02<00:00, 60.89it/s][A
 87%|########6 | 136/157 [00:02<00:00, 61.62it/s][A
 91%|#########1| 143/157 [00:02<00:00, 61.95it/s][A
 96%|#########5| 150/157 [00:02<00:00, 61.38it/s][A
100%|##########| 157/157 [00:02<00:00, 62.24it/s][A100%|##########| 157/157 [00:02<00:00, 60.80it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|4         | 7/157 [00:00<00:02, 61.85it/s][A
  9%|8         | 14/157 [00:00<00:02, 62.60it/s][A
 13%|#3        | 21/157 [00:00<00:02, 61.82it/s][A
 18%|#7        | 28/157 [00:00<00:02, 59.86it/s][A
 22%|##1       | 34/157 [00:00<00:02, 58.40it/s][A
 26%|##6       | 41/157 [00:00<00:01, 59.53it/s][A
 31%|###       | 48/157 [00:00<00:01, 60.59it/s][A
 35%|###5      | 55/157 [00:00<00:01, 61.17it/s][A
 39%|###9      | 62/157 [00:01<00:01, 61.82it/s][A
 44%|####3     | 69/157 [00:01<00:01, 62.14it/s][A
 48%|####8     | 76/157 [00:01<00:01, 62.43it/s][A
 53%|#####2    | 83/157 [00:01<00:01, 62.21it/s][A
 57%|#####7    | 90/157 [00:01<00:01, 61.36it/s][A
 62%|######1   | 97/157 [00:01<00:00, 60.59it/s][A
 66%|######6   | 104/157 [00:01<00:00, 61.22it/s][A
 71%|#######   | 111/157 [00:01<00:00, 62.24it/s][A
 75%|#######5  | 118/157 [00:01<00:00, 62.75it/s][A
 80%|#######9  | 125/157 [00:02<00:00, 63.20it/s][A
 84%|########4 | 132/157 [00:02<00:00, 63.23it/s][A
 89%|########8 | 139/157 [00:02<00:00, 63.62it/s][A
 93%|#########2| 146/157 [00:02<00:00, 63.66it/s][A
 97%|#########7| 153/157 [00:02<00:00, 62.62it/s][A100%|##########| 157/157 [00:02<00:00, 61.81it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: qat, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 57.96it/s][A
  8%|8         | 13/157 [00:00<00:02, 59.36it/s][A
 12%|#2        | 19/157 [00:00<00:02, 59.14it/s][A
 17%|#6        | 26/157 [00:00<00:02, 60.33it/s][A
 21%|##1       | 33/157 [00:00<00:02, 61.29it/s][A
 25%|##5       | 40/157 [00:00<00:01, 60.62it/s][A
 30%|##9       | 47/157 [00:00<00:01, 60.78it/s][A
 34%|###4      | 54/157 [00:00<00:01, 59.21it/s][A
 39%|###8      | 61/157 [00:01<00:01, 60.06it/s][A
 43%|####3     | 68/157 [00:01<00:01, 60.76it/s][A
 48%|####7     | 75/157 [00:01<00:01, 61.55it/s][A
 52%|#####2    | 82/157 [00:01<00:01, 61.94it/s][A
 57%|#####6    | 89/157 [00:01<00:01, 62.02it/s][A
 61%|######1   | 96/157 [00:01<00:00, 62.17it/s][A
 66%|######5   | 103/157 [00:01<00:00, 62.28it/s][A
 70%|#######   | 110/157 [00:01<00:00, 62.37it/s][A
 75%|#######4  | 117/157 [00:01<00:00, 60.89it/s][A
 79%|#######8  | 124/157 [00:02<00:00, 62.45it/s][A
 83%|########3 | 131/157 [00:02<00:00, 63.37it/s][A
 88%|########7 | 138/157 [00:02<00:00, 63.58it/s][A
 92%|#########2| 145/157 [00:02<00:00, 63.83it/s][A
 97%|#########6| 152/157 [00:02<00:00, 64.15it/s][A100%|##########| 157/157 [00:02<00:00, 62.03it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|4         | 7/157 [00:00<00:02, 61.09it/s][A
  9%|8         | 14/157 [00:00<00:02, 62.05it/s][A
 13%|#3        | 21/157 [00:00<00:02, 59.48it/s][A
 18%|#7        | 28/157 [00:00<00:02, 59.29it/s][A
 22%|##2       | 35/157 [00:00<00:02, 60.54it/s][A
 27%|##6       | 42/157 [00:00<00:01, 61.30it/s][A
 31%|###1      | 49/157 [00:00<00:01, 61.87it/s][A
 36%|###5      | 56/157 [00:00<00:01, 62.14it/s][A
 40%|####      | 63/157 [00:01<00:01, 62.54it/s][A
 45%|####4     | 70/157 [00:01<00:01, 62.01it/s][A
 49%|####9     | 77/157 [00:01<00:01, 61.41it/s][A
 54%|#####3    | 84/157 [00:01<00:01, 60.45it/s][A
 58%|#####7    | 91/157 [00:01<00:01, 61.38it/s][A
 62%|######2   | 98/157 [00:01<00:00, 62.21it/s][A
 67%|######6   | 105/157 [00:01<00:00, 62.50it/s][A
 71%|#######1  | 112/157 [00:01<00:00, 62.86it/s][A
 76%|#######5  | 119/157 [00:01<00:00, 62.88it/s][A
 80%|########  | 126/157 [00:02<00:00, 62.01it/s][A
 85%|########4 | 133/157 [00:02<00:00, 62.46it/s][A
 89%|########9 | 140/157 [00:02<00:00, 61.57it/s][A
 94%|#########3| 147/157 [00:02<00:00, 60.72it/s][A
 98%|#########8| 154/157 [00:02<00:00, 61.39it/s][A100%|##########| 157/157 [00:02<00:00, 61.66it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 57.27it/s][A
  8%|7         | 12/157 [00:00<00:02, 58.25it/s][A
 11%|#1        | 18/157 [00:00<00:02, 58.22it/s][A
 16%|#5        | 25/157 [00:00<00:02, 58.93it/s][A
 20%|##        | 32/157 [00:00<00:02, 60.59it/s][A
 25%|##4       | 39/157 [00:00<00:01, 60.26it/s][A
 29%|##9       | 46/157 [00:00<00:01, 59.85it/s][A
 34%|###3      | 53/157 [00:00<00:01, 60.42it/s][A
 38%|###8      | 60/157 [00:00<00:01, 61.17it/s][A
 43%|####2     | 67/157 [00:01<00:01, 61.68it/s][A
 47%|####7     | 74/157 [00:01<00:01, 61.76it/s][A
 52%|#####1    | 81/157 [00:01<00:01, 61.23it/s][A
 56%|#####6    | 88/157 [00:01<00:01, 61.24it/s][A
 61%|######    | 95/157 [00:01<00:01, 61.70it/s][A
 65%|######4   | 102/157 [00:01<00:00, 60.48it/s][A
 69%|######9   | 109/157 [00:01<00:00, 61.21it/s][A
 74%|#######3  | 116/157 [00:01<00:00, 61.82it/s][A
 78%|#######8  | 123/157 [00:02<00:00, 61.69it/s][A
 83%|########2 | 130/157 [00:02<00:00, 61.87it/s][A
 87%|########7 | 137/157 [00:02<00:00, 62.29it/s][A
 92%|#########1| 144/157 [00:02<00:00, 62.86it/s][A
 96%|#########6| 151/157 [00:02<00:00, 62.92it/s][A100%|##########| 157/157 [00:02<00:00, 61.35it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 52.36it/s][A
  8%|8         | 13/157 [00:00<00:02, 58.09it/s][A
 13%|#2        | 20/157 [00:00<00:02, 60.42it/s][A
 17%|#7        | 27/157 [00:00<00:02, 60.27it/s][A
 22%|##1       | 34/157 [00:00<00:02, 60.99it/s][A
 26%|##6       | 41/157 [00:00<00:01, 61.88it/s][A
 31%|###       | 48/157 [00:00<00:01, 62.30it/s][A
 35%|###5      | 55/157 [00:00<00:01, 62.01it/s][A
 39%|###9      | 62/157 [00:01<00:01, 62.42it/s][A
 44%|####3     | 69/157 [00:01<00:01, 61.04it/s][A
 48%|####8     | 76/157 [00:01<00:01, 61.76it/s][A
 53%|#####2    | 83/157 [00:01<00:01, 62.06it/s][A
 57%|#####7    | 90/157 [00:01<00:01, 62.65it/s][A
 62%|######1   | 97/157 [00:01<00:00, 62.92it/s][A
 66%|######6   | 104/157 [00:01<00:00, 62.82it/s][A
 71%|#######   | 111/157 [00:01<00:00, 62.97it/s][A
 75%|#######5  | 118/157 [00:01<00:00, 63.35it/s][A
 80%|#######9  | 125/157 [00:02<00:00, 63.32it/s][A
 84%|########4 | 132/157 [00:02<00:00, 61.75it/s][A
 89%|########8 | 139/157 [00:02<00:00, 62.37it/s][A
 93%|#########2| 146/157 [00:02<00:00, 62.66it/s][A
 97%|#########7| 153/157 [00:02<00:00, 63.65it/s][A100%|##########| 157/157 [00:02<00:00, 62.22it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 58.51it/s][A
  8%|8         | 13/157 [00:00<00:02, 59.37it/s][A
 13%|#2        | 20/157 [00:00<00:02, 59.81it/s][A
 17%|#6        | 26/157 [00:00<00:02, 59.42it/s][A
 20%|##        | 32/157 [00:00<00:02, 59.28it/s][A
 25%|##4       | 39/157 [00:00<00:01, 60.47it/s][A
 29%|##9       | 46/157 [00:00<00:01, 61.40it/s][A
 34%|###3      | 53/157 [00:00<00:01, 61.46it/s][A
 38%|###8      | 60/157 [00:00<00:01, 61.56it/s][A
 43%|####2     | 67/157 [00:01<00:01, 62.07it/s][A
 47%|####7     | 74/157 [00:01<00:01, 62.44it/s][A
 52%|#####1    | 81/157 [00:01<00:01, 62.55it/s][A
 56%|#####6    | 88/157 [00:01<00:01, 62.01it/s][A
 61%|######    | 95/157 [00:01<00:01, 60.45it/s][A
 65%|######4   | 102/157 [00:01<00:00, 61.24it/s][A
 69%|######9   | 109/157 [00:01<00:00, 61.96it/s][A
 74%|#######3  | 116/157 [00:01<00:00, 62.36it/s][A
 78%|#######8  | 123/157 [00:02<00:00, 62.71it/s][A
 83%|########2 | 130/157 [00:02<00:00, 63.19it/s][A
 87%|########7 | 137/157 [00:02<00:00, 63.43it/s][A
 92%|#########1| 144/157 [00:02<00:00, 63.19it/s][A
 96%|#########6| 151/157 [00:02<00:00, 62.47it/s][A100%|##########| 157/157 [00:02<00:00, 61.71it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: qat, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 55.62it/s][A
  8%|8         | 13/157 [00:00<00:02, 59.08it/s][A
 13%|#2        | 20/157 [00:00<00:02, 59.98it/s][A
 17%|#7        | 27/157 [00:00<00:02, 60.47it/s][A
 22%|##1       | 34/157 [00:00<00:02, 61.13it/s][A
 26%|##6       | 41/157 [00:00<00:01, 61.25it/s][A
 31%|###       | 48/157 [00:00<00:01, 60.80it/s][A
 35%|###5      | 55/157 [00:00<00:01, 58.91it/s][A
 39%|###9      | 62/157 [00:01<00:01, 60.07it/s][A
 44%|####3     | 69/157 [00:01<00:01, 60.98it/s][A
 48%|####8     | 76/157 [00:01<00:01, 61.40it/s][A
 53%|#####2    | 83/157 [00:01<00:01, 61.48it/s][A
 57%|#####7    | 90/157 [00:01<00:01, 61.76it/s][A
 62%|######1   | 97/157 [00:01<00:00, 61.85it/s][A
 66%|######6   | 104/157 [00:01<00:00, 62.02it/s][A
 71%|#######   | 111/157 [00:01<00:00, 61.90it/s][A
 75%|#######5  | 118/157 [00:01<00:00, 60.44it/s][A
 80%|#######9  | 125/157 [00:02<00:00, 61.20it/s][A
 84%|########4 | 132/157 [00:02<00:00, 62.06it/s][A
 89%|########8 | 139/157 [00:02<00:00, 62.37it/s][A
 93%|#########2| 146/157 [00:02<00:00, 62.75it/s][A
 97%|#########7| 153/157 [00:02<00:00, 62.45it/s][A100%|##########| 157/157 [00:02<00:00, 61.37it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 58.17it/s][A
  8%|7         | 12/157 [00:00<00:02, 55.56it/s][A
 11%|#1        | 18/157 [00:00<00:02, 55.84it/s][A
 16%|#5        | 25/157 [00:00<00:02, 57.58it/s][A
 20%|##        | 32/157 [00:00<00:02, 59.57it/s][A
 25%|##4       | 39/157 [00:00<00:01, 60.55it/s][A
 29%|##9       | 46/157 [00:00<00:01, 61.11it/s][A
 34%|###3      | 53/157 [00:00<00:01, 61.04it/s][A
 38%|###8      | 60/157 [00:01<00:01, 61.44it/s][A
 43%|####2     | 67/157 [00:01<00:01, 61.62it/s][A
 47%|####7     | 74/157 [00:01<00:01, 60.23it/s][A
 52%|#####1    | 81/157 [00:01<00:01, 60.25it/s][A
 56%|#####6    | 88/157 [00:01<00:01, 60.80it/s][A
 61%|######    | 95/157 [00:01<00:01, 61.17it/s][A
 65%|######4   | 102/157 [00:01<00:00, 61.52it/s][A
 69%|######9   | 109/157 [00:01<00:00, 61.44it/s][A
 74%|#######3  | 116/157 [00:01<00:00, 61.80it/s][A
 78%|#######8  | 123/157 [00:02<00:00, 61.71it/s][A
 83%|########2 | 130/157 [00:02<00:00, 61.84it/s][A
 87%|########7 | 137/157 [00:02<00:00, 60.39it/s][A
 92%|#########1| 144/157 [00:02<00:00, 60.45it/s][A
 96%|#########6| 151/157 [00:02<00:00, 61.26it/s][A100%|##########| 157/157 [00:02<00:00, 60.76it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 56.84it/s][A
  8%|7         | 12/157 [00:00<00:02, 58.62it/s][A
 12%|#2        | 19/157 [00:00<00:02, 60.37it/s][A
 17%|#6        | 26/157 [00:00<00:02, 59.42it/s][A
 21%|##1       | 33/157 [00:00<00:02, 59.42it/s][A
 25%|##4       | 39/157 [00:00<00:02, 58.96it/s][A
 29%|##9       | 46/157 [00:00<00:01, 60.07it/s][A
 34%|###3      | 53/157 [00:00<00:01, 61.30it/s][A
 38%|###8      | 60/157 [00:00<00:01, 61.91it/s][A
 43%|####2     | 67/157 [00:01<00:01, 61.71it/s][A
 47%|####7     | 74/157 [00:01<00:01, 61.06it/s][A
 52%|#####1    | 81/157 [00:01<00:01, 61.44it/s][A
 56%|#####6    | 88/157 [00:01<00:01, 61.80it/s][A
 61%|######    | 95/157 [00:01<00:01, 60.72it/s][A
 65%|######4   | 102/157 [00:01<00:00, 60.22it/s][A
 69%|######9   | 109/157 [00:01<00:00, 61.07it/s][A
 74%|#######3  | 116/157 [00:01<00:00, 61.71it/s][A
 78%|#######8  | 123/157 [00:02<00:00, 62.00it/s][A
 83%|########2 | 130/157 [00:02<00:00, 61.95it/s][A
 87%|########7 | 137/157 [00:02<00:00, 61.79it/s][A
 92%|#########1| 144/157 [00:02<00:00, 62.03it/s][A
 96%|#########6| 151/157 [00:02<00:00, 62.22it/s][A100%|##########| 157/157 [00:02<00:00, 61.10it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 58.44it/s][A
  8%|7         | 12/157 [00:00<00:02, 58.24it/s][A
 11%|#1        | 18/157 [00:00<00:02, 58.93it/s][A
 15%|#5        | 24/157 [00:00<00:02, 58.65it/s][A
 20%|#9        | 31/157 [00:00<00:02, 59.67it/s][A
 24%|##4       | 38/157 [00:00<00:01, 60.58it/s][A
 29%|##8       | 45/157 [00:00<00:01, 60.77it/s][A
 33%|###3      | 52/157 [00:00<00:01, 61.03it/s][A
 38%|###7      | 59/157 [00:00<00:01, 59.15it/s][A
 42%|####2     | 66/157 [00:01<00:01, 59.71it/s][A
 46%|####6     | 73/157 [00:01<00:01, 60.36it/s][A
 51%|#####     | 80/157 [00:01<00:01, 60.28it/s][A
 55%|#####5    | 87/157 [00:01<00:01, 60.68it/s][A
 60%|#####9    | 94/157 [00:01<00:01, 60.65it/s][A
 64%|######4   | 101/157 [00:01<00:00, 60.65it/s][A
 69%|######8   | 108/157 [00:01<00:00, 60.78it/s][A
 73%|#######3  | 115/157 [00:01<00:00, 60.20it/s][A
 78%|#######7  | 122/157 [00:02<00:00, 59.65it/s][A
 82%|########2 | 129/157 [00:02<00:00, 60.45it/s][A
 87%|########6 | 136/157 [00:02<00:00, 61.17it/s][A
 91%|#########1| 143/157 [00:02<00:00, 61.79it/s][A
 96%|#########5| 150/157 [00:02<00:00, 61.87it/s][A
100%|##########| 157/157 [00:02<00:00, 62.36it/s][A100%|##########| 157/157 [00:02<00:00, 60.62it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: qat, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 59.15it/s][A
  8%|7         | 12/157 [00:00<00:02, 58.11it/s][A
 11%|#1        | 18/157 [00:00<00:02, 56.57it/s][A
 16%|#5        | 25/157 [00:00<00:02, 58.63it/s][A
 20%|##        | 32/157 [00:00<00:02, 59.74it/s][A
 25%|##4       | 39/157 [00:00<00:01, 60.72it/s][A
 29%|##9       | 46/157 [00:00<00:01, 60.96it/s][A
 34%|###3      | 53/157 [00:00<00:01, 61.81it/s][A
 38%|###8      | 60/157 [00:00<00:01, 62.19it/s][A
 43%|####2     | 67/157 [00:01<00:01, 62.28it/s][A
 47%|####7     | 74/157 [00:01<00:01, 62.52it/s][A
 52%|#####1    | 81/157 [00:01<00:01, 60.02it/s][A
 56%|#####6    | 88/157 [00:01<00:01, 60.16it/s][A
 61%|######    | 95/157 [00:01<00:01, 60.31it/s][A
 65%|######4   | 102/157 [00:01<00:00, 60.28it/s][A
 69%|######9   | 109/157 [00:01<00:00, 60.71it/s][A
 74%|#######3  | 116/157 [00:01<00:00, 61.47it/s][A
 78%|#######8  | 123/157 [00:02<00:00, 61.72it/s][A
 83%|########2 | 130/157 [00:02<00:00, 62.16it/s][A
 87%|########7 | 137/157 [00:02<00:00, 62.50it/s][A
 92%|#########1| 144/157 [00:02<00:00, 60.42it/s][A
 96%|#########6| 151/157 [00:02<00:00, 61.15it/s][A100%|##########| 157/157 [00:02<00:00, 60.97it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: qat, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 57.39it/s][A
  8%|7         | 12/157 [00:00<00:02, 58.21it/s][A
 11%|#1        | 18/157 [00:00<00:02, 58.62it/s][A
 16%|#5        | 25/157 [00:00<00:02, 59.55it/s][A
 20%|##        | 32/157 [00:00<00:02, 60.31it/s][A
 25%|##4       | 39/157 [00:00<00:02, 58.75it/s][A
 29%|##9       | 46/157 [00:00<00:01, 59.36it/s][A
 34%|###3      | 53/157 [00:00<00:01, 59.87it/s][A
 38%|###8      | 60/157 [00:01<00:01, 60.29it/s][A
 43%|####2     | 67/157 [00:01<00:01, 60.54it/s][A
 47%|####7     | 74/157 [00:01<00:01, 60.97it/s][A
 52%|#####1    | 81/157 [00:01<00:01, 61.13it/s][A
 56%|#####6    | 88/157 [00:01<00:01, 61.23it/s][A
 61%|######    | 95/157 [00:01<00:01, 60.63it/s][A
 65%|######4   | 102/157 [00:01<00:00, 59.23it/s][A
 69%|######9   | 109/157 [00:01<00:00, 59.87it/s][A
 74%|#######3  | 116/157 [00:01<00:00, 60.46it/s][A
 78%|#######8  | 123/157 [00:02<00:00, 61.24it/s][A
 83%|########2 | 130/157 [00:02<00:00, 60.88it/s][A
 87%|########7 | 137/157 [00:02<00:00, 60.64it/s][A
 92%|#########1| 144/157 [00:02<00:00, 60.71it/s][A
 96%|#########6| 151/157 [00:02<00:00, 61.28it/s][A100%|##########| 157/157 [00:02<00:00, 60.47it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 58.16it/s][A
  8%|8         | 13/157 [00:00<00:02, 59.69it/s][A
 13%|#2        | 20/157 [00:00<00:02, 61.61it/s][A
 17%|#7        | 27/157 [00:00<00:02, 61.98it/s][A
 22%|##1       | 34/157 [00:00<00:02, 60.20it/s][A
 26%|##6       | 41/157 [00:00<00:01, 60.35it/s][A
 31%|###       | 48/157 [00:00<00:01, 60.80it/s][A
 35%|###5      | 55/157 [00:00<00:01, 61.44it/s][A
 39%|###9      | 62/157 [00:01<00:01, 59.10it/s][A
 43%|####3     | 68/157 [00:01<00:01, 59.10it/s][A
 48%|####7     | 75/157 [00:01<00:01, 59.98it/s][A
 52%|#####2    | 82/157 [00:01<00:01, 60.42it/s][A
 57%|#####6    | 89/157 [00:01<00:01, 60.47it/s][A
 61%|######1   | 96/157 [00:01<00:01, 60.41it/s][A
 66%|######5   | 103/157 [00:01<00:00, 61.27it/s][A
 70%|#######   | 110/157 [00:01<00:00, 61.54it/s][A
 75%|#######4  | 117/157 [00:01<00:00, 61.97it/s][A
 79%|#######8  | 124/157 [00:02<00:00, 60.20it/s][A
 83%|########3 | 131/157 [00:02<00:00, 60.93it/s][A
 88%|########7 | 138/157 [00:02<00:00, 61.63it/s][A
 92%|#########2| 145/157 [00:02<00:00, 62.43it/s][A
 97%|#########6| 152/157 [00:02<00:00, 63.62it/s][A100%|##########| 157/157 [00:02<00:00, 61.35it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: qat, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 59.50it/s][A
  8%|8         | 13/157 [00:00<00:02, 60.83it/s][A
 13%|#2        | 20/157 [00:00<00:02, 58.79it/s][A
 17%|#6        | 26/157 [00:00<00:02, 58.80it/s][A
 20%|##        | 32/157 [00:01<00:05, 21.44it/s][A
 25%|##4       | 39/157 [00:01<00:04, 28.04it/s][A
 29%|##9       | 46/157 [00:01<00:03, 34.45it/s][A
 33%|###3      | 52/157 [00:01<00:02, 38.48it/s][A
 38%|###7      | 59/157 [00:01<00:02, 43.61it/s][A
 42%|####2     | 66/157 [00:01<00:01, 48.25it/s][A
 46%|####6     | 73/157 [00:01<00:01, 51.80it/s][A
 51%|#####     | 80/157 [00:01<00:01, 54.68it/s][A
 55%|#####5    | 87/157 [00:01<00:01, 56.80it/s][A
 60%|#####9    | 94/157 [00:02<00:01, 58.58it/s][A
 64%|######4   | 101/157 [00:02<00:00, 59.84it/s][A
 69%|######8   | 108/157 [00:02<00:00, 60.62it/s][A
 73%|#######3  | 115/157 [00:02<00:00, 59.47it/s][A
 78%|#######7  | 122/157 [00:02<00:00, 60.52it/s][A
 82%|########2 | 129/157 [00:02<00:00, 60.99it/s][A
 87%|########6 | 136/157 [00:02<00:00, 61.72it/s][A
 91%|#########1| 143/157 [00:02<00:00, 61.51it/s][A
 96%|#########5| 150/157 [00:02<00:00, 61.68it/s][A
100%|##########| 157/157 [00:03<00:00, 62.41it/s][A100%|##########| 157/157 [00:03<00:00, 50.70it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|4         | 7/157 [00:00<00:02, 61.13it/s][A
  9%|8         | 14/157 [00:00<00:02, 57.99it/s][A
 13%|#3        | 21/157 [00:00<00:02, 59.30it/s][A
 18%|#7        | 28/157 [00:00<00:02, 59.69it/s][A
 22%|##1       | 34/157 [00:00<00:02, 59.37it/s][A
 26%|##6       | 41/157 [00:00<00:01, 59.67it/s][A
 31%|###       | 48/157 [00:00<00:01, 60.83it/s][A
 35%|###5      | 55/157 [00:00<00:01, 61.89it/s][A
 39%|###9      | 62/157 [00:01<00:01, 62.50it/s][A
 44%|####3     | 69/157 [00:01<00:01, 63.25it/s][A
 48%|####8     | 76/157 [00:01<00:01, 61.41it/s][A
 53%|#####2    | 83/157 [00:01<00:01, 62.04it/s][A
 57%|#####7    | 90/157 [00:01<00:01, 62.55it/s][A
 62%|######1   | 97/157 [00:01<00:00, 63.08it/s][A
 66%|######6   | 104/157 [00:01<00:00, 63.22it/s][A
 71%|#######   | 111/157 [00:01<00:00, 63.42it/s][A
 75%|#######5  | 118/157 [00:01<00:00, 63.49it/s][A
 80%|#######9  | 125/157 [00:02<00:00, 63.61it/s][A
 84%|########4 | 132/157 [00:02<00:00, 63.62it/s][A
 89%|########8 | 139/157 [00:02<00:00, 61.71it/s][A
 93%|#########2| 146/157 [00:02<00:00, 62.30it/s][A
 97%|#########7| 153/157 [00:02<00:00, 63.68it/s][A100%|##########| 157/157 [00:02<00:00, 62.26it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|4         | 7/157 [00:00<00:02, 61.05it/s][A
  9%|8         | 14/157 [00:00<00:02, 61.54it/s][A
 13%|#3        | 21/157 [00:00<00:02, 60.25it/s][A
 18%|#7        | 28/157 [00:00<00:02, 60.61it/s][A
 22%|##2       | 35/157 [00:00<00:02, 58.39it/s][A
 26%|##6       | 41/157 [00:00<00:01, 58.31it/s][A
 31%|###       | 48/157 [00:00<00:01, 59.04it/s][A
 34%|###4      | 54/157 [00:00<00:01, 59.25it/s][A
 39%|###8      | 61/157 [00:01<00:01, 59.99it/s][A
 43%|####3     | 68/157 [00:01<00:01, 59.86it/s][A
 47%|####7     | 74/157 [00:01<00:01, 59.71it/s][A
 51%|#####     | 80/157 [00:01<00:01, 59.21it/s][A
 55%|#####5    | 87/157 [00:01<00:01, 59.66it/s][A
 60%|#####9    | 94/157 [00:01<00:01, 59.64it/s][A
 64%|######3   | 100/157 [00:01<00:00, 58.86it/s][A
 68%|######8   | 107/157 [00:01<00:00, 59.51it/s][A
 73%|#######2  | 114/157 [00:01<00:00, 60.38it/s][A
 77%|#######7  | 121/157 [00:02<00:00, 60.66it/s][A
 82%|########1 | 128/157 [00:02<00:00, 61.32it/s][A
 86%|########5 | 135/157 [00:02<00:00, 61.82it/s][A
 90%|######### | 142/157 [00:02<00:00, 62.36it/s][A
 95%|#########4| 149/157 [00:02<00:00, 62.59it/s][A
 99%|#########9| 156/157 [00:02<00:00, 62.38it/s][A100%|##########| 157/157 [00:02<00:00, 60.44it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: static, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 59.24it/s][A
  8%|8         | 13/157 [00:00<00:02, 61.27it/s][A
 13%|#2        | 20/157 [00:00<00:02, 62.60it/s][A
 17%|#7        | 27/157 [00:00<00:02, 62.67it/s][A
 22%|##1       | 34/157 [00:00<00:02, 61.12it/s][A
 26%|##6       | 41/157 [00:00<00:01, 61.14it/s][A
 31%|###       | 48/157 [00:00<00:01, 61.35it/s][A
 35%|###5      | 55/157 [00:00<00:01, 61.72it/s][A
 39%|###9      | 62/157 [00:01<00:01, 60.42it/s][A
 44%|####3     | 69/157 [00:01<00:01, 61.10it/s][A
 48%|####8     | 76/157 [00:01<00:01, 61.62it/s][A
 53%|#####2    | 83/157 [00:01<00:01, 62.06it/s][A
 57%|#####7    | 90/157 [00:01<00:01, 62.38it/s][A
 62%|######1   | 97/157 [00:01<00:00, 62.63it/s][A
 66%|######6   | 104/157 [00:01<00:00, 62.82it/s][A
 71%|#######   | 111/157 [00:01<00:00, 62.97it/s][A
 75%|#######5  | 118/157 [00:01<00:00, 62.50it/s][A
 80%|#######9  | 125/157 [00:02<00:00, 61.04it/s][A
 84%|########4 | 132/157 [00:02<00:00, 61.73it/s][A
 89%|########8 | 139/157 [00:02<00:00, 62.20it/s][A
 93%|#########2| 146/157 [00:02<00:00, 62.54it/s][A
 97%|#########7| 153/157 [00:02<00:00, 63.14it/s][A100%|##########| 157/157 [00:02<00:00, 62.06it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: dynamic, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.float16){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 59.53it/s][A
  8%|8         | 13/157 [00:00<00:02, 60.81it/s][A
 13%|#2        | 20/157 [00:00<00:02, 57.95it/s][A
 17%|#6        | 26/157 [00:00<00:02, 56.81it/s][A
 21%|##1       | 33/157 [00:00<00:02, 58.84it/s][A
 25%|##5       | 40/157 [00:00<00:01, 60.16it/s][A
 30%|##9       | 47/157 [00:00<00:01, 60.88it/s][A
 34%|###4      | 54/157 [00:00<00:01, 60.63it/s][A
 39%|###8      | 61/157 [00:01<00:01, 60.31it/s][A
 43%|####3     | 68/157 [00:01<00:01, 60.99it/s][A
 48%|####7     | 75/157 [00:01<00:01, 61.35it/s][A
 52%|#####2    | 82/157 [00:01<00:01, 59.74it/s][A
 56%|#####6    | 88/157 [00:01<00:01, 59.69it/s][A
 61%|######    | 95/157 [00:01<00:01, 60.45it/s][A
 65%|######4   | 102/157 [00:01<00:00, 61.30it/s][A
 69%|######9   | 109/157 [00:01<00:00, 61.46it/s][A
 74%|#######3  | 116/157 [00:01<00:00, 60.85it/s][A
 78%|#######8  | 123/157 [00:02<00:00, 61.17it/s][A
 83%|########2 | 130/157 [00:02<00:00, 60.70it/s][A
 87%|########7 | 137/157 [00:02<00:00, 61.15it/s][A
 92%|#########1| 144/157 [00:02<00:00, 59.73it/s][A
 96%|#########6| 151/157 [00:02<00:00, 60.11it/s][A100%|##########| 157/157 [00:02<00:00, 60.29it/s]
[QCONFIG] Created qconfig mapping: QConfigMapping (
 global_qconfig
  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})
 object_type_qconfigs
  OrderedDict()
 module_name_regex_qconfigs
  OrderedDict()
 module_name_qconfigs
  OrderedDict()
 module_name_object_type_order_qconfigs
  OrderedDict()
)
[INIT] quant_type: qat, backend: onednn, device: cuda:0
[INIT] dtype: torch.float16, allow_embedding: False, allow_convolution: False
[INIT] qconfig: {'': QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})}

  0%|          | 0/157 [00:00<?, ?it/s][A
  4%|3         | 6/157 [00:00<00:02, 58.51it/s][A
  8%|8         | 13/157 [00:00<00:02, 59.51it/s][A
 13%|#2        | 20/157 [00:00<00:02, 60.85it/s][A
 17%|#7        | 27/157 [00:00<00:02, 61.37it/s][A
 22%|##1       | 34/157 [00:00<00:01, 61.55it/s][A
 26%|##6       | 41/157 [00:00<00:01, 60.11it/s][A
 31%|###       | 48/157 [00:00<00:01, 60.13it/s][A
 35%|###5      | 55/157 [00:00<00:01, 61.14it/s][A
 39%|###9      | 62/157 [00:01<00:01, 61.47it/s][A
 44%|####3     | 69/157 [00:01<00:01, 61.86it/s][A
 48%|####8     | 76/157 [00:01<00:01, 62.09it/s][A
 53%|#####2    | 83/157 [00:01<00:01, 62.54it/s][A
 57%|#####7    | 90/157 [00:01<00:01, 62.94it/s][A
 62%|######1   | 97/157 [00:01<00:00, 63.18it/s][A
 66%|######6   | 104/157 [00:01<00:00, 62.15it/s][A
 71%|#######   | 111/157 [00:01<00:00, 61.56it/s][A
 75%|#######5  | 118/157 [00:01<00:00, 61.77it/s][A
 80%|#######9  | 125/157 [00:02<00:00, 62.18it/s][A
 84%|########4 | 132/157 [00:02<00:00, 62.35it/s][A
 89%|########8 | 139/157 [00:02<00:00, 62.64it/s][A
 93%|#########2| 146/157 [00:02<00:00, 62.94it/s][A
 97%|#########7| 153/157 [00:02<00:00, 62.61it/s][A100%|##########| 157/157 [00:02<00:00, 61.91it/s]

  0%|          | 0/313 [00:00<?, ?it/s][A  0%|          | 0/313 [00:00<?, ?it/s]
Generations:   0%|          | 0/10000 [18:17<?, ?gen/s]
